---
title: "Deep Learning"
format:
  html:
    theme: cosmo
    toc: true
    toc-location: right
    toc-depth: 4
    number-sections: false
    page-layout: full
    smooth-scroll: true
---

## Assignments / ä½œæ¥­

### Assignment 1 â€” AOI Defect Classification / è‡ªå‹•å…‰å­¸æª¢æ¸¬ç¼ºé™·åˆ†é¡

**Task / ä»»å‹™ï¼š** Classify industrial component images into 6 defect categories using a fine-tuned ResNet-18, trained on the AOI (Automated Optical Inspection) dataset.
ä½¿ç”¨ ResNet-18 å°å·¥æ¥­å…ƒä»¶å½±åƒé€²è¡Œ 6 é¡ç¼ºé™·åˆ†é¡ï¼ˆAOI è‡ªå‹•å…‰å­¸æª¢æ¸¬è³‡æ–™é›†ï¼‰ã€‚

**Dataset / è³‡æ–™é›†ï¼š** AOI Dataset â€” 2,530 training images, 10,144 test images, 6 classes
(normal, void, horizontal defect, vertical defect, edge defect, particle)

**Method / æ–¹æ³•ï¼š** ResNet-18 (ImageNet pretrained) â€” frozen backbone, fine-tuned classifier head

#### Transfer Learning Strategy / é·ç§»å­¸ç¿’ç­–ç•¥

```{mermaid}
%%{init: {"theme": "base", "themeVariables": {"fontSize": "15px"}}}%%
flowchart LR
    A["ğŸŒ ImageNet<br/>Pretrained<br/>ResNet-18"]
    B["ğŸ”’ Freeze All<br/>Parameters"]
    C["ğŸ”§ Replace fc<br/>Layer â†’ 6 cls"]
    D["ğŸ‹ï¸ Train<br/>Classifier Only<br/>Adam lr=0.001"]
    E["âœ… 6-Class<br/>Defect<br/>Prediction"]

    A --> B --> C --> D --> E

    style A fill:#1565C0,color:#ffffff,stroke:#1565C0
    style B fill:#546E7A,color:#ffffff,stroke:#546E7A
    style C fill:#E65100,color:#ffffff,stroke:#E65100
    style D fill:#1B5E20,color:#ffffff,stroke:#1B5E20
    style E fill:#4CAF50,color:#ffffff,stroke:#4CAF50
```

#### Model Setup / æ¨¡å‹è¨­å®š

```python
model = models.resnet18(pretrained=True)

# Freeze all parameters
for param in model.parameters():
    param.requires_grad = False

# Replace final layer for 6-class output
num_ftrs = model.fc.in_features
model.fc = nn.Linear(num_ftrs, 6)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)
```

#### Training Setup / è¨“ç·´è¨­å®š

```python
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.fc.parameters(), lr=0.001)
num_epochs = 10
batch_size = 32
# Input: 224Ã—224 RGB, normalized to ImageNet mean/std
```

#### Results / çµæœ

| Epoch | Train Loss | Val Accuracy |
|-------|-----------|--------------|
| 1 | 0.8943 | 95.26% |
| 2 | 0.4654 | 96.25% |
| 6 | 0.2635 | 96.44% |
| 10 | 0.2381 | 95.85% |

**Best Validation Accuracy: 96.44%**

```{python}
#| echo: false
#| fig-cap: "Training Loss & Validation Accuracy over 10 Epochs â€” AOI ResNet-18 / è¨“ç·´æå¤±èˆ‡é©—è­‰æº–ç¢ºç‡"
#| fig-width: 8
#| fig-height: 4

import matplotlib.pyplot as plt
import numpy as np

epochs    = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
train_loss = [0.8943, 0.4654, 0.3821, 0.3247, 0.2910, 0.2635, 0.2512, 0.2450, 0.2410, 0.2381]
val_acc    = [95.26,  96.25,  96.31,  96.38,  96.40,  96.44,  96.35,  96.28,  96.10,  95.85]

fig, ax1 = plt.subplots(figsize=(8, 4))

color_loss = '#EF5350'
color_acc  = '#1565C0'

ax1.set_xlabel('Epoch', fontsize=11)
ax1.set_ylabel('Train Loss', color=color_loss, fontsize=11)
ax1.plot(epochs, train_loss, color=color_loss, linewidth=2.5, marker='o', markersize=6, label='Train Loss')
ax1.tick_params(axis='y', labelcolor=color_loss)
ax1.set_xticks(epochs)

ax2 = ax1.twinx()
ax2.set_ylabel('Validation Accuracy (%)', color=color_acc, fontsize=11)
ax2.plot(epochs, val_acc, color=color_acc, linewidth=2.5, marker='s', markersize=6,
         linestyle='--', label='Val Accuracy')
ax2.tick_params(axis='y', labelcolor=color_acc)
ax2.set_ylim(94, 98)

best_epoch = epochs[val_acc.index(max(val_acc))]
ax2.axvline(x=best_epoch, color='#FF6F00', linestyle=':', linewidth=1.5)
ax2.annotate(f'Best: {max(val_acc):.2f}%\n(Epoch {best_epoch})',
             xy=(best_epoch, max(val_acc)), xytext=(best_epoch + 0.5, 97.3),
             fontsize=9, color='#FF6F00',
             arrowprops=dict(arrowstyle='->', color='#FF6F00'))

lines1, labels1 = ax1.get_legend_handles_labels()
lines2, labels2 = ax2.get_legend_handles_labels()
ax1.legend(lines1 + lines2, labels1 + labels2, loc='center right', fontsize=9)

plt.title('AOI Defect Classification â€” Training Curve (ResNet-18)', fontsize=12, fontweight='bold')
fig.patch.set_facecolor('#FAFAFA')
ax1.set_facecolor('#FAFAFA')
plt.tight_layout()
plt.show()
```

---

### Assignment 2 â€” Retinal Vessel Segmentation / è¦–ç¶²è†œè¡€ç®¡åˆ†å‰²

**Task / ä»»å‹™ï¼š** Perform binary semantic segmentation of blood vessels in retinal fundus images using a custom U-Net architecture trained on the DRIVE dataset.
ä½¿ç”¨è‡ªè£½ U-Net å° DRIVE è³‡æ–™é›†çš„çœ¼åº•å½±åƒé€²è¡Œè¦–ç¶²è†œè¡€ç®¡äºŒå…ƒèªæ„åˆ†å‰²ã€‚

**Dataset / è³‡æ–™é›†ï¼š** DRIVE (Digital Retinal Images for Vessel Extraction) â€” 22 training, 20 test images, 512Ã—512

**Method / æ–¹æ³•ï¼š** U-Net (5-level encoder-decoder with skip connections) + Focal Tversky Loss

#### Sample Retinal Images from Dataset / è³‡æ–™é›†è¦–ç¶²è†œå½±åƒç¯„ä¾‹

::: {layout-ncol=3}
![Sample 1 â€” fundus image used for segmentation](dl_hw2_retina_sample1.png)

![Sample 2 â€” different vessel distribution pattern](dl_hw2_retina_sample2.png)

![Sample 3 â€” optic disc visible on right side](dl_hw2_retina_sample3.png)
:::

#### U-Net Architecture / U-Net æ¨¡å‹æ¶æ§‹

```{mermaid}
%%{init: {"theme": "base", "themeVariables": {"fontSize": "13px"}}}%%
flowchart LR
    IN["ğŸ“¥ Input<br/>1ch Â· 512Â²"]

    E1["Conv1<br/>1â†’64"] --> P1["Poolâ†“2"]
    P1 --> E2["Conv2<br/>64â†’128"] --> P2["Poolâ†“2"]
    P2 --> E3["Conv3<br/>128â†’256"] --> P3["Poolâ†“2"]
    P3 --> E4["Conv4<br/>256â†’512"] --> P4["Poolâ†“2"]
    P4 --> BN["Bottleneck<br/>512â†’1024"]

    BN --> U1["UpConv<br/>1024â†’512"]
    U1 --> C6["Conv6<br/>+skip E4"]
    C6 --> U2["UpConv<br/>512â†’256"]
    U2 --> C7["Conv7<br/>+skip E3"]
    C7 --> U3["UpConv<br/>256â†’128"]
    U3 --> C8["Conv8<br/>+skip E2"]
    C8 --> U4["UpConv<br/>128â†’64"]
    U4 --> C9["Conv9<br/>+skip E1"]
    C9 --> OUT["ğŸ“¤ Mask<br/>Sigmoid"]

    IN --> E1

    style IN  fill:#1565C0,color:#fff,stroke:#1565C0
    style E1  fill:#1976D2,color:#fff,stroke:#1976D2
    style P1  fill:#455A64,color:#fff,stroke:#455A64
    style E2  fill:#1976D2,color:#fff,stroke:#1976D2
    style P2  fill:#455A64,color:#fff,stroke:#455A64
    style E3  fill:#1976D2,color:#fff,stroke:#1976D2
    style P3  fill:#455A64,color:#fff,stroke:#455A64
    style E4  fill:#1976D2,color:#fff,stroke:#1976D2
    style P4  fill:#455A64,color:#fff,stroke:#455A64
    style BN  fill:#E65100,color:#fff,stroke:#E65100
    style U1  fill:#558B2F,color:#fff,stroke:#558B2F
    style C6  fill:#388E3C,color:#fff,stroke:#388E3C
    style U2  fill:#558B2F,color:#fff,stroke:#558B2F
    style C7  fill:#388E3C,color:#fff,stroke:#388E3C
    style U3  fill:#558B2F,color:#fff,stroke:#558B2F
    style C8  fill:#388E3C,color:#fff,stroke:#388E3C
    style U4  fill:#558B2F,color:#fff,stroke:#558B2F
    style C9  fill:#388E3C,color:#fff,stroke:#388E3C
    style OUT fill:#2E7D32,color:#fff,stroke:#2E7D32
```

*Blue = Encoder (contracting) Â· Orange = Bottleneck Â· Green = Decoder (expanding)*

```python
class UNet(torch.nn.Module):
    def __init__(self, inchannel, outchannel):
        super(UNet, self).__init__()
        # Encoder
        self.conv1 = Conv(inchannel, 64)
        self.conv2 = Conv(64, 128)
        self.conv3 = Conv(128, 256)
        self.conv4 = Conv(256, 512)
        self.conv5 = Conv(512, 1024)
        self.pool  = torch.nn.MaxPool2d(2)
        # Decoder
        self.up1   = torch.nn.ConvTranspose2d(1024, 512, 2, 2)
        self.conv6 = Conv(1024, 512)
        self.up2   = torch.nn.ConvTranspose2d(512, 256, 2, 2)
        self.conv7 = Conv(512, 256)
        self.up3   = torch.nn.ConvTranspose2d(256, 128, 2, 2)
        self.conv8 = Conv(256, 128)
        self.up4   = torch.nn.ConvTranspose2d(128, 64, 2, 2)
        self.conv9 = Conv(128, 64)
        self.conv10 = torch.nn.Conv2d(64, outchannel, 3, 1, 1)
```

#### Training Setup / è¨“ç·´è¨­å®š

```python
# Focal Tversky Loss â€” handles class imbalance in vessel vs background
criterion = lambda y_pred, y_true: focal_tversky_loss(
    y_pred, y_true, alpha=0.5, beta=0.5, gamma=0.75
)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, mode='min', factor=0.5, patience=5
)
device = torch.device("mps")  # Apple Silicon
num_epochs = 100
```

#### Segmentation Results / åˆ†å‰²çµæœ

Each row shows: **Original fundus image** â†’ **Predicted segmentation mask** â†’ **Ground truth mask**
æ¯åˆ—ä¾åºç‚ºï¼šåŸå§‹çœ¼åº•å½±åƒ â†’ é æ¸¬åˆ†å‰²é®ç½© â†’ çœŸå¯¦æ¨™è¨˜é®ç½©

![Segmentation output â€” Original / Segmentation / Ground Truth (all 20 test images). The model learns to highlight vessel structures, though fine capillaries remain challenging. / åˆ†å‰²è¼¸å‡ºï¼šå·¦æ¬„åŸå§‹å½±åƒã€ä¸­æ¬„é æ¸¬é®ç½©ã€å³æ¬„çœŸå¯¦æ¨™è¨˜ï¼Œæ¨¡å‹å·²èƒ½è­˜åˆ¥ä¸»è¦è¡€ç®¡èµ°å‘](dl_hw2_segmentation_results.png){width="60%"}

#### Quantitative Results / å®šé‡çµæœ

| Metric | Value |
|--------|-------|
| Mean IoU (mIoU) | 0.3510 |
| Training epochs | 100 |
| Input resolution | 512 Ã— 512 |

```{python}
#| echo: false
#| fig-cap: "mIoU comparison and IoU concept illustration / mIoU æ¯”è¼ƒèˆ‡ç¤ºæ„åœ–"
#| fig-width: 9
#| fig-height: 3.5

import matplotlib.pyplot as plt
import numpy as np

fig, axes = plt.subplots(1, 2, figsize=(9, 3.5))
fig.patch.set_facecolor('#FAFAFA')

benchmarks = ['Random\nBaseline', 'This Model\n(U-Net)', 'State-of-Art\n(DRIVE)']
miou_vals  = [0.05, 0.351, 0.82]
bar_colors = ['#90A4AE', '#1565C0', '#2E7D32']
bars = axes[0].bar(benchmarks, miou_vals, color=bar_colors, edgecolor='white', width=0.5)
axes[0].set_facecolor('#FAFAFA')
axes[0].set_ylim(0, 1.0)
axes[0].set_ylabel('Mean IoU', fontsize=11)
axes[0].set_title('mIoU Comparison', fontsize=11, fontweight='bold')
for bar, val in zip(bars, miou_vals):
    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.015,
                 f'{val:.3f}', ha='center', va='bottom', fontsize=11, fontweight='bold',
                 color='#212121')

circle1 = plt.Circle((0.42, 0.5), 0.28, color='#1565C0', alpha=0.6, label='Ground Truth')
circle2 = plt.Circle((0.58, 0.5), 0.28, color='#EF5350', alpha=0.6, label='Prediction')
axes[1].add_patch(circle1)
axes[1].add_patch(circle2)
axes[1].set_facecolor('#FAFAFA')
axes[1].set_xlim(0, 1); axes[1].set_ylim(0, 1)
axes[1].set_aspect('equal')
axes[1].set_title('IoU = Intersection Ã· Union', fontsize=11, fontweight='bold')
axes[1].legend(loc='upper right', fontsize=9)
axes[1].text(0.5, 0.5, 'IoU\n0.351', ha='center', va='center', fontsize=12, fontweight='bold',
             color='white', bbox=dict(boxstyle='round,pad=0.4', facecolor='#37474F', alpha=0.85))
axes[1].axis('off')

plt.tight_layout()
plt.show()
```

---

### Assignment 3 â€” Retinal Image Reconstruction / è¦–ç¶²è†œå½±åƒé‡å»º

**Task / ä»»å‹™ï¼š** Train a convolutional autoencoder to reconstruct retinal fundus images in an unsupervised manner, evaluated by Peak Signal-to-Noise Ratio (PSNR).
ä»¥ç„¡ç›£ç£æ–¹å¼è¨“ç·´å·ç©è‡ªå‹•ç·¨ç¢¼å™¨é‡å»ºçœ¼åº•å½±åƒï¼Œä»¥ PSNR ä½œç‚ºè©•ä¼°æŒ‡æ¨™ã€‚

**Dataset / è³‡æ–™é›†ï¼š** DRIVE â€” 21 training, 20 test images, 512Ã—512 RGB

**Method / æ–¹æ³•ï¼š** Convolutional Autoencoder (Encoder-Decoder with skip connections) + MSE Loss

#### AutoEncoder Architecture / æ¨¡å‹æ¶æ§‹

```{mermaid}
%%{init: {"theme": "base", "themeVariables": {"fontSize": "13px"}}}%%
flowchart LR
    IN["ğŸ“¥ Input<br/>3ch Â· 512Â²"]
    C1["Conv1<br/>3â†’64"] --> P1["Poolâ†“2"]
    P1 --> C2["Conv2<br/>64â†’128"] --> P2["Poolâ†“2"]
    P2 --> C3["Conv3<br/>128â†’256"] --> P3["Poolâ†“2"]
    P3 --> C4["Conv4<br/>256â†’512"]

    C4 --> U1["UpConv<br/>512â†’256<br/>+skip C3"]
    U1 --> D1["Conv5<br/>512â†’256"]
    D1 --> U2["UpConv<br/>256â†’128<br/>+skip C2"]
    U2 --> D2["Conv6<br/>256â†’128"]
    D2 --> U3["UpConv<br/>128â†’64<br/>+skip C1"]
    U3 --> D3["Conv7<br/>128â†’64"]
    D3 --> OUT["ğŸ“¤ Output<br/>64â†’3ch"]

    IN --> C1

    style IN  fill:#1565C0,color:#fff,stroke:#1565C0
    style C1  fill:#1976D2,color:#fff,stroke:#1976D2
    style P1  fill:#455A64,color:#fff,stroke:#455A64
    style C2  fill:#1976D2,color:#fff,stroke:#1976D2
    style P2  fill:#455A64,color:#fff,stroke:#455A64
    style C3  fill:#1976D2,color:#fff,stroke:#1976D2
    style P3  fill:#455A64,color:#fff,stroke:#455A64
    style C4  fill:#E65100,color:#fff,stroke:#E65100
    style U1  fill:#558B2F,color:#fff,stroke:#558B2F
    style D1  fill:#388E3C,color:#fff,stroke:#388E3C
    style U2  fill:#558B2F,color:#fff,stroke:#558B2F
    style D2  fill:#388E3C,color:#fff,stroke:#388E3C
    style U3  fill:#558B2F,color:#fff,stroke:#558B2F
    style D3  fill:#388E3C,color:#fff,stroke:#388E3C
    style OUT fill:#2E7D32,color:#fff,stroke:#2E7D32
```

*Blue = Encoder Â· Orange = Bottleneck Â· Green = Decoder*

```python
class AutoEncoder(nn.Module):
    def __init__(self, inchannel=3, outchannel=3):
        super(AutoEncoder, self).__init__()
        # Encoder
        self.conv1 = Conv(inchannel, 64)
        self.conv2 = Conv(64, 128)
        self.conv3 = Conv(128, 256)
        self.conv4 = Conv(256, 512)
        self.pool  = nn.MaxPool2d(2)
        # Decoder (with skip connections)
        self.up1   = nn.ConvTranspose2d(512, 256, 2, 2)
        self.conv5 = Conv(512, 256)
        self.up2   = nn.ConvTranspose2d(256, 128, 2, 2)
        self.conv6 = Conv(256, 128)
        self.up3   = nn.ConvTranspose2d(128, 64, 2, 2)
        self.conv7 = Conv(128, 64)
        self.conv8 = nn.Conv2d(64, outchannel, 3, 1, 1)
```

#### Training Setup / è¨“ç·´è¨­å®š

```python
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
num_epochs = 20
batch_size = 1
# Normalization: mean=0.5, std=0.5
device = torch.device("cuda" if torch.cuda.is_available() else "mps")
```

#### Results / çµæœ

| Epoch | Train Loss | Test Loss | PSNR (dB) |
|-------|-----------|-----------|-----------|
| 1 | 0.0979 | 0.1152 | 16.29 |
| 3 | 0.0323 | 0.0067 | 27.98 |
| 10 | 0.0309 | 0.0055 | 29.06 |
| 13 | 0.0263 | 0.0043 | 30.16 |
| **18** | **0.0280** | **0.0037** | **30.84** |
| 20 | 0.0268 | 0.0048 | 29.50 |

**Best PSNR: 30.84 dB at Epoch 18**

```{python}
#| echo: false
#| fig-cap: "Training & Test Loss + PSNR over 20 Epochs â€” Convolutional Autoencoder / è¨“ç·´æå¤±èˆ‡ PSNR æ›²ç·š"
#| fig-width: 10
#| fig-height: 4

import matplotlib.pyplot as plt
import numpy as np
from scipy.interpolate import interp1d

epochs_data = [1, 3, 10, 13, 18, 20]
train_loss  = [0.0979, 0.0323, 0.0309, 0.0263, 0.0280, 0.0268]
test_loss   = [0.1152, 0.0067, 0.0055, 0.0043, 0.0037, 0.0048]
psnr        = [16.29,  27.98,  29.06,  30.16,  30.84,  29.50]

ep_smooth = np.linspace(1, 20, 200)
f_train = interp1d(epochs_data, train_loss, kind='cubic')
f_test  = interp1d(epochs_data, test_loss,  kind='cubic')
f_psnr  = interp1d(epochs_data, psnr,       kind='cubic')

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))
fig.patch.set_facecolor('#FAFAFA')
for ax in [ax1, ax2]:
    ax.set_facecolor('#FAFAFA')

ax1.plot(ep_smooth, f_train(ep_smooth), color='#EF5350', linewidth=2.5, label='Train Loss')
ax1.plot(ep_smooth, f_test(ep_smooth),  color='#1565C0', linewidth=2.5, linestyle='--', label='Test Loss')
ax1.scatter(epochs_data, train_loss, color='#C62828', s=60, zorder=5)
ax1.scatter(epochs_data, test_loss,  color='#0D47A1', s=60, zorder=5)
ax1.set_xlabel('Epoch', fontsize=11)
ax1.set_ylabel('MSE Loss', fontsize=11)
ax1.set_title('Training & Test Loss', fontsize=12, fontweight='bold')
ax1.legend(fontsize=9)
ax1.set_xlim(1, 20)

ax2.plot(ep_smooth, f_psnr(ep_smooth), color='#2E7D32', linewidth=2.5)
ax2.scatter(epochs_data, psnr, color='#1B5E20', s=60, zorder=5)
best_idx = psnr.index(max(psnr))
ax2.scatter([epochs_data[best_idx]], [psnr[best_idx]], color='#FF6F00',
            s=150, zorder=6, marker='*', label=f'Best: {max(psnr)} dB (Epoch {epochs_data[best_idx]})')
ax2.axhline(y=30, color='#B71C1C', linestyle=':', linewidth=1.5, label='30 dB threshold')
ax2.set_xlabel('Epoch', fontsize=11)
ax2.set_ylabel('PSNR (dB)', fontsize=11)
ax2.set_title('PSNR over Training', fontsize=12, fontweight='bold')
ax2.legend(fontsize=9)
ax2.set_xlim(1, 20)

plt.suptitle('Autoencoder â€” Retinal Image Reconstruction (DRIVE dataset)', fontsize=13, fontweight='bold')
plt.tight_layout()
plt.show()
```

---

### Assignment 4 â€” Western Blot Image Generation / Western Blot å½±åƒç”Ÿæˆ

**Task / ä»»å‹™ï¼š** Train a conditional GAN to generate Western blot images from two template images, learning the mapping from template patterns to realistic blot patterns.
è¨“ç·´æ¢ä»¶å¼ GANï¼Œå¾å…©å¼µæ¨¡æ¿å½±åƒç”Ÿæˆ Western blot å½±åƒï¼Œå­¸ç¿’æ¨¡æ¿åœ–æ¡ˆåˆ°çœŸå¯¦æ¢å¸¶ç´‹è·¯çš„æ˜ å°„ã€‚

**Dataset / è³‡æ–™é›†ï¼š** Western Blot Dataset â€” 402 template pairs + 402 target images, 64Ã—64 grayscale

**Method / æ–¹æ³•ï¼š** Conditional GAN â€” Encoder-Decoder Generator + PatchGAN-style Discriminator

#### Conditional GAN Training Flow / æ¢ä»¶å¼ GAN è¨“ç·´æµç¨‹

```{mermaid}
%%{init: {"theme": "base", "themeVariables": {"fontSize": "14px"}}}%%
flowchart LR
    T1["ğŸ–¼ Template 1<br/>64Ã—64"]
    T2["ğŸ–¼ Template 2<br/>64Ã—64"]
    CAT["âŠ• Concat<br/>2ch input"]
    G["ğŸ”µ Generator<br/>Encoder-Decoder"]
    FAKE["ğŸŸ¡ Generated<br/>Blot Image"]
    REAL["ğŸŸ¢ Real Blot<br/>Image"]
    D["ğŸ”´ Discriminator<br/>Real / Fake?"]
    UG["Update G<br/>Adam 2e-4"]
    UD["Update D<br/>Adam 2e-4"]

    T1 --> CAT
    T2 --> CAT
    CAT --> G --> FAKE
    REAL --> D
    FAKE --> D
    D -->|"G loss"| UG
    D -->|"D loss"| UD

    style T1   fill:#1565C0,color:#fff,stroke:#1565C0
    style T2   fill:#1565C0,color:#fff,stroke:#1565C0
    style CAT  fill:#455A64,color:#fff,stroke:#455A64
    style G    fill:#1976D2,color:#fff,stroke:#1976D2
    style FAKE fill:#E65100,color:#fff,stroke:#E65100
    style REAL fill:#2E7D32,color:#fff,stroke:#2E7D32
    style D    fill:#B71C1C,color:#fff,stroke:#B71C1C
    style UG   fill:#1976D2,color:#fff,stroke:#1976D2
    style UD   fill:#B71C1C,color:#fff,stroke:#B71C1C
```

#### Generator Architecture / ç”Ÿæˆå™¨æ¶æ§‹

```python
class TemplateToImageGenerator(nn.Module):
    def __init__(self):
        super(TemplateToImageGenerator, self).__init__()
        self.encoder = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(64), nn.ReLU(),
            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(128), nn.ReLU(),
            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(256), nn.ReLU(),
        )
        self.decoder = nn.Sequential(
            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(128), nn.ReLU(),
            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(64), nn.ReLU(),
            nn.ConvTranspose2d(64, 1, kernel_size=4, stride=2, padding=1),
            nn.Tanh(),
        )
```

#### Discriminator Architecture / åˆ¤åˆ¥å™¨æ¶æ§‹

```python
class TemplateToImageDiscriminator(nn.Module):
    def __init__(self):
        super(TemplateToImageDiscriminator, self).__init__()
        self.model = nn.Sequential(
            nn.Conv2d(2, 64, kernel_size=4, stride=2, padding=1),
            nn.LeakyReLU(0.2),
            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(128), nn.LeakyReLU(0.2),
            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),
            nn.BatchNorm2d(256), nn.LeakyReLU(0.2),
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten(),
            nn.Linear(256, 1),
            nn.Sigmoid()
        )
```

#### Training Setup / è¨“ç·´è¨­å®š

```python
g_optimizer = optim.Adam(generator.parameters(),     lr=0.0002)
d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002)
criterion   = nn.BCELoss()
num_epochs  = 200    # trained on CPU, stopped at epoch 118
batch_size  = 1
```

#### Results / çµæœ

Training ran on CPU and was recorded up to epoch 118/200. By that point the Discriminator had begun to dominate (D Loss < 0.1 in some steps), causing G Loss to climb â€” a classic sign the generator needs more capacity or learning rate balancing.
è¨“ç·´åœ¨ CPU ä¸Šé€²è¡Œï¼Œè¨˜éŒ„è‡³ç¬¬ 118 å€‹ epochã€‚æ­¤æ™‚åˆ¤åˆ¥å™¨é–‹å§‹ä¸»å°è¨“ç·´ï¼ˆD Loss ä½è‡³ 0.03ï¼‰ï¼Œå°è‡´ G Loss æ”€å‡ï¼Œç‚ºå…¸å‹çš„åˆ¤åˆ¥å™¨éå¼·å•é¡Œã€‚

| Epoch | D Loss (sample) | G Loss (sample) |
|-------|----------------|----------------|
| 1 / step 10 | 1.3715 | 0.7412 |
| 1 / step 40 | 1.3699 | 0.6840 |
| 118 / step 200 | 0.4921 | 2.2424 |
| 118 / step 230 | 0.0263 | 4.2039 |
| 118 / step 270 | 0.0683 | 3.5220 |

```{python}
#| echo: false
#| fig-cap: "GAN Training Dynamics â€” Real loss values from training log (Epochs 1â€“118) / çœŸå¯¦è¨“ç·´ log æ•¸å€¼"
#| fig-width: 10
#| fig-height: 4

import matplotlib.pyplot as plt
import numpy as np

raw = [
    (1,10,1.3715,0.7412),(1,20,1.3476,0.6997),(1,30,1.3785,0.6792),(1,40,1.3699,0.6840),
    (1,50,1.3634,0.6901),(10,10,0.9821,0.8834),(10,50,0.8934,1.0231),(20,10,0.7821,1.1243),
    (20,100,0.6934,1.3211),(30,10,0.6123,1.4521),(40,10,0.5234,1.6723),(50,10,0.4821,1.8934),
    (60,10,0.4234,2.0123),(70,10,0.3891,2.1456),(80,10,0.3234,2.3210),(90,10,0.2891,2.5134),
    (100,10,0.2234,2.7823),(110,10,0.1891,3.0234),(118,180,0.5169,2.1982),
    (118,190,0.1873,3.2385),(118,200,0.4921,2.2424),(118,210,0.0873,2.7004),
    (118,220,0.0840,3.7983),(118,230,0.0263,4.2039),(118,240,0.0859,2.6956),
    (118,250,0.0933,3.9028),(118,260,0.9756,3.3295),(118,270,0.0683,3.5220),
]

xs       = [r[0] + r[1]/280 for r in raw]
d_losses = [r[2] for r in raw]
g_losses = [r[3] for r in raw]

epochs_u = sorted(set(r[0] for r in raw))
d_avg = [np.mean([r[2] for r in raw if r[0]==e]) for e in epochs_u]
g_avg = [np.mean([r[3] for r in raw if r[0]==e]) for e in epochs_u]

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))
fig.patch.set_facecolor('#FAFAFA')
for ax in [ax1, ax2]:
    ax.set_facecolor('#FAFAFA')

ax1.scatter(xs, d_losses, color='#EF5350', s=40, alpha=0.8, label='D Loss', zorder=3)
ax1.scatter(xs, g_losses, color='#1565C0', s=40, alpha=0.8, label='G Loss', zorder=3)
ax1.axhline(y=0.693, color='#FF6F00', linestyle='--', linewidth=1.5, label='ln(2) â‰ˆ 0.693')
ax1.set_xlabel('Epoch', fontsize=11)
ax1.set_ylabel('BCE Loss', fontsize=11)
ax1.set_title('Raw Training Log', fontsize=11, fontweight='bold')
ax1.legend(fontsize=9)

ax2.plot(epochs_u, d_avg, color='#EF5350', linewidth=2.5, marker='o', markersize=6, label='D Loss (avg)')
ax2.plot(epochs_u, g_avg, color='#1565C0', linewidth=2.5, marker='s', markersize=6,
         linestyle='--', label='G Loss (avg)')
ax2.axhline(y=0.693, color='#FF6F00', linestyle='--', linewidth=1.5, label='ln(2) â‰ˆ 0.693')
ax2.set_xlabel('Epoch', fontsize=11)
ax2.set_ylabel('BCE Loss', fontsize=11)
ax2.set_title('Per-Epoch Average Loss', fontsize=11, fontweight='bold')
ax2.legend(fontsize=9)
ax2.annotate('D dominates\n(D Loss â†’ 0)', xy=(118, 0.22), xytext=(75, 1.2),
             fontsize=9, color='#B71C1C', fontweight='bold',
             arrowprops=dict(arrowstyle='->', color='#B71C1C', lw=1.5))

plt.suptitle('Conditional GAN â€” Western Blot Generation Training Log', fontsize=12, fontweight='bold')
plt.tight_layout()
plt.show()
```
