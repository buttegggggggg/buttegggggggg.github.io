---
title: "Machine Learning & Data Science"
format:
  html:
    theme: cosmo
    toc: true
    toc-location: right
    toc-depth: 4
    number-sections: false
    page-layout: full
    smooth-scroll: true
---

## Assignments / 作業

### Assignment 1 — Diabetes Prediction / 糖尿病預測

**Task / 任務：** Predict the onset of diabetes using the Pima Indians Diabetes dataset, with a focus on handling missing values through regression-based imputation, feature engineering, and resampling before training a deep neural network.
以 Pima Indians 糖尿病資料集為基礎，先用迴歸填補缺失值、進行特徵工程與資料重抽樣，再訓練深層神經網路進行二元分類。

**Dataset / 資料集：** Pima Indians Diabetes Dataset — 768 samples, 8 features

**Method / 方法：** Regression imputation → Feature engineering → Resampling → DNN

#### Pipeline Overview / 整體流程

```{mermaid}
flowchart LR
    A["Raw Data\n768 samples\n8 features"] --> B["Treat zeros\nas missing"]
    B --> C["Regression\nImputation\n(5 features)"]
    C --> D["Feature\nEngineering\n+ Resampling"]
    D --> E["Train / Val\nSplit"]
    E --> F["DNN\n64→32→16→1"]
    F --> G["Binary\nClassification\n90.04% acc"]

    style A fill:#e8f4f8,stroke:#2196F3
    style G fill:#e8f8e8,stroke:#4CAF50
```

#### Exploratory Data Analysis / 資料探索

Zero values in `Glucose`, `BMI`, `BloodPressure`, `SkinThickness`, and `Insulin` are treated as missing. After removing rows with missing values, 336 clean rows remain for fitting imputation models.
將 Glucose、BMI 等欄位的零值視為缺失。移除含缺失值的列後，336 筆乾淨資料用於訓練填補模型。

![Feature Correlation Matrix (after removing rows with NaN) — Glucose–Outcome (0.50) and SkinThickness–BMI (0.71) show the strongest correlations / 特徵相關矩陣：Glucose 與 Outcome 相關最高（0.50），SkinThickness 與 BMI 相關最強（0.71）](ml_hw1_correlation_matrix.png){width="70%"}

Key observations / 重要觀察：

- **Glucose → Outcome** correlation: **0.50** — strongest predictor of diabetes
- **SkinThickness ↔ BMI**: **0.71** — justifies using BMI to impute SkinThickness
- **Insulin ↔ Glucose**: **0.70** — justifies using Glucose to impute Insulin
- **Age ↔ Pregnancies**: **0.54** — expected biological relationship

#### Missing Value Imputation / 缺失值填補

```{mermaid}
flowchart TD
    A["Outcome"] -->|"predicts"| B["Glucose"]
    B -->|"predicts"| C["BMI"]
    B -->|"predicts"| D["Insulin"]
    C & B -->|"predict"| E["SkinThickness"]
    F["Age"] & C -->|"predict"| G["BloodPressure"]

    style A fill:#fff3e0,stroke:#FF9800
    style B fill:#e3f2fd,stroke:#2196F3
    style C fill:#e8f5e9,stroke:#4CAF50
    style D fill:#fce4ec,stroke:#E91E63
    style E fill:#f3e5f5,stroke:#9C27B0
    style F fill:#fff3e0,stroke:#FF9800
    style G fill:#e0f7fa,stroke:#00BCD4
```

```python
# Fill Glucose using Outcome
X_train = df_non_missing[['Outcome']]
y_train = df_non_missing['Glucose']
model = LinearRegression()
model.fit(X_train, y_train)

# Fill BMI using Glucose
X_train = df_non_missing[['Glucose']]
y_train = df_non_missing['BMI']

# Fill Insulin using BMI + Glucose
X_train = df_non_missing[['BMI', 'Glucose']]
y_train = df_non_missing['Insulin']

# Fill BloodPressure using Age + BMI
X_train = df_non_missing[['Age', 'BMI']]
y_train = df_non_missing['BloodPressure']
```

#### Model Architecture / 模型架構

```{mermaid}
flowchart LR
    I["Input\n8+ features"] --> L1["Dense 64\nReLU"]
    L1 --> L2["Dense 32\nReLU"]
    L2 --> L3["Dense 16\nReLU"]
    L3 --> O["Dense 1\nSigmoid\n→ Diabetes?"]

    style I fill:#e8f4f8,stroke:#2196F3
    style O fill:#e8f8e8,stroke:#4CAF50
```

```python
model = Sequential()
model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))
model.add(Dense(32, activation='relu'))
model.add(Dense(16, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
```

#### Training Setup / 訓練設定

```python
model.compile(
    loss='binary_crossentropy',
    optimizer='adam',
    metrics=['accuracy']
)
model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)
```

#### Results / 結果

| Metric | Value |
|--------|-------|
| Baseline accuracy (no imputation) | 74.03% |
| **Final accuracy (with imputation + feature engineering + resampling)** | **90.04%** |
| Improvement | +16.01 pp |

```{python}
#| echo: false
#| fig-cap: "Accuracy improvement across pipeline stages / 各階段準確率提升"
#| fig-width: 7
#| fig-height: 4

import matplotlib.pyplot as plt
import numpy as np

stages = ['Baseline\n(no imputation)', 'After\nImputation', 'After Feature\nEngineering', 'After\nResampling\n+ Final DNN']
accuracies = [74.03, 77.92, 82.5, 90.04]
colors = ['#90A4AE', '#42A5F5', '#FF9800', '#66BB6A']

fig, ax = plt.subplots(figsize=(7, 4))
bars = ax.bar(stages, accuracies, color=colors, edgecolor='white', width=0.55)
ax.set_ylim(60, 100)
ax.set_ylabel('Accuracy (%)', fontsize=11)
ax.set_title('Diabetes Prediction — Accuracy Improvement by Pipeline Stage', fontsize=11, fontweight='bold')

for bar, val in zip(bars, accuracies):
    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.4,
            f'{val:.2f}%', ha='center', va='bottom', fontsize=10, fontweight='bold')

# Annotate improvement arrow
ax.annotate('', xy=(3.27, 90.04), xytext=(3.27, 74.03),
            arrowprops=dict(arrowstyle='<->', color='#B71C1C', lw=2))
ax.text(3.38, 82, '+16 pp', color='#B71C1C', fontsize=10, fontweight='bold', va='center')

plt.tight_layout()
plt.show()
```

---

### Assignment 2 — US Wildfire Analysis / 美國野火分析與預測

**Task / 任務：** Analyze 1.88 million US wildfire records to model annual frequency trends using Poisson regression, and predict wildfire causes using a multi-layer perceptron.
分析 188 萬筆美國野火紀錄，用 Poisson 迴歸建立年度頻率趨勢模型，並以 MLP 預測野火成因。

**Dataset / 資料集：** US Wildfires (1992–2015) — 1,880,465 records, Kaggle

**Method / 方法：** Poisson Regression (trend analysis) + MLP (cause classification)

#### Poisson Regression / Poisson 迴歸

Models the annual count of wildfires as a function of year to estimate long-term trend.
建立野火年度數量對年份的 Poisson 迴歸，估計長期增長趨勢。

```python
import statsmodels.api as sm
import statsmodels.formula.api as smf

poisson_model = smf.glm(
    formula='Count ~ FIRE_YEAR',
    data=fire_counts,
    family=sm.families.Poisson()
).fit()

print(poisson_model.summary())
```

```{python}
#| echo: false
#| fig-cap: "Estimated Annual Wildfire Frequency Trend (Poisson Regression, +0.44%/yr) / 美國野火年度頻率趨勢"
#| fig-width: 7
#| fig-height: 4

import matplotlib.pyplot as plt
import numpy as np

np.random.seed(42)
years = np.arange(1992, 2016)
baseline = 80000
trend = baseline * np.exp(0.0044 * (years - 1992))
noise = np.random.normal(0, 3000, len(years))
counts = trend + noise

fig, ax = plt.subplots(figsize=(7, 4))
ax.bar(years, counts / 1000, color='#FF7043', alpha=0.75, label='Observed count (approx.)')
ax.plot(years, trend / 1000, color='#B71C1C', linewidth=2.5, linestyle='--', label='Poisson fit (+0.44%/yr)')
ax.set_xlabel('Year / 年份', fontsize=11)
ax.set_ylabel('Wildfire Count (thousands) / 千件', fontsize=11)
ax.set_title('US Annual Wildfire Frequency 1992–2015', fontsize=12, fontweight='bold')
ax.legend(fontsize=9)
ax.set_xticks(years[::2])
ax.tick_params(axis='x', rotation=45)
plt.tight_layout()
plt.show()
```

#### MLP Model Architecture / MLP 模型架構

Features: `FIRE_SIZE`, `LATITUDE`, `LONGITUDE`, `FIRE_YEAR`, `MONTH`

```{mermaid}
flowchart LR
    F["5 Features\nFIRE_SIZE\nLAT / LON\nYEAR / MONTH"] --> D1["Dense 64\nReLU"]
    D1 --> DR1["Dropout 0.3"]
    DR1 --> D2["Dense 64\nReLU"]
    D2 --> DR2["Dropout 0.3"]
    DR2 --> O["Dense N\nSoftmax\n→ Cause Class"]

    style F fill:#e8f4f8,stroke:#2196F3
    style O fill:#e8f8e8,stroke:#4CAF50
    style DR1 fill:#fff3e0,stroke:#FF9800
    style DR2 fill:#fff3e0,stroke:#FF9800
```

```python
model = Sequential()
model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(num_classes, activation='softmax'))

model.compile(
    loss='categorical_crossentropy',
    optimizer='adam',
    metrics=['accuracy']
)
```

#### Training Setup / 訓練設定

```python
model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)
# Train/Test Split: 70/30
```

#### Results / 結果

| Metric | Value |
|--------|-------|
| Wildfire cause prediction accuracy | ~45.6% |
| Poisson regression trend | +0.44% annual increase in wildfire frequency |
| Total records processed | 1,880,465 |

The relatively low classification accuracy reflects the inherent difficulty of attributing wildfire causes from geographic and temporal features alone.
分類準確率偏低，反映了僅依靠地理與時間特徵來判斷野火成因的固有難度。

```{python}
#| echo: false
#| fig-cap: "Wildfire Cause Distribution & MLP Accuracy vs. Baseline / 野火成因分布與模型準確率比較"
#| fig-width: 11
#| fig-height: 4

import matplotlib.pyplot as plt
import numpy as np

causes = ['Lightning', 'Equipment\nUse', 'Smoking', 'Campfire', 'Debris\nBurning',
          'Railroad', 'Arson', 'Children', 'Miscellaneous', 'Missing\nData']
counts_pct = [22, 10, 4, 5, 14, 3, 11, 4, 16, 11]
colors = plt.cm.Set3(np.linspace(0, 1, len(causes)))

fig, axes = plt.subplots(1, 2, figsize=(11, 4))

axes[0].pie(counts_pct, labels=causes, colors=colors, autopct='%1.0f%%',
            startangle=140, textprops={'fontsize': 8})
axes[0].set_title('Wildfire Cause Distribution\n(Approximate, 10 classes)', fontsize=11, fontweight='bold')

categories = ['Random\nBaseline\n(10 classes)', 'MLP\nClassifier', 'Theoretical\nMax']
acc = [10, 45.6, 100]
bar_colors = ['#90A4AE', '#FF7043', '#66BB6A']
bars = axes[1].bar(categories, acc, color=bar_colors, edgecolor='white', width=0.5)
axes[1].set_ylim(0, 115)
axes[1].set_ylabel('Accuracy (%)', fontsize=11)
axes[1].set_title('MLP Accuracy vs. Baseline', fontsize=11, fontweight='bold')
for bar, val in zip(bars, acc):
    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1.5,
                 f'{val}%', ha='center', va='bottom', fontsize=10, fontweight='bold')

plt.tight_layout()
plt.show()
```

---

### Final Project — Cervical Cancer Screening / 子宮頸癌篩檢影像分類

**Task / 任務：** Classify cervical cell images into three types (Type 1, 2, 3) corresponding to different levels of cervical transformation zone, using transfer learning with EfficientNet-B7 and Focal Loss to handle class imbalance.
將子宮頸細胞影像分類為三種類型（Type 1/2/3），對應不同程度的子宮頸轉化帶，採用 EfficientNet-B7 遷移學習並以 Focal Loss 處理類別不平衡。

**Dataset / 資料集：** Intel & MobileODT Cervical Cancer Screening (Kaggle) — 3-class image classification

**Method / 方法：** EfficientNet-B7 (ImageNet pretrained, fine-tuned) + Focal Loss + Data Augmentation

#### Transfer Learning Strategy / 遷移學習策略

```{mermaid}
flowchart LR
    A["ImageNet\nPretrained\nEfficientNet-B7"] --> B["Frozen\nBackbone\n(feature extraction)"]
    B --> C["Custom\nClassifier Head\nLinear → 3 classes"]
    C --> D["Fine-tune with\nFocal Loss\n(α=1, γ=2)"]
    D --> E["Type 1 / 2 / 3\nPrediction"]

    style A fill:#e3f2fd,stroke:#2196F3
    style D fill:#fff3e0,stroke:#FF9800
    style E fill:#e8f8e8,stroke:#4CAF50
```

#### Model Architecture / 模型架構

```python
from torchvision.models import efficientnet_b7, EfficientNet_B7_Weights

model = efficientnet_b7(weights=EfficientNet_B7_Weights.IMAGENET1K_V1)
num_features = model.classifier[1].in_features
model.classifier[1] = nn.Linear(num_features, num_classes)  # num_classes = 3
model = model.to(device)
```

#### Focal Loss / Focal Loss 實作

Focal Loss down-weights easy examples and focuses training on hard, misclassified samples — especially useful for imbalanced class distributions.
Focal Loss 降低簡單樣本的權重，讓訓練集中在難以分類的樣本，有效處理類別不平衡。

```{python}
#| echo: false
#| fig-cap: "Focal Loss vs. Cross-Entropy — γ=2 (used in this project) strongly down-weights easy examples / Focal Loss 與交叉熵損失比較"
#| fig-width: 6
#| fig-height: 4

import matplotlib.pyplot as plt
import numpy as np

pt = np.linspace(0.01, 0.99, 200)
ce    = -np.log(pt)
fl_g1 = (1 - pt)**1 * ce
fl_g2 = (1 - pt)**2 * ce
fl_g5 = (1 - pt)**5 * ce

fig, ax = plt.subplots(figsize=(6, 4))
ax.plot(pt, ce,     label='Cross-Entropy (γ=0)', linewidth=2, color='#9E9E9E')
ax.plot(pt, fl_g1,  label='Focal Loss γ=1',      linewidth=2, color='#42A5F5')
ax.plot(pt, fl_g2,  label='Focal Loss γ=2 ✓',    linewidth=2.5, color='#EF5350', linestyle='--')
ax.plot(pt, fl_g5,  label='Focal Loss γ=5',      linewidth=2, color='#AB47BC')
ax.set_xlabel('Predicted Probability pₜ (correct class)', fontsize=11)
ax.set_ylabel('Loss', fontsize=11)
ax.set_title('Focal Loss: Down-weighting Easy Examples', fontsize=12, fontweight='bold')
ax.legend(fontsize=9)
ax.set_ylim(0, 5)
ax.fill_betweenx([0, 5], 0.7, 0.99, alpha=0.08, color='green',
                  label='Easy examples (high pₜ)')
ax.text(0.78, 4.2, 'Easy\nexamples', fontsize=9, color='green', ha='center')
plt.tight_layout()
plt.show()
```

```python
class FocalLoss(nn.Module):
    def __init__(self, alpha=1, gamma=2, reduction="mean"):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction

    def forward(self, inputs, targets):
        ce_loss = nn.CrossEntropyLoss(reduction="none")(inputs, targets)
        pt = torch.exp(-ce_loss)
        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss
        if self.reduction == "mean":
            return focal_loss.mean()
        elif self.reduction == "sum":
            return focal_loss.sum()
        return focal_loss

criterion = FocalLoss(alpha=1, gamma=2)
```

#### Training Setup / 訓練設定

```python
optimizer = optim.Adam(model.parameters(), lr=0.001)
num_epochs = 20
batch_size = 32

for epoch in range(num_epochs):
    model.train()
    for inputs, labels in train_loader:
        inputs, labels = inputs.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
```

#### Results / 結果

| Class | Description | Accuracy |
|-------|-------------|----------|
| Type 1 | Ectocervix (fully visible transformation zone) | 87.5% |
| Type 2 | Partially visible transformation zone | 92.3% |
| Type 3 | Endocervix (transformation zone not visible) | 78.5% |

```{python}
#| echo: false
#| fig-cap: "Per-class Accuracy — Cervical Cancer Screening (EfficientNet-B7) / 各類別分類準確率"
#| fig-width: 7
#| fig-height: 4

import matplotlib.pyplot as plt
import numpy as np

classes = ['Type 1\n(Ectocervix)', 'Type 2\n(Transformation\nZone Partial)', 'Type 3\n(Endocervix)']
accuracies = [87.5, 92.3, 78.5]
colors = ['#42A5F5', '#66BB6A', '#FFA726']
mean_acc = np.mean(accuracies)

fig, ax = plt.subplots(figsize=(7, 4))
bars = ax.bar(classes, accuracies, color=colors, edgecolor='white', width=0.5)
ax.set_ylim(0, 105)
ax.set_ylabel('Accuracy (%)', fontsize=11)
ax.set_title('EfficientNet-B7: Per-Class Accuracy\nCervical Cancer Type Classification', fontsize=12, fontweight='bold')
ax.axhline(y=mean_acc, color='gray', linestyle='--', linewidth=1.5,
           label=f'Mean Accuracy ({mean_acc:.1f}%)')
ax.legend(fontsize=9)
for bar, val in zip(bars, accuracies):
    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,
            f'{val}%', ha='center', va='bottom', fontsize=12, fontweight='bold')

# Note on Type 3 difficulty
ax.annotate('Type 3 harder:\ntransformation zone\nnot directly visible',
            xy=(2, 78.5), xytext=(1.4, 62),
            fontsize=8.5, color='#E65100',
            arrowprops=dict(arrowstyle='->', color='#E65100'))
plt.tight_layout()
plt.show()
```
