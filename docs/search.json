[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This site is a working portfolio and notebook.\nIt documents selected projects, visual studies, and experiments as they evolve.\nNot all content represents final results. Some materials are exploratory or incomplete."
  },
  {
    "objectID": "EPPS6356.html",
    "href": "EPPS6356.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Assignment 1\n\n\n\nAssignment 2\n\n\n\nAssignment 3\n\n\n\n\ndata(iris)\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.1     ✔ stringr   1.5.2\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# 1. Divide the dataset into three rectangles based on species.\n# The average of Petal.Length and Petal.Width is the length and width.\n# Draw three rectangles arranged horizontally.\n\n#1\n\nplot_data &lt;- iris %&gt;%\n  mutate(\n    sepal_length_group = cut(\n      Sepal.Length,\n      breaks = c(4, 5.5, 7.0, 8.0),\n      labels = c(\"Small (4.0-5.5)\", \"Medium (5.6-7.0)\", \"Large (7.1-8.0)\"),\n      include.lowest = TRUE\n    )\n  ) %&gt;%\n  group_by(sepal_length_group) %&gt;%\n  summarise(\n    count = n(),\n    avg_petal_length = mean(Petal.Length)\n  ) %&gt;%\n  mutate(\n    xmax = cumsum(count),\n    xmin = xmax - count,\n    x_label_pos = (xmin + xmax) / 2\n  )\n\nggplot(plot_data, aes(ymin = 0)) +\n  geom_rect(\n    aes(\n      xmin = xmin,\n      xmax = xmax,\n      ymax = avg_petal_length,\n      fill = sepal_length_group\n    ),\n    color = \"white\"\n  ) +\n  scale_x_continuous(\n    breaks = plot_data$x_label_pos,\n    labels = plot_data$sepal_length_group,\n    expand = c(0, 0)\n  ) +\n  scale_fill_viridis_d(option = \"D\", direction = -1) +\n  labs(\n    title = \"Average Petal Length by Sepal Length Group\",\n    subtitle = \"Column width is proportional to the number of flowers in each group\",\n    x = \"Count of Flowers in Group\",\n    y = \"Average Petal Length (cm)\",\n    fill = \"Sepal Length Group\"\n  ) +\n  # Apply a clean theme\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 18),\n    legend.position = \"bottom\",\n    panel.grid.major.x = element_blank(), # Remove vertical grid lines\n    panel.grid.minor.x = element_blank()\n  )\n\n\n\n\n\n\n\n\n\n# 2. table with embedded charts\niris_long &lt;- iris %&gt;%\n  pivot_longer(cols = -Species, names_to = \"Measurement\", values_to = \"Value\")\n\nggplot(iris_long, aes(x = Value, fill = Species)) +\n  geom_histogram(color = \"white\", bins = 15) +\n  facet_grid(Species ~ Measurement, scales = \"free\") +\n  scale_fill_manual( #coloring each species\n    values = c(\n      \"setosa\" = \"steelblue\", \n      \"versicolor\" = \"orange\",   \n      \"virginica\" = \"seagreen\"     \n    ) \n    ) + #labels\n      labs(\n        title = \"Distribution of Iris Measurements by Species\",\n        x = \"Measurement Value (cm)\",\n        y = \"Count\"\n      ) +\n  theme_bw() +\n    theme(\n      plot.title = element_text(hjust = 0.5, face = \"bold\"),\n      strip.text.x = element_text(face = \"bold\"),\n      strip.text.y = element_text(face = \"bold\"),\n      panel.border = element_rect(color = \"grey80\", fill = NA),\n      legend.position = \"bottom\"\n    )\n\n\n\n\n\n\n\n\n\n# 3. Extract setona and versicolor from species.\n# Then create df_2 and df_3. Draw a bar plot using petal.width: p1 p2.\n# Finally, use gridExtra to combine the plots.'\nlibrary(\"gridExtra\")\n\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\ndf_2 &lt;- subset(iris, Species %in% \"setosa\")\ndf_3 &lt;- subset(iris, Species %in% \"versicolor\")\ndf_2$id &lt;- 1:nrow(df_2)\ndf_3$id &lt;- 1:nrow(df_3)\n\n\n\np1 = ggplot(df_2, aes(x = factor(id), y = Petal.Width)) +\n  geom_bar(stat = \"identity\", fill = 'red', color = \"black\") +\n  coord_flip() +\n  labs(title = \"setosa\") +\n  theme(\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank() #this was by GPT\n  )\n\n\np2 = ggplot(df_3, aes(x = factor(id), y = Petal.Width)) +\n  geom_bar(stat = \"identity\", fill = \"blue\", color = \"black\") +\n  coord_flip() +\n  labs(title = \"versicolor\")+\n  theme(\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank() #this was by GPT\n  )\n\n\n\ngridExtra::grid.arrange(p1, p2, ncol = 2)\n\n\n\n\n\n\n\n\n\n# 4 Column Chart\n# getting means of Petal length and width for each species\n# and mean sepal length and sepal width\niris_means &lt;- iris %&gt;%\n  group_by(Species) %&gt;%\n  summarise(\n    mean_sepal_length = mean(Sepal.Length),\n    mean_sepal_width = mean(Sepal.Width),\n    mean_petal_length = mean(Petal.Length),\n    mean_petal_width = mean(Petal.Width)\n  ) %&gt;%\n  pivot_longer(\n    cols = -Species,\n    names_to = \"Measurement\",\n    values_to = \"MeanValue\"\n    )\n\nggplot(iris_means, aes(x = Measurement, y = MeanValue, fill = Species)) +\n  geom_col(position = position_dodge(width = 0.8)) + \n  labs(title = \"Mean Iris Measurements by Species\",\n       x = \"Measurement\", y = \"Mean Value\") + \n  theme_minimal(base_size = 12) +\n  scale_fill_manual(values = c(\"steelblue\", \"orange\", \"seagreen\"))\n\n\n\n\n\n\n\n\n\nClass coding competition\n\n\nlibrary(ggplot2)\nmpg &lt;- as.data.frame(mpg)\n#2seater, compact, midsize, minivan, pickup, subcompact, suv scatterplots in one view\nggplot(mpg, aes(x=displ, y=hwy)) +\n  geom_point(color = \"black\") +\n  facet_wrap(~ class) +\n  labs(x=\"displ\",\n       y=\"hwy\") +\n  theme_gray()\n\n\n\n\n\n\n\n\n\n#improving the chart\nggplot(mpg, aes(x=displ, y=hwy)) +\n  geom_point(color = \"blue\", size=2, alpha=0.3) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"#E65100\", linewidth = 0.8) +\n  facet_wrap(~ class) +\n  labs(title=\"Engine Displacement vs Highway MPG by Vehicle Class\",\n       x=\"Engine Displacement (liters)\",\n       y=\"Highway Miles per Gallon (MPG)\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size=16, face=\"bold\"),\n    axis.title.x = element_text(size=12),\n    axis.title.y = element_text(size=12)\n  )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n# GPT was used for picking colors and family.\n# GPT was used for adjusting the format of the code.\nlibrary(ggplot2)\nlibrary(scales)   # for alpha()\n\n\nAttaching package: 'scales'\n\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\ndata(iris)\ncolor1 &lt;- \"#18A3A380\"\ncolor2 &lt;- \"#FF4D8DCC\"\ncolor3 &lt;- \"#7A7A7A\"\ncolor4 &lt;- \"#000000\"\nbase_family &lt;- \"sans\"\n\n# custom theme used across plots\ntheme1 &lt;- function() {\n  theme_minimal(base_family = base_family) +\n    theme(\n      text        = element_text(family = base_family, colour = color4),\n      plot.title  = element_text(face = \"bold\", colour = color4, size = 13),\n      axis.title  = element_text(colour = color4),\n      axis.text   = element_text(colour = color3),\n      panel.grid.major = element_line(color = scales::alpha(color3, 0.3), linetype = \"dotted\"),\n      panel.grid.minor = element_blank()\n    )\n}\n\n\nHisto &lt;- function(){\n  hist(iris$Sepal.Length,\n       main=\"Distribution of Sepal Length (iris)\",\n       col=color1, border=color3)\n}\n\nBar1 &lt;- function(){\n  barplot(table(iris$Species),\n          col=c(color1,color2,color3),\n          border=color4,\n          main=\"Count by Species\",\n          xlab=\"Species\", ylab=\"Count\")\n}\n\nBar2 &lt;- function(){\n  barplot(table(iris$Species),\n          horiz=TRUE,\n          col=c(color1,color2,color3),\n          border=color4,\n          main=\"Count by Species (Horizontal)\",\n          xlab=\"Count\", ylab=\"Species\")\n}\n\nPie &lt;- function(){\n  pie(table(iris$Species),\n      col=c(color1,color2,color3),\n      main=\"Species Composition\",\n      clockwise=TRUE)\n}\n\nBox &lt;- function(){\n  boxplot(Sepal.Length~Species, data=iris,\n          col=c(color1,color2,color3),\n          main=\"Sepal Length by Species\",\n          xlab=\"Species\", ylab=\"Sepal Length (cm)\")\n}\n\nScat &lt;- function(){\n  plot(iris$Petal.Length, iris$Sepal.Length,\n       main=\"Sepal vs Petal Length\",\n       xlab=\"Petal Length (cm)\", ylab=\"Sepal Length (cm)\",\n       pch=19, col=color1)\n}\n\n\nlibrary(gridExtra)\n\npar(mfrow=c(2,3), mar=c(4,4,2.5,1), family=\"sans\")\nHisto(); Bar1(); Bar2(); Pie(); Box(); Scat()\n\n\n\n\n\n\n\n\n\ndraw6 &lt;- function(){\n  par(mfrow=c(2,3), mar=c(4,4,2.5,1), family=base_family)\n  Histo(); Bar1(); Bar2(); Pie(); Box(); Scat()\n}\n\nsave_plot &lt;- function(fmt, file){\n  switch(fmt,\n    pdf  = pdf(file, width=10, height=7, family=base_family),\n    jpg  = jpeg(file, width=2400, height=1600, res=300, quality=95),\n    svg  = svg(file, width=2400, height=1600),\n    tiff = tiff(file, width=2400, height=1600, res=300),\n    bmp  = bmp(file, width=2400, height=1600, res=300), # cannot find bmg, and was told it might be .bmp by GPT\n  )\n  draw6(); invisible(dev.off())\n}\n\n\nsave_plot(\"pdf\",  \"/Users/buttegg/Documents/IAmTryingToUseQuartoDoSthCool/buttegggggggg.github.io/baseR_6plots.pdf\")\nsave_plot(\"jpg\",  \"/Users/buttegg/Documents/IAmTryingToUseQuartoDoSthCool/buttegggggggg.github.io/baseR_6plots.jpg\")\nsave_plot(\"svg\",  \"/Users/buttegg/Documents/IAmTryingToUseQuartoDoSthCool/buttegggggggg.github.io/baseR_6plots.svg\")\nsave_plot(\"tiff\", \"/Users/buttegg/Documents/IAmTryingToUseQuartoDoSthCool/buttegggggggg.github.io/baseR_6plots.tiff\")\nsave_plot(\"bmp\",  \"/Users/buttegg/Documents/IAmTryingToUseQuartoDoSthCool/buttegggggggg.github.io/baseR_6plots.bmp\")\n\nPDF of base R plots\nJPG of base R plots\nSVG of base R plots\nTIFF of base R plots\nBMP of base R plots\n\nggHisto &lt;- ggplot(iris, aes(x=Sepal.Length)) +\n  geom_histogram(fill=color1, color=color3, bins=20) +\n  labs(title=\"Distribution of Sepal Length (iris)\") +\n  theme1()\n\nggBar1 &lt;- ggplot(iris, aes(x=Species, fill=Species)) +\n  geom_bar(color=color4) +\n  scale_fill_manual(values=c(color1,color2,color3)) +\n  labs(title=\"Count by Species\") +\n  theme1() + ggplot2::theme(legend.position=\"none\")\n\nggBar2 &lt;- ggplot(iris, aes(x=Species, fill=Species)) +\n  geom_bar(color=color4) + coord_flip() +\n  scale_fill_manual(values=c(color1,color2,color3)) +\n  labs(title=\"Count by Species (Horizontal)\") +\n  theme1() + ggplot2::theme(legend.position=\"none\")\n\ndf &lt;- as.data.frame(prop.table(table(iris$Species)))\ncolnames(df) &lt;- c(\"Species\",\"prop\")\nggPie &lt;- ggplot(df, aes(x=\"\", y=prop, fill=Species)) +\n  geom_col(width=1, color=NA) + coord_polar(theta=\"y\") +\n  scale_fill_manual(values=c(color1,color2,color3)) +\n  labs(title=\"Species Composition\") +\n  theme1() + ggplot2::theme(axis.text=ggplot2::element_blank(),\n                            axis.title=ggplot2::element_blank(),\n                            panel.grid=ggplot2::element_blank(),\n                            legend.position=\"right\")\n\nggBox &lt;- ggplot(iris, aes(x=Species, y=Sepal.Length, fill=Species)) +\n  geom_boxplot(color=color4) +\n  scale_fill_manual(values=c(color1,color2,color3)) +\n  labs(title=\"Sepal Length by Species\") +\n  theme1() + ggplot2::theme(legend.position=\"none\")\n\nggScat &lt;- ggplot(iris, aes(x=Petal.Length, y=Sepal.Length)) +\n  geom_point(color=color1, size=2) +\n  labs(title=\"Sepal vs Petal Length\") +\n  theme1()\n\n\ngridExtra::grid.arrange(ggHisto, ggBar1, ggBar2, ggPie, ggBox, ggScat, ncol = 3)\n\n\n\n\n\n\n\n\n\ncombo &lt;- gridExtra::arrangeGrob(ggHisto, ggBar1, ggBar2, ggPie, ggBox, ggScat, ncol = 3)\noutdir &lt;- \"/Users/buttegg/Documents/IAmTryingToUseQuartoDoSthCool/buttegggggggg.github.io\"\nformats &lt;- c(\"pdf\", \"jpg\", \"svg\", \"tiff\", \"bmp\")\n\nfor (fmt in formats) {\n  outpath &lt;- file.path(outdir, paste0(\"ggplot_6plots.\", fmt))\n  ggplot2::ggsave(\n    filename = outpath,\n    plot = combo,\n    width = 10, height = 7, dpi = 300\n  )\n}\n\nPDF of ggplot2 plots\nJPG of ggplot2 plots\nSVG of ggplot2 plots\nTIFF of ggplot2 plots\nBMP of ggplot2 plots"
  },
  {
    "objectID": "EPPS6356.html#assignments",
    "href": "EPPS6356.html#assignments",
    "title": "Data Visualization",
    "section": "",
    "text": "Assignment 1\n\n\n\nAssignment 2\n\n\n\nAssignment 3\n\n\n\n\ndata(iris)\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.1     ✔ stringr   1.5.2\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# 1. Divide the dataset into three rectangles based on species.\n# The average of Petal.Length and Petal.Width is the length and width.\n# Draw three rectangles arranged horizontally.\n\n#1\n\nplot_data &lt;- iris %&gt;%\n  mutate(\n    sepal_length_group = cut(\n      Sepal.Length,\n      breaks = c(4, 5.5, 7.0, 8.0),\n      labels = c(\"Small (4.0-5.5)\", \"Medium (5.6-7.0)\", \"Large (7.1-8.0)\"),\n      include.lowest = TRUE\n    )\n  ) %&gt;%\n  group_by(sepal_length_group) %&gt;%\n  summarise(\n    count = n(),\n    avg_petal_length = mean(Petal.Length)\n  ) %&gt;%\n  mutate(\n    xmax = cumsum(count),\n    xmin = xmax - count,\n    x_label_pos = (xmin + xmax) / 2\n  )\n\nggplot(plot_data, aes(ymin = 0)) +\n  geom_rect(\n    aes(\n      xmin = xmin,\n      xmax = xmax,\n      ymax = avg_petal_length,\n      fill = sepal_length_group\n    ),\n    color = \"white\"\n  ) +\n  scale_x_continuous(\n    breaks = plot_data$x_label_pos,\n    labels = plot_data$sepal_length_group,\n    expand = c(0, 0)\n  ) +\n  scale_fill_viridis_d(option = \"D\", direction = -1) +\n  labs(\n    title = \"Average Petal Length by Sepal Length Group\",\n    subtitle = \"Column width is proportional to the number of flowers in each group\",\n    x = \"Count of Flowers in Group\",\n    y = \"Average Petal Length (cm)\",\n    fill = \"Sepal Length Group\"\n  ) +\n  # Apply a clean theme\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 18),\n    legend.position = \"bottom\",\n    panel.grid.major.x = element_blank(), # Remove vertical grid lines\n    panel.grid.minor.x = element_blank()\n  )\n\n\n\n\n\n\n\n\n\n# 2. table with embedded charts\niris_long &lt;- iris %&gt;%\n  pivot_longer(cols = -Species, names_to = \"Measurement\", values_to = \"Value\")\n\nggplot(iris_long, aes(x = Value, fill = Species)) +\n  geom_histogram(color = \"white\", bins = 15) +\n  facet_grid(Species ~ Measurement, scales = \"free\") +\n  scale_fill_manual( #coloring each species\n    values = c(\n      \"setosa\" = \"steelblue\", \n      \"versicolor\" = \"orange\",   \n      \"virginica\" = \"seagreen\"     \n    ) \n    ) + #labels\n      labs(\n        title = \"Distribution of Iris Measurements by Species\",\n        x = \"Measurement Value (cm)\",\n        y = \"Count\"\n      ) +\n  theme_bw() +\n    theme(\n      plot.title = element_text(hjust = 0.5, face = \"bold\"),\n      strip.text.x = element_text(face = \"bold\"),\n      strip.text.y = element_text(face = \"bold\"),\n      panel.border = element_rect(color = \"grey80\", fill = NA),\n      legend.position = \"bottom\"\n    )\n\n\n\n\n\n\n\n\n\n# 3. Extract setona and versicolor from species.\n# Then create df_2 and df_3. Draw a bar plot using petal.width: p1 p2.\n# Finally, use gridExtra to combine the plots.'\nlibrary(\"gridExtra\")\n\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\ndf_2 &lt;- subset(iris, Species %in% \"setosa\")\ndf_3 &lt;- subset(iris, Species %in% \"versicolor\")\ndf_2$id &lt;- 1:nrow(df_2)\ndf_3$id &lt;- 1:nrow(df_3)\n\n\n\np1 = ggplot(df_2, aes(x = factor(id), y = Petal.Width)) +\n  geom_bar(stat = \"identity\", fill = 'red', color = \"black\") +\n  coord_flip() +\n  labs(title = \"setosa\") +\n  theme(\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank() #this was by GPT\n  )\n\n\np2 = ggplot(df_3, aes(x = factor(id), y = Petal.Width)) +\n  geom_bar(stat = \"identity\", fill = \"blue\", color = \"black\") +\n  coord_flip() +\n  labs(title = \"versicolor\")+\n  theme(\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank() #this was by GPT\n  )\n\n\n\ngridExtra::grid.arrange(p1, p2, ncol = 2)\n\n\n\n\n\n\n\n\n\n# 4 Column Chart\n# getting means of Petal length and width for each species\n# and mean sepal length and sepal width\niris_means &lt;- iris %&gt;%\n  group_by(Species) %&gt;%\n  summarise(\n    mean_sepal_length = mean(Sepal.Length),\n    mean_sepal_width = mean(Sepal.Width),\n    mean_petal_length = mean(Petal.Length),\n    mean_petal_width = mean(Petal.Width)\n  ) %&gt;%\n  pivot_longer(\n    cols = -Species,\n    names_to = \"Measurement\",\n    values_to = \"MeanValue\"\n    )\n\nggplot(iris_means, aes(x = Measurement, y = MeanValue, fill = Species)) +\n  geom_col(position = position_dodge(width = 0.8)) + \n  labs(title = \"Mean Iris Measurements by Species\",\n       x = \"Measurement\", y = \"Mean Value\") + \n  theme_minimal(base_size = 12) +\n  scale_fill_manual(values = c(\"steelblue\", \"orange\", \"seagreen\"))\n\n\n\n\n\n\n\n\n\nClass coding competition\n\n\nlibrary(ggplot2)\nmpg &lt;- as.data.frame(mpg)\n#2seater, compact, midsize, minivan, pickup, subcompact, suv scatterplots in one view\nggplot(mpg, aes(x=displ, y=hwy)) +\n  geom_point(color = \"black\") +\n  facet_wrap(~ class) +\n  labs(x=\"displ\",\n       y=\"hwy\") +\n  theme_gray()\n\n\n\n\n\n\n\n\n\n#improving the chart\nggplot(mpg, aes(x=displ, y=hwy)) +\n  geom_point(color = \"blue\", size=2, alpha=0.3) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"#E65100\", linewidth = 0.8) +\n  facet_wrap(~ class) +\n  labs(title=\"Engine Displacement vs Highway MPG by Vehicle Class\",\n       x=\"Engine Displacement (liters)\",\n       y=\"Highway Miles per Gallon (MPG)\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size=16, face=\"bold\"),\n    axis.title.x = element_text(size=12),\n    axis.title.y = element_text(size=12)\n  )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n# GPT was used for picking colors and family.\n# GPT was used for adjusting the format of the code.\nlibrary(ggplot2)\nlibrary(scales)   # for alpha()\n\n\nAttaching package: 'scales'\n\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\ndata(iris)\ncolor1 &lt;- \"#18A3A380\"\ncolor2 &lt;- \"#FF4D8DCC\"\ncolor3 &lt;- \"#7A7A7A\"\ncolor4 &lt;- \"#000000\"\nbase_family &lt;- \"sans\"\n\n# custom theme used across plots\ntheme1 &lt;- function() {\n  theme_minimal(base_family = base_family) +\n    theme(\n      text        = element_text(family = base_family, colour = color4),\n      plot.title  = element_text(face = \"bold\", colour = color4, size = 13),\n      axis.title  = element_text(colour = color4),\n      axis.text   = element_text(colour = color3),\n      panel.grid.major = element_line(color = scales::alpha(color3, 0.3), linetype = \"dotted\"),\n      panel.grid.minor = element_blank()\n    )\n}\n\n\nHisto &lt;- function(){\n  hist(iris$Sepal.Length,\n       main=\"Distribution of Sepal Length (iris)\",\n       col=color1, border=color3)\n}\n\nBar1 &lt;- function(){\n  barplot(table(iris$Species),\n          col=c(color1,color2,color3),\n          border=color4,\n          main=\"Count by Species\",\n          xlab=\"Species\", ylab=\"Count\")\n}\n\nBar2 &lt;- function(){\n  barplot(table(iris$Species),\n          horiz=TRUE,\n          col=c(color1,color2,color3),\n          border=color4,\n          main=\"Count by Species (Horizontal)\",\n          xlab=\"Count\", ylab=\"Species\")\n}\n\nPie &lt;- function(){\n  pie(table(iris$Species),\n      col=c(color1,color2,color3),\n      main=\"Species Composition\",\n      clockwise=TRUE)\n}\n\nBox &lt;- function(){\n  boxplot(Sepal.Length~Species, data=iris,\n          col=c(color1,color2,color3),\n          main=\"Sepal Length by Species\",\n          xlab=\"Species\", ylab=\"Sepal Length (cm)\")\n}\n\nScat &lt;- function(){\n  plot(iris$Petal.Length, iris$Sepal.Length,\n       main=\"Sepal vs Petal Length\",\n       xlab=\"Petal Length (cm)\", ylab=\"Sepal Length (cm)\",\n       pch=19, col=color1)\n}\n\n\nlibrary(gridExtra)\n\npar(mfrow=c(2,3), mar=c(4,4,2.5,1), family=\"sans\")\nHisto(); Bar1(); Bar2(); Pie(); Box(); Scat()\n\n\n\n\n\n\n\n\n\ndraw6 &lt;- function(){\n  par(mfrow=c(2,3), mar=c(4,4,2.5,1), family=base_family)\n  Histo(); Bar1(); Bar2(); Pie(); Box(); Scat()\n}\n\nsave_plot &lt;- function(fmt, file){\n  switch(fmt,\n    pdf  = pdf(file, width=10, height=7, family=base_family),\n    jpg  = jpeg(file, width=2400, height=1600, res=300, quality=95),\n    svg  = svg(file, width=2400, height=1600),\n    tiff = tiff(file, width=2400, height=1600, res=300),\n    bmp  = bmp(file, width=2400, height=1600, res=300), # cannot find bmg, and was told it might be .bmp by GPT\n  )\n  draw6(); invisible(dev.off())\n}\n\n\nsave_plot(\"pdf\",  \"/Users/buttegg/Documents/IAmTryingToUseQuartoDoSthCool/buttegggggggg.github.io/baseR_6plots.pdf\")\nsave_plot(\"jpg\",  \"/Users/buttegg/Documents/IAmTryingToUseQuartoDoSthCool/buttegggggggg.github.io/baseR_6plots.jpg\")\nsave_plot(\"svg\",  \"/Users/buttegg/Documents/IAmTryingToUseQuartoDoSthCool/buttegggggggg.github.io/baseR_6plots.svg\")\nsave_plot(\"tiff\", \"/Users/buttegg/Documents/IAmTryingToUseQuartoDoSthCool/buttegggggggg.github.io/baseR_6plots.tiff\")\nsave_plot(\"bmp\",  \"/Users/buttegg/Documents/IAmTryingToUseQuartoDoSthCool/buttegggggggg.github.io/baseR_6plots.bmp\")\n\nPDF of base R plots\nJPG of base R plots\nSVG of base R plots\nTIFF of base R plots\nBMP of base R plots\n\nggHisto &lt;- ggplot(iris, aes(x=Sepal.Length)) +\n  geom_histogram(fill=color1, color=color3, bins=20) +\n  labs(title=\"Distribution of Sepal Length (iris)\") +\n  theme1()\n\nggBar1 &lt;- ggplot(iris, aes(x=Species, fill=Species)) +\n  geom_bar(color=color4) +\n  scale_fill_manual(values=c(color1,color2,color3)) +\n  labs(title=\"Count by Species\") +\n  theme1() + ggplot2::theme(legend.position=\"none\")\n\nggBar2 &lt;- ggplot(iris, aes(x=Species, fill=Species)) +\n  geom_bar(color=color4) + coord_flip() +\n  scale_fill_manual(values=c(color1,color2,color3)) +\n  labs(title=\"Count by Species (Horizontal)\") +\n  theme1() + ggplot2::theme(legend.position=\"none\")\n\ndf &lt;- as.data.frame(prop.table(table(iris$Species)))\ncolnames(df) &lt;- c(\"Species\",\"prop\")\nggPie &lt;- ggplot(df, aes(x=\"\", y=prop, fill=Species)) +\n  geom_col(width=1, color=NA) + coord_polar(theta=\"y\") +\n  scale_fill_manual(values=c(color1,color2,color3)) +\n  labs(title=\"Species Composition\") +\n  theme1() + ggplot2::theme(axis.text=ggplot2::element_blank(),\n                            axis.title=ggplot2::element_blank(),\n                            panel.grid=ggplot2::element_blank(),\n                            legend.position=\"right\")\n\nggBox &lt;- ggplot(iris, aes(x=Species, y=Sepal.Length, fill=Species)) +\n  geom_boxplot(color=color4) +\n  scale_fill_manual(values=c(color1,color2,color3)) +\n  labs(title=\"Sepal Length by Species\") +\n  theme1() + ggplot2::theme(legend.position=\"none\")\n\nggScat &lt;- ggplot(iris, aes(x=Petal.Length, y=Sepal.Length)) +\n  geom_point(color=color1, size=2) +\n  labs(title=\"Sepal vs Petal Length\") +\n  theme1()\n\n\ngridExtra::grid.arrange(ggHisto, ggBar1, ggBar2, ggPie, ggBox, ggScat, ncol = 3)\n\n\n\n\n\n\n\n\n\ncombo &lt;- gridExtra::arrangeGrob(ggHisto, ggBar1, ggBar2, ggPie, ggBox, ggScat, ncol = 3)\noutdir &lt;- \"/Users/buttegg/Documents/IAmTryingToUseQuartoDoSthCool/buttegggggggg.github.io\"\nformats &lt;- c(\"pdf\", \"jpg\", \"svg\", \"tiff\", \"bmp\")\n\nfor (fmt in formats) {\n  outpath &lt;- file.path(outdir, paste0(\"ggplot_6plots.\", fmt))\n  ggplot2::ggsave(\n    filename = outpath,\n    plot = combo,\n    width = 10, height = 7, dpi = 300\n  )\n}\n\nPDF of ggplot2 plots\nJPG of ggplot2 plots\nSVG of ggplot2 plots\nTIFF of ggplot2 plots\nBMP of ggplot2 plots"
  },
  {
    "objectID": "EPPS6356.html#reviews",
    "href": "EPPS6356.html#reviews",
    "title": "Data Visualization",
    "section": "Reviews",
    "text": "Reviews\n\nReview: Inge Druckrey – Teaching to See\nReview: Journalism in the Age of Data\nReview: The Future of Data Analysis\nReview: Data Visualization and Data Science\nReview: The Week in Charts & 2024 The Year in Charts"
  },
  {
    "objectID": "EPPS6356.html#notes",
    "href": "EPPS6356.html#notes",
    "title": "Data Visualization",
    "section": "Notes",
    "text": "Notes\n\nNotes: Big Data Pitfalls"
  },
  {
    "objectID": "EPPS6356.html#checklist",
    "href": "EPPS6356.html#checklist",
    "title": "Data Visualization",
    "section": "Checklist",
    "text": "Checklist\n\nTzu-Yuan’s Data Guide Checklist"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "This is Tzu-Yuan’s Quarto website"
  },
  {
    "objectID": "about.html#about-this-site",
    "href": "about.html#about-this-site",
    "title": "About",
    "section": "",
    "text": "This site is a working portfolio and notebook.\nIt documents selected projects, visual studies, and experiments as they evolve.\nNot all content represents final results. Some materials are exploratory or incomplete."
  },
  {
    "objectID": "EPPS6354.html",
    "href": "EPPS6354.html",
    "title": "Information Management",
    "section": "",
    "text": "Name and describe three applications you have used that employed a database system to store and access persistent data. (e.g. airlines, online trade, banking, university system)\nFor the first question, one example that comes to mind is video games. In video games, a player’s level and experience points, as well as the items and equipment they have obtained, are recorded, so the player can still access them the next time they log in. Another example is online shopping. For instance, Amazon records information such as the price of each product, the catalog it belongs to, whether it is eligible for free shipping, and whether it is in stock. A third example is a streaming platform, such as Netflix, which records a user’s region and subscription level. All of this data is stored persistently and can be accessed at a later time.\n\n\n\nPropose three applications in domain projects (e.g. criminology, economics, brain science, etc.) Be sure you include: i. Purpose ii. Functions iii. Simple interface design\n\n\n\n\nThe main purpose of this wardrobe management database is to minimize the time spent choosing outfits before going out.\nFor many people, the difficulty in daily outfit selection is not a lack of clothing, but the need to simultaneously consider colors, styles, occasions, and overall coordination, which leads to a high decision-making cost.\nTherefore, I model the wardrobe as a relational database, which not only records individual clothing items but also describes the relationships between items, allowing outfit selection to be handled in a systematic way.\nBy structuring clothing data, this system aims to transform “rethinking what to wear every day” into “quickly selecting optimal combinations from a database.”\n\n\n\nIn this system, each clothing item is treated as a data entity and described using a set of attributes, such as:\n\ncategory (T-shirts, jeans, outerwear, shoes),\ncolor (including the proportion of each color),\nstyle (clean-fit, formal, vintage, sports, etc.),\nmaterial (denim, linen, cotton).\n\nThese attributes are normalized into multiple tables, and many-to-many relationships are used to represent that a single item can belong to multiple styles or be suitable for different occasions.\nThe core function of this database is not only to store items, but to describe the compatibility between items.\nThe system uses compatibility rules to define:\n\nVisual aesthetic constraints, such as avoiding more than three colors in a single outfit and limiting the number of style tags to maintain overall consistency\nClimate adaptability, where combinations are evaluated based on insulation-related variables to ensure balanced warmth between upper and lower body layers, and higher overall insulation is preferred as the temperature decreases\n\nWhen a user selects a specific item (for example, a pink T-shirt), the system can immediately recommend other highly compatible items (such as light blue jeans and white sneakers) based on database relationships and rules, and rank these combinations by compatibility score to help the user make decisions more efficiently.\nIn addition, as data accumulates, the system can analyze the overall structure of the wardrobe, such as:\n\nWhether certain styles or clothing categories are lacking\nWhether colors or item types are overly concentrated\nWhether newly purchased items overlap in function with existing ones\nWhich older items have not been used for a long time and could be considered for removal\n\nThis allows the wardrobe to function not just as an item list, but as a system that can be queried, analyzed, and optimized, and that can be extended to daily life applications such as outfit recommendations and purchase decision support.\nThis problem is particularly well suited for a relational database, because outfit selection inherently involves structured data and many-to-many relationships (such as items, styles, and compatibility rules), which can be efficiently combined and analyzed through relational queries.\n\n\n\nWhen users enter the system, the home page displays a table view of all items in the wardrobe, including basic information such as category, color, style, material, and seasonality. The interface supports multi-select functionality.\nUsers can select one or more items they plan to wear and submit their selection to generate outfit results.\nBased on the selected items and the compatibility rules stored in the database, the system generates multiple outfit candidates.\nThe outfit results page provides different sorting options, such as sorting by comfort score, aesthetic score, or climate fit score.\nEach outfit displays its corresponding numerical scores, allowing users to quickly compare options and select the most suitable combination without repeatedly trying on clothes or overthinking the decision.\nThe interface supports fast decision-making: select items → generate outfits → sort by scores → pick the best match.\n\n\n\n\n\n\nThe purpose of this 3D printing farm database is to systematize the entire workflow—from customer order intake to automated estimation, machine scheduling, and progress tracking—so the farm can operate efficiently as order volume grows. The goals are to shorten turnaround time, reduce human scheduling errors, improve machine utilization, and maximize profitability.\nIn practice, 3D printing orders vary widely (model size, material, resolution, multi-color requirements, and post-processing such as painting). If pricing and scheduling rely on manual judgment, it is easy to underestimate time/cost, assign the wrong machine, or create bottlenecks in the order queue. Therefore, this system uses a relational database to store orders, machine capabilities, material usage, and scheduling states in a structured way, enabling fast and consistent decisions through rules and queries.\n\n\n\nOrder intake & requirement tagging (Order Intake & Requirement Tagging)\nWhen a customer submits an order, the system stores it as an order record with structured attributes, such as:\n\nModel size and volume (bounding box / volume)\nPrinting type (FDM / SLA)\nResolution settings (layer height / resolution)\nMulti-color requirement (multi-color)\nMaterial type (material type)\nPost-processing needs (post-processing, e.g., painting/sanding)\nOther customization requests (stored as tags)\n\nThese fields can be normalized into multiple tables, with many-to-many relationships used to represent that a single order can have multiple requirement tags.\nPer-machine estimation (Per-Machine Estimation)\nThe key is not only to calculate an overall price for the order, but to estimate how the same order would perform on different machines, since time, cost, and completion time may vary by machine. This supports better machine assignment and scheduling decisions.\nFor each candidate machine, the system applies pricing rules or an estimation model to perform per-machine estimation, including:\n\nEstimated print time (estimated print time)\nEstimated material usage (estimated material usage)\nMachine-specific estimated cost & quote (machine-specific estimated cost & quote)\nEstimated completion time (estimated completion time, considering current workload)\n\nThe system stores these “order × machine” estimates for querying and ranking using different objective functions, such as lowest cost, earliest completion, or the most stable option within a deadline.\nOrder queue & status tracking (Order Queue & Status Tracking)\nAll orders are automatically added to an order queue (order list), and each order maintains a clear status, such as:\n\npending\nqueued\nprinting\npost-processing\ncompleted\nfailed\n\nManagers can query:\n\nWhat is currently in the queue and its priority\nWhich orders are printing vs. waiting for machines\nWhich failed orders require reprinting or manual intervention\n\nMachine capability modeling & assignment recommendations (Machine Capability & Assignment)\nThe database stores each machine’s capabilities and constraints, such as: - Machine type: multi-color / single-color / SLA / FDM - Maximum build volume (max build volume) - Supported materials (supported materials) - Speed/quality profile (speed/quality profile) - Current workload and availability (workload & availability)\nWhen a new order arrives, the system first performs constraint filtering (e.g., size, material, multi-color requirements) to identify feasible machines, then uses per-machine estimation to generate recommended assignments, for example:\n\nEarliest completion time (earliest completion time)\nLowest estimated cost (lowest estimated cost)\nBalanced option (deadline + stability)\n\nThis turns scheduling into a decision-support process rather than manual guesswork.\n\n\n\nOn the customer side, the system provides a customer order page where users can upload a 3D model or specify printing requirements such as size, material, resolution, multi-color options, and post-processing needs. Based on this information, the system automatically returns an estimated price and an estimated delivery time.\nOn the admin side, the system offers an order dashboard that displays the current order queue and order statuses. Administrators can sort or filter orders by deadline, priority, or processing status to manage workflow more efficiently.\nThe system also includes a machine dashboard that lists all available machines along with their machine type, maximum build volume, supported materials, current workload, and estimated availability. This allows operators to quickly understand machine capacity and constraints.\nWhen an order is selected, the scheduling view presents a list of candidate machines that can fulfill the order. For each candidate machine, the system displays the estimated print time, estimated material usage, machine-specific cost and quote, and estimated completion time. The interface supports one-click sorting options, such as fastest, cheapest, or most stable, to assist administrators in making assignment decisions.\nThe interface supports efficient operations: submit order → per-machine estimation → queue order → recommend machines → schedule & track progress.\n\n\n\n\n\n\nThe purpose of this system is to manage the core information of a farm—such as fields, crop types, growth stages, and irrigation equipment—using a relational database.\nAt the same time, the system retrieves and stores weather data through APIs provided by weather forecast services, and combines this information with a set of irrigation rules to automatically generate a daily irrigation schedule.\nThe goal is to reduce manual decision-making costs while improving water-use efficiency and consistency in crop management.\n\n\n\nIn this system, the database is not used only for data storage. Its core function is to integrate internal farm information with external weather data and automatically generate irrigation decisions based on predefined rules.\nCore Data Management\nThe system uses a relational schema to manage the main entities of the farm, including:\n\nField: field ID, location, area, and the crop currently planted\nCrop: crop type and its basic water requirements\nGrowth Stage: stages such as germination, growth, flowering, and fruiting, each with different water needs\nIrrigation Equipment: equipment type (e.g., drip irrigation, sprinkler), flow rate or efficiency factor, and availability status\n\nThese entities are connected through relationships. For example, each field is associated with a specific crop and a current growth stage, and can be assigned available irrigation equipment.\nWeather Data Integration\nThe system retrieves weather information through external weather forecast APIs, such as:\n\nPredicted rainfall amount\nProbability of precipitation\nTemperature range\n\nThis weather data is stored in the database and used as an important input for daily irrigation decisions, without requiring manual input from users.\nIrrigation Rules and Schedule Generation\nThe system maintains a set of irrigation rules that describe irrigation requirements under different conditions, such as:\n\nCrop type × growth stage → recommended baseline irrigation amount\nIf predicted rainfall exceeds a certain threshold → automatically reduce or cancel irrigation for the day\nDifferences in irrigation equipment efficiency → adjust actual irrigation duration\n\nWhen the daily scheduling process runs, the system combines:\n\nThe crop type and growth stage of each field\nThe weather forecast for the day\nThe availability and efficiency of irrigation equipment\n\nBased on this information, the system automatically generates a daily irrigation schedule, indicating whether each field requires irrigation and the recommended water amount or irrigation time.\n\n\n\nWhen users enter the system, the home page displays a table view of all fields on the farm, including the current crop type, growth stage, and the system’s irrigation recommendation for the day.\nUsers can generate the daily irrigation schedule with a single action. Based on field information, weather forecasts, and irrigation rules, the system lists which fields require irrigation and provides recommended water amounts or irrigation durations.\nThe schedule is presented in a simple list format, allowing users to quickly review and execute irrigation tasks. After completion, users can mark irrigation status for record-keeping and future reference.\n\n\n\n\n\nWhat are the things current database system cannot do?\nCurrent database systems are not capable of understanding the semantics behind data. As a result, in more complex applications, they often rely on manually defined rules or continuously adjusted weights to produce reasonable outputs. In addition, databases are limited in handling cross-context decision-making, where multiple competing objectives must be balanced simultaneously.\nFor example, in a wardrobe management database, the system can evaluate outfits based on structured criteria such as color combinations, style tags, material properties, and weather conditions. It can assign scores for factors like aesthetic quality, comfort, and climate suitability, and generate multiple candidate outfits that satisfy predefined rules. However, the database cannot determine which outfit represents the optimal balance among being visually appealing, comfortable, and suitable for the weather.\nThis limitation arises because preferences such as “looking good” or “feeling comfortable” are inherently subjective and context-dependent, and there is no single optimal solution that applies to all users or situations. Therefore, the role of the database is not to make the final decision, but to support decision-making by filtering infeasible options, structuring relevant information, and presenting comparable alternatives with transparent evaluation metrics.\nUltimately, the final choice must be made by the user, who can decide whether to prioritize comfort, aesthetics, or climate suitability in a given context. This highlights a fundamental limitation of current database systems: they are effective at decision support, but they cannot replace human judgment in complex, value-driven decisions.\n\n\n\nDescribe at least three tables that might be used to store information in a social-network/social media system such as Twitter or Reddit.\nA social-network or social media system such as Twitter or Reddit may be supported by at least the following three core tables:\n1. User Table\nThe user table stores basic information about users, such as: - user_id - username - account creation time - profile metadata (e.g., bio or status)\nThis table represents the identities of users and serves as a reference for other tables in the system.\n2. Post Table\nThe post table stores content created by users, such as:\n\npost_id\nauthor_id (foreign key referencing the User table)\ncontent\ntimestamp\n\nEach post is associated with a specific user, forming a one-to-many relationship between users and posts.\n3. Comment Table\nThe comment table stores replies to posts (or other comments), such as:\n\ncomment_id\npost_id (foreign key referencing the Post table)\nauthor_id\ncontent\ntimestamp\n\nThis table supports threaded discussions and allows multiple users to participate in conversations under the same post.\nThese tables are separated to support relational queries, maintain data consistency, and enable efficient retrieval of users, posts, and discussion threads.\n\n\n\n\n\n\nWhat are the differences between relation schema, relation, and instance? Give an example using the university database to illustrate.\n\nRelation Schema = The logical structure of a relation: a list of attribute names and their domains. It does not change over time.\nExample: instructor(ID, name, dept_name, salary)\nRelation = Informally used to refer to both the schema and instance together.\nExample: “The department relation” can refer to either the schema department(dept_name, building, budget) or the actual data it currently holds.\nInstance = A snapshot of the actual data in a relation at a given point in time. It changes as tuples are inserted, updated, or deleted.\nExample: The department relation instance in Figure 2.5 contains 7 tuples. If the university adds a “Data Science” department, the instance grows to 8 tuples, but the schema remains department(dept_name, building, budget).\n\n\n\n\nDraw a schema diagram for the following bank database. Identify primary keys (underlined) and foreign keys.\nThe bank database consists of the following relations:\n\nbranch(branch_name, branch_city, assets)\ncustomer(ID, customer_name, customer_street, customer_city)\nloan(loan_number, branch_name, amount)\nborrower(ID, loan_number)\naccount(account_number, branch_name, balance)\ndepositor(ID, account_number)\n\n\n\n\nBank Database Schema Diagram\n\n\n\n\n\nDescribe two ways artificial intelligence or LLM can assist in managing or querying a database. In your answer, briefly explain how each method improves efficiency or accuracy compared to traditional (non-AI) approaches. (3–5 sentences)\n\nNatural Language to SQL (Querying): LLMs can translate plain language questions directly into executable SQL queries, lowering the barrier for non-technical users and reducing syntax errors compared to writing SQL manually.\nAI-Driven Database Tuning (Managing): LLMs can automatically analyze slow queries and recommend index optimizations, replacing the traditionally time-consuming process of a DBA manually examining query logs and execution plans.\n\nOverall, both approaches reduce the need for specialized expertise and allow faster, more accurate database operations compared to traditional manual methods.\n\n\n\n\n\n\nOpen the Online SQL interpreter and load the university database.\n\n\n\nWrite SQL codes to get a list of: i. Student IDs, ii. Instructors, iii. Departments\n\n\n\nQ2 — Student IDs (from takes), Instructors, and Departments\n\n\n\n\n\nWrite SQL codes to do the following queries:\ni. Find the ID and name of each student who has taken at least one Comp. Sci. course; make sure there are no duplicate names in the result.\n\n\n\nQ3i — Students who took at least one Comp. Sci. course\n\n\nii. Add grades to the list\n\n\n\nQ3ii — Add grades to the result\n\n\niii. Find the ID and name of each student who has not taken any course offered before 2017.\n\n\n\nQ3iii — Students who have not taken any course before 2017\n\n\niv. For each department, find the maximum salary of instructors in that department.\n\n\n\nQ3iv — Maximum instructor salary per department\n\n\nv. Find the lowest, across all departments, of the per-department maximum salary computed by the preceding query.\n\n\n\nQ3v — Lowest of the per-department maximum salaries\n\n\nvi. Add names to the list\n\n\n\nQ3vi — Add instructor names to the result\n\n\n\n\n\nFind instructor (with name and ID) who has never given an A grade in any course she or he has taught. (Instructors who have never taught a course trivially satisfy this condition.)\n\n\n\nQ4 — Instructors who have never given an A grade"
  },
  {
    "objectID": "EPPS6354.html#assignments",
    "href": "EPPS6354.html#assignments",
    "title": "Information Management",
    "section": "",
    "text": "Name and describe three applications you have used that employed a database system to store and access persistent data. (e.g. airlines, online trade, banking, university system)\nFor the first question, one example that comes to mind is video games. In video games, a player’s level and experience points, as well as the items and equipment they have obtained, are recorded, so the player can still access them the next time they log in. Another example is online shopping. For instance, Amazon records information such as the price of each product, the catalog it belongs to, whether it is eligible for free shipping, and whether it is in stock. A third example is a streaming platform, such as Netflix, which records a user’s region and subscription level. All of this data is stored persistently and can be accessed at a later time.\n\n\n\nPropose three applications in domain projects (e.g. criminology, economics, brain science, etc.) Be sure you include: i. Purpose ii. Functions iii. Simple interface design\n\n\n\n\nThe main purpose of this wardrobe management database is to minimize the time spent choosing outfits before going out.\nFor many people, the difficulty in daily outfit selection is not a lack of clothing, but the need to simultaneously consider colors, styles, occasions, and overall coordination, which leads to a high decision-making cost.\nTherefore, I model the wardrobe as a relational database, which not only records individual clothing items but also describes the relationships between items, allowing outfit selection to be handled in a systematic way.\nBy structuring clothing data, this system aims to transform “rethinking what to wear every day” into “quickly selecting optimal combinations from a database.”\n\n\n\nIn this system, each clothing item is treated as a data entity and described using a set of attributes, such as:\n\ncategory (T-shirts, jeans, outerwear, shoes),\ncolor (including the proportion of each color),\nstyle (clean-fit, formal, vintage, sports, etc.),\nmaterial (denim, linen, cotton).\n\nThese attributes are normalized into multiple tables, and many-to-many relationships are used to represent that a single item can belong to multiple styles or be suitable for different occasions.\nThe core function of this database is not only to store items, but to describe the compatibility between items.\nThe system uses compatibility rules to define:\n\nVisual aesthetic constraints, such as avoiding more than three colors in a single outfit and limiting the number of style tags to maintain overall consistency\nClimate adaptability, where combinations are evaluated based on insulation-related variables to ensure balanced warmth between upper and lower body layers, and higher overall insulation is preferred as the temperature decreases\n\nWhen a user selects a specific item (for example, a pink T-shirt), the system can immediately recommend other highly compatible items (such as light blue jeans and white sneakers) based on database relationships and rules, and rank these combinations by compatibility score to help the user make decisions more efficiently.\nIn addition, as data accumulates, the system can analyze the overall structure of the wardrobe, such as:\n\nWhether certain styles or clothing categories are lacking\nWhether colors or item types are overly concentrated\nWhether newly purchased items overlap in function with existing ones\nWhich older items have not been used for a long time and could be considered for removal\n\nThis allows the wardrobe to function not just as an item list, but as a system that can be queried, analyzed, and optimized, and that can be extended to daily life applications such as outfit recommendations and purchase decision support.\nThis problem is particularly well suited for a relational database, because outfit selection inherently involves structured data and many-to-many relationships (such as items, styles, and compatibility rules), which can be efficiently combined and analyzed through relational queries.\n\n\n\nWhen users enter the system, the home page displays a table view of all items in the wardrobe, including basic information such as category, color, style, material, and seasonality. The interface supports multi-select functionality.\nUsers can select one or more items they plan to wear and submit their selection to generate outfit results.\nBased on the selected items and the compatibility rules stored in the database, the system generates multiple outfit candidates.\nThe outfit results page provides different sorting options, such as sorting by comfort score, aesthetic score, or climate fit score.\nEach outfit displays its corresponding numerical scores, allowing users to quickly compare options and select the most suitable combination without repeatedly trying on clothes or overthinking the decision.\nThe interface supports fast decision-making: select items → generate outfits → sort by scores → pick the best match.\n\n\n\n\n\n\nThe purpose of this 3D printing farm database is to systematize the entire workflow—from customer order intake to automated estimation, machine scheduling, and progress tracking—so the farm can operate efficiently as order volume grows. The goals are to shorten turnaround time, reduce human scheduling errors, improve machine utilization, and maximize profitability.\nIn practice, 3D printing orders vary widely (model size, material, resolution, multi-color requirements, and post-processing such as painting). If pricing and scheduling rely on manual judgment, it is easy to underestimate time/cost, assign the wrong machine, or create bottlenecks in the order queue. Therefore, this system uses a relational database to store orders, machine capabilities, material usage, and scheduling states in a structured way, enabling fast and consistent decisions through rules and queries.\n\n\n\nOrder intake & requirement tagging (Order Intake & Requirement Tagging)\nWhen a customer submits an order, the system stores it as an order record with structured attributes, such as:\n\nModel size and volume (bounding box / volume)\nPrinting type (FDM / SLA)\nResolution settings (layer height / resolution)\nMulti-color requirement (multi-color)\nMaterial type (material type)\nPost-processing needs (post-processing, e.g., painting/sanding)\nOther customization requests (stored as tags)\n\nThese fields can be normalized into multiple tables, with many-to-many relationships used to represent that a single order can have multiple requirement tags.\nPer-machine estimation (Per-Machine Estimation)\nThe key is not only to calculate an overall price for the order, but to estimate how the same order would perform on different machines, since time, cost, and completion time may vary by machine. This supports better machine assignment and scheduling decisions.\nFor each candidate machine, the system applies pricing rules or an estimation model to perform per-machine estimation, including:\n\nEstimated print time (estimated print time)\nEstimated material usage (estimated material usage)\nMachine-specific estimated cost & quote (machine-specific estimated cost & quote)\nEstimated completion time (estimated completion time, considering current workload)\n\nThe system stores these “order × machine” estimates for querying and ranking using different objective functions, such as lowest cost, earliest completion, or the most stable option within a deadline.\nOrder queue & status tracking (Order Queue & Status Tracking)\nAll orders are automatically added to an order queue (order list), and each order maintains a clear status, such as:\n\npending\nqueued\nprinting\npost-processing\ncompleted\nfailed\n\nManagers can query:\n\nWhat is currently in the queue and its priority\nWhich orders are printing vs. waiting for machines\nWhich failed orders require reprinting or manual intervention\n\nMachine capability modeling & assignment recommendations (Machine Capability & Assignment)\nThe database stores each machine’s capabilities and constraints, such as: - Machine type: multi-color / single-color / SLA / FDM - Maximum build volume (max build volume) - Supported materials (supported materials) - Speed/quality profile (speed/quality profile) - Current workload and availability (workload & availability)\nWhen a new order arrives, the system first performs constraint filtering (e.g., size, material, multi-color requirements) to identify feasible machines, then uses per-machine estimation to generate recommended assignments, for example:\n\nEarliest completion time (earliest completion time)\nLowest estimated cost (lowest estimated cost)\nBalanced option (deadline + stability)\n\nThis turns scheduling into a decision-support process rather than manual guesswork.\n\n\n\nOn the customer side, the system provides a customer order page where users can upload a 3D model or specify printing requirements such as size, material, resolution, multi-color options, and post-processing needs. Based on this information, the system automatically returns an estimated price and an estimated delivery time.\nOn the admin side, the system offers an order dashboard that displays the current order queue and order statuses. Administrators can sort or filter orders by deadline, priority, or processing status to manage workflow more efficiently.\nThe system also includes a machine dashboard that lists all available machines along with their machine type, maximum build volume, supported materials, current workload, and estimated availability. This allows operators to quickly understand machine capacity and constraints.\nWhen an order is selected, the scheduling view presents a list of candidate machines that can fulfill the order. For each candidate machine, the system displays the estimated print time, estimated material usage, machine-specific cost and quote, and estimated completion time. The interface supports one-click sorting options, such as fastest, cheapest, or most stable, to assist administrators in making assignment decisions.\nThe interface supports efficient operations: submit order → per-machine estimation → queue order → recommend machines → schedule & track progress.\n\n\n\n\n\n\nThe purpose of this system is to manage the core information of a farm—such as fields, crop types, growth stages, and irrigation equipment—using a relational database.\nAt the same time, the system retrieves and stores weather data through APIs provided by weather forecast services, and combines this information with a set of irrigation rules to automatically generate a daily irrigation schedule.\nThe goal is to reduce manual decision-making costs while improving water-use efficiency and consistency in crop management.\n\n\n\nIn this system, the database is not used only for data storage. Its core function is to integrate internal farm information with external weather data and automatically generate irrigation decisions based on predefined rules.\nCore Data Management\nThe system uses a relational schema to manage the main entities of the farm, including:\n\nField: field ID, location, area, and the crop currently planted\nCrop: crop type and its basic water requirements\nGrowth Stage: stages such as germination, growth, flowering, and fruiting, each with different water needs\nIrrigation Equipment: equipment type (e.g., drip irrigation, sprinkler), flow rate or efficiency factor, and availability status\n\nThese entities are connected through relationships. For example, each field is associated with a specific crop and a current growth stage, and can be assigned available irrigation equipment.\nWeather Data Integration\nThe system retrieves weather information through external weather forecast APIs, such as:\n\nPredicted rainfall amount\nProbability of precipitation\nTemperature range\n\nThis weather data is stored in the database and used as an important input for daily irrigation decisions, without requiring manual input from users.\nIrrigation Rules and Schedule Generation\nThe system maintains a set of irrigation rules that describe irrigation requirements under different conditions, such as:\n\nCrop type × growth stage → recommended baseline irrigation amount\nIf predicted rainfall exceeds a certain threshold → automatically reduce or cancel irrigation for the day\nDifferences in irrigation equipment efficiency → adjust actual irrigation duration\n\nWhen the daily scheduling process runs, the system combines:\n\nThe crop type and growth stage of each field\nThe weather forecast for the day\nThe availability and efficiency of irrigation equipment\n\nBased on this information, the system automatically generates a daily irrigation schedule, indicating whether each field requires irrigation and the recommended water amount or irrigation time.\n\n\n\nWhen users enter the system, the home page displays a table view of all fields on the farm, including the current crop type, growth stage, and the system’s irrigation recommendation for the day.\nUsers can generate the daily irrigation schedule with a single action. Based on field information, weather forecasts, and irrigation rules, the system lists which fields require irrigation and provides recommended water amounts or irrigation durations.\nThe schedule is presented in a simple list format, allowing users to quickly review and execute irrigation tasks. After completion, users can mark irrigation status for record-keeping and future reference.\n\n\n\n\n\nWhat are the things current database system cannot do?\nCurrent database systems are not capable of understanding the semantics behind data. As a result, in more complex applications, they often rely on manually defined rules or continuously adjusted weights to produce reasonable outputs. In addition, databases are limited in handling cross-context decision-making, where multiple competing objectives must be balanced simultaneously.\nFor example, in a wardrobe management database, the system can evaluate outfits based on structured criteria such as color combinations, style tags, material properties, and weather conditions. It can assign scores for factors like aesthetic quality, comfort, and climate suitability, and generate multiple candidate outfits that satisfy predefined rules. However, the database cannot determine which outfit represents the optimal balance among being visually appealing, comfortable, and suitable for the weather.\nThis limitation arises because preferences such as “looking good” or “feeling comfortable” are inherently subjective and context-dependent, and there is no single optimal solution that applies to all users or situations. Therefore, the role of the database is not to make the final decision, but to support decision-making by filtering infeasible options, structuring relevant information, and presenting comparable alternatives with transparent evaluation metrics.\nUltimately, the final choice must be made by the user, who can decide whether to prioritize comfort, aesthetics, or climate suitability in a given context. This highlights a fundamental limitation of current database systems: they are effective at decision support, but they cannot replace human judgment in complex, value-driven decisions.\n\n\n\nDescribe at least three tables that might be used to store information in a social-network/social media system such as Twitter or Reddit.\nA social-network or social media system such as Twitter or Reddit may be supported by at least the following three core tables:\n1. User Table\nThe user table stores basic information about users, such as: - user_id - username - account creation time - profile metadata (e.g., bio or status)\nThis table represents the identities of users and serves as a reference for other tables in the system.\n2. Post Table\nThe post table stores content created by users, such as:\n\npost_id\nauthor_id (foreign key referencing the User table)\ncontent\ntimestamp\n\nEach post is associated with a specific user, forming a one-to-many relationship between users and posts.\n3. Comment Table\nThe comment table stores replies to posts (or other comments), such as:\n\ncomment_id\npost_id (foreign key referencing the Post table)\nauthor_id\ncontent\ntimestamp\n\nThis table supports threaded discussions and allows multiple users to participate in conversations under the same post.\nThese tables are separated to support relational queries, maintain data consistency, and enable efficient retrieval of users, posts, and discussion threads.\n\n\n\n\n\n\nWhat are the differences between relation schema, relation, and instance? Give an example using the university database to illustrate.\n\nRelation Schema = The logical structure of a relation: a list of attribute names and their domains. It does not change over time.\nExample: instructor(ID, name, dept_name, salary)\nRelation = Informally used to refer to both the schema and instance together.\nExample: “The department relation” can refer to either the schema department(dept_name, building, budget) or the actual data it currently holds.\nInstance = A snapshot of the actual data in a relation at a given point in time. It changes as tuples are inserted, updated, or deleted.\nExample: The department relation instance in Figure 2.5 contains 7 tuples. If the university adds a “Data Science” department, the instance grows to 8 tuples, but the schema remains department(dept_name, building, budget).\n\n\n\n\nDraw a schema diagram for the following bank database. Identify primary keys (underlined) and foreign keys.\nThe bank database consists of the following relations:\n\nbranch(branch_name, branch_city, assets)\ncustomer(ID, customer_name, customer_street, customer_city)\nloan(loan_number, branch_name, amount)\nborrower(ID, loan_number)\naccount(account_number, branch_name, balance)\ndepositor(ID, account_number)\n\n\n\n\nBank Database Schema Diagram\n\n\n\n\n\nDescribe two ways artificial intelligence or LLM can assist in managing or querying a database. In your answer, briefly explain how each method improves efficiency or accuracy compared to traditional (non-AI) approaches. (3–5 sentences)\n\nNatural Language to SQL (Querying): LLMs can translate plain language questions directly into executable SQL queries, lowering the barrier for non-technical users and reducing syntax errors compared to writing SQL manually.\nAI-Driven Database Tuning (Managing): LLMs can automatically analyze slow queries and recommend index optimizations, replacing the traditionally time-consuming process of a DBA manually examining query logs and execution plans.\n\nOverall, both approaches reduce the need for specialized expertise and allow faster, more accurate database operations compared to traditional manual methods.\n\n\n\n\n\n\nOpen the Online SQL interpreter and load the university database.\n\n\n\nWrite SQL codes to get a list of: i. Student IDs, ii. Instructors, iii. Departments\n\n\n\nQ2 — Student IDs (from takes), Instructors, and Departments\n\n\n\n\n\nWrite SQL codes to do the following queries:\ni. Find the ID and name of each student who has taken at least one Comp. Sci. course; make sure there are no duplicate names in the result.\n\n\n\nQ3i — Students who took at least one Comp. Sci. course\n\n\nii. Add grades to the list\n\n\n\nQ3ii — Add grades to the result\n\n\niii. Find the ID and name of each student who has not taken any course offered before 2017.\n\n\n\nQ3iii — Students who have not taken any course before 2017\n\n\niv. For each department, find the maximum salary of instructors in that department.\n\n\n\nQ3iv — Maximum instructor salary per department\n\n\nv. Find the lowest, across all departments, of the per-department maximum salary computed by the preceding query.\n\n\n\nQ3v — Lowest of the per-department maximum salaries\n\n\nvi. Add names to the list\n\n\n\nQ3vi — Add instructor names to the result\n\n\n\n\n\nFind instructor (with name and ID) who has never given an A grade in any course she or he has taught. (Instructors who have never taught a course trivially satisfy this condition.)\n\n\n\nQ4 — Instructors who have never given an A grade"
  }
]