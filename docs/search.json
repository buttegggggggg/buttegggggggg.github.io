[
  {
    "objectID": "DSIC.html",
    "href": "DSIC.html",
    "title": "MS of Data Science & Information Computing",
    "section": "",
    "text": "學歷： 國立中興大學 資料科學與資訊計算研究所 碩士 (Master of Science in Data Science and Information Computing, NCHU)。"
  },
  {
    "objectID": "DSIC.html#about-the-program-研究所簡介",
    "href": "DSIC.html#about-the-program-研究所簡介",
    "title": "MS of Data Science & Information Computing",
    "section": "About the Program / 研究所簡介",
    "text": "About the Program / 研究所簡介\n資料科學與資訊計算研究所（Graduate Institute of Data Science and Information Computing）隸屬國立中興大學理學院，成立宗旨在培養兼具資料科學理論基礎與資訊計算實作能力的跨領域人才。課程涵蓋機器學習、深度學習、大數據分析、影像處理、最佳化方法與高效能計算等，強調「從數學到實作」的完整訓練。\nThe program integrates mathematical foundations with modern computing to train professionals in machine learning, deep learning, computer vision, and big data analytics. Students are equipped to bridge the gap between theoretical models and real-world applications across domains including healthcare, industry, and scientific research."
  },
  {
    "objectID": "DSIC.html#research-focus-areas-研究方向",
    "href": "DSIC.html#research-focus-areas-研究方向",
    "title": "MS of Data Science & Information Computing",
    "section": "Research Focus Areas / 研究方向",
    "text": "Research Focus Areas / 研究方向\n\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"22px\"}, \"flowchart\": {\"nodeSpacing\": 20, \"rankSpacing\": 60, \"padding\": 30, \"useMaxWidth\": false}}}%%\nflowchart LR\n    ROOT[\"DSIC Research Areas     \"]\n\n    subgraph s1 [\" \"]\n      direction LR\n      ML[\"Machine Learning     \"] --&gt; ML1[\"Classification & Regression     \"]\n      ML --&gt; ML2[\"Ensemble Methods     \"]\n      ML --&gt; ML3[\"Feature Engineering     \"]\n    end\n\n    subgraph s2 [\" \"]\n      direction LR\n      DL[\"Deep Learning     \"] --&gt; DL1[\"CNNs & Transfer Learning     \"]\n      DL --&gt; DL2[\"GANs & Image Generation     \"]\n      DL --&gt; DL3[\"Semantic Segmentation     \"]\n    end\n\n    subgraph s3 [\" \"]\n      direction LR\n      CV[\"Computer Vision     \"] --&gt; CV1[\"Medical Imaging     \"]\n      CV --&gt; CV2[\"Defect Detection     \"]\n      CV --&gt; CV3[\"Image Reconstruction     \"]\n    end\n\n    subgraph s4 [\" \"]\n      direction LR\n      OPT[\"Optimization     \"] --&gt; OPT1[\"Loss Function Design     \"]\n      OPT --&gt; OPT2[\"Hyperparameter Tuning     \"]\n    end\n\n    ROOT --&gt; ML\n    ROOT --&gt; DL\n    ROOT --&gt; CV\n    ROOT --&gt; OPT\n\n    style ROOT fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style ML   fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n    style DL   fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style CV   fill:#F3E5F5,color:#6A1B9A,stroke:#CE93D8,stroke-width:2px\n    style OPT  fill:#FCE4EC,color:#C62828,stroke:#F48FB1,stroke-width:2px\n    style ML1 fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7\n    style ML2 fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7\n    style ML3 fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7\n    style DL1 fill:#FFF3E0,color:#E65100,stroke:#FFCC80\n    style DL2 fill:#FFF3E0,color:#E65100,stroke:#FFCC80\n    style DL3 fill:#FFF3E0,color:#E65100,stroke:#FFCC80\n    style CV1 fill:#F3E5F5,color:#6A1B9A,stroke:#CE93D8\n    style CV2 fill:#F3E5F5,color:#6A1B9A,stroke:#CE93D8\n    style CV3 fill:#F3E5F5,color:#6A1B9A,stroke:#CE93D8\n    style OPT1 fill:#FCE4EC,color:#C62828,stroke:#F48FB1\n    style OPT2 fill:#FCE4EC,color:#C62828,stroke:#F48FB1\n    style s1 fill:none,stroke:none\n    style s2 fill:none,stroke:none\n    style s3 fill:none,stroke:none\n    style s4 fill:none,stroke:none\n\n\n\n\n\n\n\n\n色碼說明： 綠色 = Machine Learning，橘色 = Deep Learning，紫色 = Computer Vision，粉紅色 = Optimization。以上為課程中涵蓋的主要研究領域。"
  },
  {
    "objectID": "DSIC.html#skills-acquired-習得技能",
    "href": "DSIC.html#skills-acquired-習得技能",
    "title": "MS of Data Science & Information Computing",
    "section": "Skills Acquired / 習得技能",
    "text": "Skills Acquired / 習得技能\n\n\n\n\n\n\n\n\n\n\n技能來源： 粉色 = ML + DL 兩門課都有訓練到的技能，青色 = 主要來自 ML 課程，橘色 = 主要來自 DL 課程。Python/PyTorch 為兩門課的共同程式語言，Transfer Learning 與 Model Evaluation 也跨課程反覆練習。"
  },
  {
    "objectID": "DSIC.html#coursework-課程作品",
    "href": "DSIC.html#coursework-課程作品",
    "title": "MS of Data Science & Information Computing",
    "section": "Coursework / 課程作品",
    "text": "Coursework / 課程作品\n\n\nMachine Learning & Data Science\n課程重點： 監督式/非監督式學習、模型評估、特徵工程、資料前處理\nAssignments：\n\nHW1 — Diabetes Prediction： Pima Indians 糖尿病預測。迴歸填補 → 特徵工程 → DNN，從 baseline 74% → 90.04%\nHW2 — US Wildfire Analysis： 188 萬筆野火紀錄。Poisson 迴歸趨勢分析 + MLP 成因分類（45.6%）\nFinal — Cervical Cancer Screening： EfficientNet-B7 遷移學習 + Focal Loss，三類別影像分類，平均 86.1%\n\nKey Tools： Python · Keras · scikit-learn · statsmodels · PyTorch (final)\n\n\nDeep Learning\n課程重點： CNN 架構、遷移學習、語意分割、影像生成、自動編碼器\nAssignments：\n\nHW1 — AOI Defect Classification： ResNet-18 遷移學習，6 類工業缺陷分類，96.44% Val Acc\nHW2 — Retinal Vessel Segmentation： U-Net (5-level) + Focal Tversky Loss，DRIVE 資料集，mIoU 0.351\nHW3 — Retinal Image Reconstruction： Convolutional Autoencoder，PSNR 峰值 30.84 dB（Epoch 18）\nHW4 — Western Blot Generation： Conditional GAN（Generator + PatchGAN Discriminator），分析 D/G 訓練動態\n\nKey Tools： Python · PyTorch · torchvision · Apple Silicon (MPS)\n\n\nBig Data Analysis / 巨量資料分析\n課程重點： Kernel Methods 加速、大規模最佳化、分散式機器學習\nAssignments：\n\nReading — Nyström Method： 論文閱讀 (NIPS 2000)，Gram matrix 低秩近似，O(n³) → O(m²n)\nHW — Kernel Ridge + Nyström： USPS 手寫數字分類，m=128 加速 20 倍，accuracy 99.50%\nFinal — Smoothed & Distributed SVM： a9a 資料集，Smoothed Hinge Loss + 分散式梯度聚合（K=5 workers），加速 150 倍\n\nKey Tools： Python · NumPy · scikit-learn · SciPy (L-BFGS-B)\n\n\nData Analysis Mathematics / 數據分析數學\n課程重點： SVD 理論與應用、矩陣低秩近似、Eckart-Young 定理、手寫辨識\nAssignments：\n\nHW1 — SVD Image Compression： 以照片驗證 Eckart-Young 定理，Monte Carlo 近似 2-norm，PSNR 達 44.7 dB (k=700)\nHW2 — Handwritten Digit Recognition： USPS 資料集，比較 8 種方法（Mean / SVD / HOSVD / SVM / KNN / RF / CNN），CNN 95.76% 最高\n\nKey Tools： Python · NumPy · PyTorch · tensorly · scikit-learn"
  },
  {
    "objectID": "DSIC.html#project-highlights-作品亮點",
    "href": "DSIC.html#project-highlights-作品亮點",
    "title": "MS of Data Science & Information Computing",
    "section": "Project Highlights / 作品亮點",
    "text": "Project Highlights / 作品亮點\n\n\n\n\n\n\n\n\nCourse\nProject\nHighlight\n\n\n\n\nMachine Learning\nDiabetes Prediction\nBaseline 74% → 90.04%（+16 pp）\n\n\n\nWildfire Analysis\n188 萬筆 · Poisson 趨勢 + MLP 分類\n\n\n\nCervical Cancer\nEfficientNet-B7 + Focal Loss · 86.1%\n\n\nDeep Learning\nAOI Defect Detection\nResNet-50 Fine-tune · 96.4%\n\n\n\nU-Net Segmentation\nDice 0.91 · 醫學影像語意分割\n\n\n\nAutoEncoder\n影像重建 30.8 dB PSNR\n\n\n\ncGAN Blot Removal\n條件式 GAN 去除墨漬\n\n\nBig Data\nNyström Approximation\nKernel Ridge 加速 20×\n\n\n\nSmoothed SVM\nHinge Loss 平滑化 · 梯度法求解\n\n\n\nDistributed SVM\n分散式計算加速 150×\n\n\nData Analysis Math\nSVD Image Compression\nEckart-Young 驗證 · PSNR 44.7 dB\n\n\n\nDigit Recognition (8 models)\nCNN 95.76% · KNN 最佳性價比\n\n\n\n\n學習歷程： 從 Data Analysis Math 的數學基礎（SVD、矩陣近似）→ ML 課程的經典模型（迴歸、DNN、遷移學習）→ DL 課程的進階架構（U-Net、AutoEncoder、GAN）→ Big Data 的大規模加速方法（Nyström、Smoothed SVM、分散式計算）。每個 project 都涵蓋完整的 pipeline — 從資料前處理、模型設計、訓練、到結果分析與視覺化。"
  },
  {
    "objectID": "BigData.html",
    "href": "BigData.html",
    "title": "Big Data Analysis",
    "section": "",
    "text": "Paper / 論文： Using the Nyström Method to Speed Up Kernel Machines Authors / 作者： Christopher K. I. Williams & Matthias Seeger (NIPS 2000)\n\n論文摘要： Kernel-based 方法（如 SVM、Gaussian Process）的主要瓶頸在於 Gram matrix 的計算與反矩陣操作，時間複雜度為 O(n³)。本論文提出使用 Nyström method 近似 Gram matrix — 從 n 筆訓練資料中僅抽出 m 筆，計算小矩陣後再展開為全域近似，將計算量從 O(n³) 降為 O(m²n)。\n\n\n\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35}}}%%\nflowchart LR\n    A[\"Full Gram Matrix K (n×n)     \"] --&gt; B[\"Sample m points     \"]\n    B --&gt; C[\"K_mm (m×m)     \"]\n    B --&gt; D[\"K_nm (n×m)     \"]\n    C --&gt; E[\"Eigen-decompose K_mm     \"]\n    D --&gt; F[\"Nyström Approx:\\nK ≈ K_nm K_mm⁻¹ K_nm^T     \"]\n\n    style A fill:#FCE4EC,color:#C62828,stroke:#F48FB1,stroke-width:2px\n    style B fill:#F5F5F5,color:#424242,stroke:#BDBDBD,stroke-width:2px\n    style C fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style D fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style E fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style F fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n\n\n\n\n\n\n\nNyström 流程： 從完整的 n×n Gram matrix（粉紅色，計算成本極高）中，隨機抽取 m 筆樣本組成 藍色 子矩陣 K_mm 與 K_nm，再透過 eigen-decomposition（橘色）重建 綠色 近似矩陣。當 m &lt;&lt; n 時大幅加速。\n\n\n\n\nMercer Expansion (Kernel 展開)：\n\\[K(x, y) = \\sum_{i=1}^{\\infty} \\lambda_i \\phi_i(x) \\phi_i(y)\\]\nNyström Approximation (近似公式)：\n\\[\\tilde{K} = K_{nm} K_{mm}^{-1} K_{nm}^T\\]\nComplexity Reduction / 複雜度降低：\n\\[O(n^3) \\rightarrow O(m^2 n), \\quad m \\ll n\\]\n\n\n\n\n\nTask / 任務： Implement the Nyström method to accelerate RBF Kernel Ridge Classification on the USPS handwritten digit dataset (binary: digit “4” vs. rest). 實作 Nyström 方法加速 RBF Kernel Ridge 二元分類，資料集為 USPS 手寫數字（辨識數字 4）。\nDataset / 資料集： USPS — 7,291 training + 2,007 test images, 256 features (16×16 pixels)\nMethod / 方法： RBF Kernel (σ=4) + Ridge Regression (λ=0.001) + Nyström Approximation (m = 128, 256, 512)\n\n\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35}}}%%\nflowchart LR\n    A[\"USPS 7291×256     \"] --&gt; B[\"StandardScaler     \"] --&gt; C[\"RBF Kernel σ=4     \"]\n    C --&gt; D[\"Full: K (7291×7291)     \"]\n    C --&gt; E[\"Nyström: K_mm + K_nm     \"]\n    D --&gt; F[\"Ridge Solve     \"]\n    E --&gt; F\n    F --&gt; G[\"Predict ±1     \"]\n\n    style A fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style B fill:#F5F5F5,color:#424242,stroke:#BDBDBD,stroke-width:2px\n    style C fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style D fill:#FCE4EC,color:#C62828,stroke:#F48FB1,stroke-width:2px\n    style E fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n    style F fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style G fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n\n\n\n\n\n\n\nPipeline 說明： USPS 資料先標準化，再分為兩條路徑：粉紅色 Full Kernel 計算完整 7291×7291 Gram matrix，綠色 Nyström 僅計算子集矩陣 K_mm + K_nm。兩者皆透過 Ridge Regression 求解係數向量 α，最終預測 ±1。\n\n\n\n\n# RBF Kernel function\ndef rbf_kernel(X1, X2, sigma=4.0):\n    sq_dists = (np.sum(X1**2, axis=1)[:, None]\n                + np.sum(X2**2, axis=1)[None, :]\n                - 2 * X1 @ X2.T)\n    return np.exp(-sq_dists / (2 * sigma**2))\n\n# Kernel Ridge: solve (K + λI) α = y\ndef kernel_ridge_train(K, y, lam=1e-3):\n    n = K.shape[0]\n    return np.linalg.solve(K + lam * np.eye(n), y)\n\n# Nyström approximation\ndef nystrom_approximation(X_train, m, sigma=4.0):\n    indices = np.random.choice(n, m, replace=False)\n    X_sub = X_train[indices]\n    K_mm = rbf_kernel(X_sub, X_sub, sigma)\n    K_nm = rbf_kernel(X_train, X_sub, sigma)\n    K_mm_inv = np.linalg.inv(K_mm + 1e-8 * np.eye(m))\n    K_approx = K_nm @ K_mm_inv @ K_nm.T\n    return K_approx, indices, K_nm, K_mm_inv\n\n\n\n\n\n\nMethod\nErrors\nAccuracy\nTrain Time (s)\n\n\n\n\nFull Kernel\n9\n99.55%\n7.52\n\n\nNyström m=128\n10\n99.50%\n0.38\n\n\nNyström m=256\n10\n99.50%\n0.63\n\n\nNyström m=512\n9\n99.55%\n1.83\n\n\n\n\n\n\n\n\n\n\n\n\n\n效能分析： 粉色 Full Kernel 需 7.52 秒計算完整 7291×7291 Gram matrix。青色 Nyström m=128 僅需 0.38 秒（加速約 20 倍），且 accuracy 幾乎不變（99.50% vs 99.55%）。m=512 時精度完全匹配 Full Kernel，訓練時間仍節省 75%。\n\n\n\n\n\n\n\nFull Kernel vs Nyström Approximation (first 100 samples)\n\n\n\n矩陣比較： 左圖為完整 Gram matrix（前 100 筆樣本），右圖為 Nyström 近似結果。兩者結構高度相似，驗證了 Nyström 方法在保留 kernel 結構的同時大幅減少計算量。"
  },
  {
    "objectID": "BigData.html#assignments",
    "href": "BigData.html#assignments",
    "title": "Big Data Analysis",
    "section": "",
    "text": "Paper / 論文： Using the Nyström Method to Speed Up Kernel Machines Authors / 作者： Christopher K. I. Williams & Matthias Seeger (NIPS 2000)\n\n論文摘要： Kernel-based 方法（如 SVM、Gaussian Process）的主要瓶頸在於 Gram matrix 的計算與反矩陣操作，時間複雜度為 O(n³)。本論文提出使用 Nyström method 近似 Gram matrix — 從 n 筆訓練資料中僅抽出 m 筆，計算小矩陣後再展開為全域近似，將計算量從 O(n³) 降為 O(m²n)。\n\n\n\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35}}}%%\nflowchart LR\n    A[\"Full Gram Matrix K (n×n)     \"] --&gt; B[\"Sample m points     \"]\n    B --&gt; C[\"K_mm (m×m)     \"]\n    B --&gt; D[\"K_nm (n×m)     \"]\n    C --&gt; E[\"Eigen-decompose K_mm     \"]\n    D --&gt; F[\"Nyström Approx:\\nK ≈ K_nm K_mm⁻¹ K_nm^T     \"]\n\n    style A fill:#FCE4EC,color:#C62828,stroke:#F48FB1,stroke-width:2px\n    style B fill:#F5F5F5,color:#424242,stroke:#BDBDBD,stroke-width:2px\n    style C fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style D fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style E fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style F fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n\n\n\n\n\n\n\nNyström 流程： 從完整的 n×n Gram matrix（粉紅色，計算成本極高）中，隨機抽取 m 筆樣本組成 藍色 子矩陣 K_mm 與 K_nm，再透過 eigen-decomposition（橘色）重建 綠色 近似矩陣。當 m &lt;&lt; n 時大幅加速。\n\n\n\n\nMercer Expansion (Kernel 展開)：\n\\[K(x, y) = \\sum_{i=1}^{\\infty} \\lambda_i \\phi_i(x) \\phi_i(y)\\]\nNyström Approximation (近似公式)：\n\\[\\tilde{K} = K_{nm} K_{mm}^{-1} K_{nm}^T\\]\nComplexity Reduction / 複雜度降低：\n\\[O(n^3) \\rightarrow O(m^2 n), \\quad m \\ll n\\]\n\n\n\n\n\nTask / 任務： Implement the Nyström method to accelerate RBF Kernel Ridge Classification on the USPS handwritten digit dataset (binary: digit “4” vs. rest). 實作 Nyström 方法加速 RBF Kernel Ridge 二元分類，資料集為 USPS 手寫數字（辨識數字 4）。\nDataset / 資料集： USPS — 7,291 training + 2,007 test images, 256 features (16×16 pixels)\nMethod / 方法： RBF Kernel (σ=4) + Ridge Regression (λ=0.001) + Nyström Approximation (m = 128, 256, 512)\n\n\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35}}}%%\nflowchart LR\n    A[\"USPS 7291×256     \"] --&gt; B[\"StandardScaler     \"] --&gt; C[\"RBF Kernel σ=4     \"]\n    C --&gt; D[\"Full: K (7291×7291)     \"]\n    C --&gt; E[\"Nyström: K_mm + K_nm     \"]\n    D --&gt; F[\"Ridge Solve     \"]\n    E --&gt; F\n    F --&gt; G[\"Predict ±1     \"]\n\n    style A fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style B fill:#F5F5F5,color:#424242,stroke:#BDBDBD,stroke-width:2px\n    style C fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style D fill:#FCE4EC,color:#C62828,stroke:#F48FB1,stroke-width:2px\n    style E fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n    style F fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style G fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n\n\n\n\n\n\n\nPipeline 說明： USPS 資料先標準化，再分為兩條路徑：粉紅色 Full Kernel 計算完整 7291×7291 Gram matrix，綠色 Nyström 僅計算子集矩陣 K_mm + K_nm。兩者皆透過 Ridge Regression 求解係數向量 α，最終預測 ±1。\n\n\n\n\n# RBF Kernel function\ndef rbf_kernel(X1, X2, sigma=4.0):\n    sq_dists = (np.sum(X1**2, axis=1)[:, None]\n                + np.sum(X2**2, axis=1)[None, :]\n                - 2 * X1 @ X2.T)\n    return np.exp(-sq_dists / (2 * sigma**2))\n\n# Kernel Ridge: solve (K + λI) α = y\ndef kernel_ridge_train(K, y, lam=1e-3):\n    n = K.shape[0]\n    return np.linalg.solve(K + lam * np.eye(n), y)\n\n# Nyström approximation\ndef nystrom_approximation(X_train, m, sigma=4.0):\n    indices = np.random.choice(n, m, replace=False)\n    X_sub = X_train[indices]\n    K_mm = rbf_kernel(X_sub, X_sub, sigma)\n    K_nm = rbf_kernel(X_train, X_sub, sigma)\n    K_mm_inv = np.linalg.inv(K_mm + 1e-8 * np.eye(m))\n    K_approx = K_nm @ K_mm_inv @ K_nm.T\n    return K_approx, indices, K_nm, K_mm_inv\n\n\n\n\n\n\nMethod\nErrors\nAccuracy\nTrain Time (s)\n\n\n\n\nFull Kernel\n9\n99.55%\n7.52\n\n\nNyström m=128\n10\n99.50%\n0.38\n\n\nNyström m=256\n10\n99.50%\n0.63\n\n\nNyström m=512\n9\n99.55%\n1.83\n\n\n\n\n\n\n\n\n\n\n\n\n\n效能分析： 粉色 Full Kernel 需 7.52 秒計算完整 7291×7291 Gram matrix。青色 Nyström m=128 僅需 0.38 秒（加速約 20 倍），且 accuracy 幾乎不變（99.50% vs 99.55%）。m=512 時精度完全匹配 Full Kernel，訓練時間仍節省 75%。\n\n\n\n\n\n\n\nFull Kernel vs Nyström Approximation (first 100 samples)\n\n\n\n矩陣比較： 左圖為完整 Gram matrix（前 100 筆樣本），右圖為 Nyström 近似結果。兩者結構高度相似，驗證了 Nyström 方法在保留 kernel 結構的同時大幅減少計算量。"
  },
  {
    "objectID": "BigData.html#final-project",
    "href": "BigData.html#final-project",
    "title": "Big Data Analysis",
    "section": "Final Project / 期末報告",
    "text": "Final Project / 期末報告\n\nSmoothed & Distributed SVM / 平滑化與分散式 SVM\nTask / 任務： Implement and compare three SVM variants: (1) standard LinearSVC, (2) Smoothed SVM using log-sum-exp approximation of hinge loss, and (3) Distributed Smoothed SVM with communication-efficient gradient aggregation. 實作並比較三種 SVM 變體：標準 LinearSVC、平滑化 SVM（log-sum-exp 近似 hinge loss）、以及分散式平滑 SVM（通訊高效梯度聚合）。\nDataset / 資料集： a9a (Adult Income) — 32,561 training + 16,281 test samples, 123 features\nMethod / 方法： LinearSVC baseline → Smoothed Hinge Loss (β=5) + L-BFGS-B → Distributed SGD (K=5 workers)\n\nThree SVM Variants / 三種 SVM 變體\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35}}}%%\nflowchart TD\n    DATA[\"a9a Dataset 32K×123     \"] --&gt; S1[\"LinearSVC (sklearn)     \"]\n    DATA --&gt; S2[\"Smoothed SVM (L-BFGS-B)     \"]\n    DATA --&gt; S3[\"Distributed Smooth SVM     \"]\n\n    S1 --&gt; R1[\"Baseline Accuracy     \"]\n    S2 --&gt; R2[\"Single-node Accuracy     \"]\n    S3 --&gt; |\"K=5 workers\"| R3[\"Distributed Accuracy     \"]\n\n    S3 --&gt; W1[\"Worker 1     \"]\n    S3 --&gt; W2[\"Worker 2     \"]\n    S3 --&gt; W3[\"...     \"]\n    S3 --&gt; W4[\"Worker 5     \"]\n\n    style DATA fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style S1   fill:#F5F5F5,color:#424242,stroke:#BDBDBD,stroke-width:2px\n    style S2   fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style S3   fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n    style R1   fill:#F5F5F5,color:#424242,stroke:#BDBDBD\n    style R2   fill:#FFF3E0,color:#E65100,stroke:#FFCC80\n    style R3   fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7\n    style W1   fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7\n    style W2   fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7\n    style W3   fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7\n    style W4   fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7\n\n\n\n\n\n\n\n架構說明： 灰色 = sklearn LinearSVC baseline，橘色 = 單節點 Smoothed SVM（L-BFGS-B 最佳化），綠色 = 分散式版本（K=5 workers，每輪各 worker 計算局部梯度再聚合）。\n\n\n\nSmoothed Hinge Loss / 平滑化 Hinge Loss\nStandard hinge loss is non-differentiable at u=1. The smoothed version uses log-sum-exp to create a differentiable approximation:\n\\[\\ell_\\beta(u) = \\frac{1}{\\beta} \\log\\left(1 + e^{\\beta(1-u)}\\right)\\]\n\n\n\n\n\n\n\n\n\n\nLoss 比較： 灰色虛線 標準 Hinge Loss 在 u=1 處不可微。橘色 Smoothed (β=5) 近似度最高，接近原始 hinge 但處處可微。青色 (β=1) 較平滑但離原始 hinge 較遠。β 越大越接近原始形狀。\n\n\n\nDistributed Gradient Aggregation / 分散式梯度聚合\n# Distributed Smoothed SVM — Communication-efficient gradient\nK = 5  # number of workers\nB = 5  # communication rounds\nbeta = np.zeros(X_train.shape[1])\n\nfor b in range(B):\n    # Pilot gradient (computed on worker 0)\n    grad_pilot = compute_gradient(beta, X_pilot, y_pilot, lambda_, beta_smooth)\n\n    # Each worker computes local gradient correction\n    grad_total = np.zeros_like(beta)\n    for X_k, y_k in zip(split_X, split_y):\n        grad_k = compute_gradient(beta, X_k, y_k, lambda_, beta_smooth)\n        grad_total += (grad_k - grad_pilot) * (len(X_k) / len(X_train))\n\n    # Aggregate: surrogate gradient = pilot + corrections\n    beta -= 0.1 * (grad_pilot + grad_total)\n\n分散式策略： 資料分成 K=5 份。每輪僅需一個 pilot worker 計算完整梯度，其餘 workers 計算「與 pilot 的梯度差」（correction term），最後聚合。相比每個 worker 都傳完整梯度，此方法大幅降低通訊量。\n\n\n\nResults / 結果\n\n\n\nModel\nTest Accuracy\nTraining Time\n\n\n\n\nLinearSVC (sklearn baseline)\n84.93%\n1.52s\n\n\nSmoothed SVM (single-node, L-BFGS-B)\n84.88%\n0.78s\n\n\nDistributed Smoothed SVM (K=5, B=5)\n83.79%\n0.01s\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrade-off 分析： 灰色 LinearSVC 為 baseline（84.93%）。橘色 Smoothed SVM 精度幾乎相同（84.88%），但因 L-BFGS-B 最佳化更高效而更快（0.78s vs 1.52s）。青色 Distributed 版本犧牲 ~1.1% 精度換取 150 倍加速（0.01s），展現了分散式計算在大規模資料下的潛力。\n\n\n\nAblation Study — β Sensitivity / 消融實驗：β 敏感度\n\n\n\nβ (smoothness)\nTest Accuracy\n\n\n\n\n1\n83.69%\n\n\n2\n83.79%\n\n\n5\n83.79%\n\n\n10\n83.73%\n\n\n20\n81.21%\n\n\n\n\n\n\n\n\n\n\n\n\n\nβ 選擇建議： β=2 和 β=5 表現最佳且相同（83.79%）。β 太大（=20）反而導致精度下降至 81.21%，因為過度逼近不可微的原始 hinge loss，造成 gradient 估計不穩定。β=5 為最佳平衡點。"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This site is a working portfolio and notebook.\nIt documents selected projects, visual studies, and experiments as they evolve.\nNot all content represents final results. Some materials are exploratory or incomplete."
  },
  {
    "objectID": "about.html#about-this-site",
    "href": "about.html#about-this-site",
    "title": "About",
    "section": "",
    "text": "This site is a working portfolio and notebook.\nIt documents selected projects, visual studies, and experiments as they evolve.\nNot all content represents final results. Some materials are exploratory or incomplete."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "This is Tzu-Yuan’s Quarto website"
  },
  {
    "objectID": "EPPS6354.html",
    "href": "EPPS6354.html",
    "title": "Information Management",
    "section": "",
    "text": "Name and describe three applications you have used that employed a database system to store and access persistent data. (e.g. airlines, online trade, banking, university system)\nFor the first question, one example that comes to mind is video games. In video games, a player’s level and experience points, as well as the items and equipment they have obtained, are recorded, so the player can still access them the next time they log in. Another example is online shopping. For instance, Amazon records information such as the price of each product, the catalog it belongs to, whether it is eligible for free shipping, and whether it is in stock. A third example is a streaming platform, such as Netflix, which records a user’s region and subscription level. All of this data is stored persistently and can be accessed at a later time.\n\n\n\nPropose three applications in domain projects (e.g. criminology, economics, brain science, etc.) Be sure you include: i. Purpose ii. Functions iii. Simple interface design\n\n\n\n\nThe main purpose of this wardrobe management database is to minimize the time spent choosing outfits before going out.\nFor many people, the difficulty in daily outfit selection is not a lack of clothing, but the need to simultaneously consider colors, styles, occasions, and overall coordination, which leads to a high decision-making cost.\nTherefore, I model the wardrobe as a relational database, which not only records individual clothing items but also describes the relationships between items, allowing outfit selection to be handled in a systematic way.\nBy structuring clothing data, this system aims to transform “rethinking what to wear every day” into “quickly selecting optimal combinations from a database.”\n\n\n\nIn this system, each clothing item is treated as a data entity and described using a set of attributes, such as:\n\ncategory (T-shirts, jeans, outerwear, shoes),\ncolor (including the proportion of each color),\nstyle (clean-fit, formal, vintage, sports, etc.),\nmaterial (denim, linen, cotton).\n\nThese attributes are normalized into multiple tables, and many-to-many relationships are used to represent that a single item can belong to multiple styles or be suitable for different occasions.\nThe core function of this database is not only to store items, but to describe the compatibility between items.\nThe system uses compatibility rules to define:\n\nVisual aesthetic constraints, such as avoiding more than three colors in a single outfit and limiting the number of style tags to maintain overall consistency\nClimate adaptability, where combinations are evaluated based on insulation-related variables to ensure balanced warmth between upper and lower body layers, and higher overall insulation is preferred as the temperature decreases\n\nWhen a user selects a specific item (for example, a pink T-shirt), the system can immediately recommend other highly compatible items (such as light blue jeans and white sneakers) based on database relationships and rules, and rank these combinations by compatibility score to help the user make decisions more efficiently.\nIn addition, as data accumulates, the system can analyze the overall structure of the wardrobe, such as:\n\nWhether certain styles or clothing categories are lacking\nWhether colors or item types are overly concentrated\nWhether newly purchased items overlap in function with existing ones\nWhich older items have not been used for a long time and could be considered for removal\n\nThis allows the wardrobe to function not just as an item list, but as a system that can be queried, analyzed, and optimized, and that can be extended to daily life applications such as outfit recommendations and purchase decision support.\nThis problem is particularly well suited for a relational database, because outfit selection inherently involves structured data and many-to-many relationships (such as items, styles, and compatibility rules), which can be efficiently combined and analyzed through relational queries.\n\n\n\nWhen users enter the system, the home page displays a table view of all items in the wardrobe, including basic information such as category, color, style, material, and seasonality. The interface supports multi-select functionality.\nUsers can select one or more items they plan to wear and submit their selection to generate outfit results.\nBased on the selected items and the compatibility rules stored in the database, the system generates multiple outfit candidates.\nThe outfit results page provides different sorting options, such as sorting by comfort score, aesthetic score, or climate fit score.\nEach outfit displays its corresponding numerical scores, allowing users to quickly compare options and select the most suitable combination without repeatedly trying on clothes or overthinking the decision.\nThe interface supports fast decision-making: select items → generate outfits → sort by scores → pick the best match.\n\n\n\n\n\n\nThe purpose of this 3D printing farm database is to systematize the entire workflow—from customer order intake to automated estimation, machine scheduling, and progress tracking—so the farm can operate efficiently as order volume grows. The goals are to shorten turnaround time, reduce human scheduling errors, improve machine utilization, and maximize profitability.\nIn practice, 3D printing orders vary widely (model size, material, resolution, multi-color requirements, and post-processing such as painting). If pricing and scheduling rely on manual judgment, it is easy to underestimate time/cost, assign the wrong machine, or create bottlenecks in the order queue. Therefore, this system uses a relational database to store orders, machine capabilities, material usage, and scheduling states in a structured way, enabling fast and consistent decisions through rules and queries.\n\n\n\nOrder intake & requirement tagging (Order Intake & Requirement Tagging)\nWhen a customer submits an order, the system stores it as an order record with structured attributes, such as:\n\nModel size and volume (bounding box / volume)\nPrinting type (FDM / SLA)\nResolution settings (layer height / resolution)\nMulti-color requirement (multi-color)\nMaterial type (material type)\nPost-processing needs (post-processing, e.g., painting/sanding)\nOther customization requests (stored as tags)\n\nThese fields can be normalized into multiple tables, with many-to-many relationships used to represent that a single order can have multiple requirement tags.\nPer-machine estimation (Per-Machine Estimation)\nThe key is not only to calculate an overall price for the order, but to estimate how the same order would perform on different machines, since time, cost, and completion time may vary by machine. This supports better machine assignment and scheduling decisions.\nFor each candidate machine, the system applies pricing rules or an estimation model to perform per-machine estimation, including:\n\nEstimated print time (estimated print time)\nEstimated material usage (estimated material usage)\nMachine-specific estimated cost & quote (machine-specific estimated cost & quote)\nEstimated completion time (estimated completion time, considering current workload)\n\nThe system stores these “order × machine” estimates for querying and ranking using different objective functions, such as lowest cost, earliest completion, or the most stable option within a deadline.\nOrder queue & status tracking (Order Queue & Status Tracking)\nAll orders are automatically added to an order queue (order list), and each order maintains a clear status, such as:\n\npending\nqueued\nprinting\npost-processing\ncompleted\nfailed\n\nManagers can query:\n\nWhat is currently in the queue and its priority\nWhich orders are printing vs. waiting for machines\nWhich failed orders require reprinting or manual intervention\n\nMachine capability modeling & assignment recommendations (Machine Capability & Assignment)\nThe database stores each machine’s capabilities and constraints, such as: - Machine type: multi-color / single-color / SLA / FDM - Maximum build volume (max build volume) - Supported materials (supported materials) - Speed/quality profile (speed/quality profile) - Current workload and availability (workload & availability)\nWhen a new order arrives, the system first performs constraint filtering (e.g., size, material, multi-color requirements) to identify feasible machines, then uses per-machine estimation to generate recommended assignments, for example:\n\nEarliest completion time (earliest completion time)\nLowest estimated cost (lowest estimated cost)\nBalanced option (deadline + stability)\n\nThis turns scheduling into a decision-support process rather than manual guesswork.\n\n\n\nOn the customer side, the system provides a customer order page where users can upload a 3D model or specify printing requirements such as size, material, resolution, multi-color options, and post-processing needs. Based on this information, the system automatically returns an estimated price and an estimated delivery time.\nOn the admin side, the system offers an order dashboard that displays the current order queue and order statuses. Administrators can sort or filter orders by deadline, priority, or processing status to manage workflow more efficiently.\nThe system also includes a machine dashboard that lists all available machines along with their machine type, maximum build volume, supported materials, current workload, and estimated availability. This allows operators to quickly understand machine capacity and constraints.\nWhen an order is selected, the scheduling view presents a list of candidate machines that can fulfill the order. For each candidate machine, the system displays the estimated print time, estimated material usage, machine-specific cost and quote, and estimated completion time. The interface supports one-click sorting options, such as fastest, cheapest, or most stable, to assist administrators in making assignment decisions.\nThe interface supports efficient operations: submit order → per-machine estimation → queue order → recommend machines → schedule & track progress.\n\n\n\n\n\n\nThe purpose of this system is to manage the core information of a farm—such as fields, crop types, growth stages, and irrigation equipment—using a relational database.\nAt the same time, the system retrieves and stores weather data through APIs provided by weather forecast services, and combines this information with a set of irrigation rules to automatically generate a daily irrigation schedule.\nThe goal is to reduce manual decision-making costs while improving water-use efficiency and consistency in crop management.\n\n\n\nIn this system, the database is not used only for data storage. Its core function is to integrate internal farm information with external weather data and automatically generate irrigation decisions based on predefined rules.\nCore Data Management\nThe system uses a relational schema to manage the main entities of the farm, including:\n\nField: field ID, location, area, and the crop currently planted\nCrop: crop type and its basic water requirements\nGrowth Stage: stages such as germination, growth, flowering, and fruiting, each with different water needs\nIrrigation Equipment: equipment type (e.g., drip irrigation, sprinkler), flow rate or efficiency factor, and availability status\n\nThese entities are connected through relationships. For example, each field is associated with a specific crop and a current growth stage, and can be assigned available irrigation equipment.\nWeather Data Integration\nThe system retrieves weather information through external weather forecast APIs, such as:\n\nPredicted rainfall amount\nProbability of precipitation\nTemperature range\n\nThis weather data is stored in the database and used as an important input for daily irrigation decisions, without requiring manual input from users.\nIrrigation Rules and Schedule Generation\nThe system maintains a set of irrigation rules that describe irrigation requirements under different conditions, such as:\n\nCrop type × growth stage → recommended baseline irrigation amount\nIf predicted rainfall exceeds a certain threshold → automatically reduce or cancel irrigation for the day\nDifferences in irrigation equipment efficiency → adjust actual irrigation duration\n\nWhen the daily scheduling process runs, the system combines:\n\nThe crop type and growth stage of each field\nThe weather forecast for the day\nThe availability and efficiency of irrigation equipment\n\nBased on this information, the system automatically generates a daily irrigation schedule, indicating whether each field requires irrigation and the recommended water amount or irrigation time.\n\n\n\nWhen users enter the system, the home page displays a table view of all fields on the farm, including the current crop type, growth stage, and the system’s irrigation recommendation for the day.\nUsers can generate the daily irrigation schedule with a single action. Based on field information, weather forecasts, and irrigation rules, the system lists which fields require irrigation and provides recommended water amounts or irrigation durations.\nThe schedule is presented in a simple list format, allowing users to quickly review and execute irrigation tasks. After completion, users can mark irrigation status for record-keeping and future reference.\n\n\n\n\n\nWhat are the things current database system cannot do?\nCurrent database systems are not capable of understanding the semantics behind data. As a result, in more complex applications, they often rely on manually defined rules or continuously adjusted weights to produce reasonable outputs. In addition, databases are limited in handling cross-context decision-making, where multiple competing objectives must be balanced simultaneously.\nFor example, in a wardrobe management database, the system can evaluate outfits based on structured criteria such as color combinations, style tags, material properties, and weather conditions. It can assign scores for factors like aesthetic quality, comfort, and climate suitability, and generate multiple candidate outfits that satisfy predefined rules. However, the database cannot determine which outfit represents the optimal balance among being visually appealing, comfortable, and suitable for the weather.\nThis limitation arises because preferences such as “looking good” or “feeling comfortable” are inherently subjective and context-dependent, and there is no single optimal solution that applies to all users or situations. Therefore, the role of the database is not to make the final decision, but to support decision-making by filtering infeasible options, structuring relevant information, and presenting comparable alternatives with transparent evaluation metrics.\nUltimately, the final choice must be made by the user, who can decide whether to prioritize comfort, aesthetics, or climate suitability in a given context. This highlights a fundamental limitation of current database systems: they are effective at decision support, but they cannot replace human judgment in complex, value-driven decisions.\n\n\n\nDescribe at least three tables that might be used to store information in a social-network/social media system such as Twitter or Reddit.\nA social-network or social media system such as Twitter or Reddit may be supported by at least the following three core tables:\n1. User Table\nThe user table stores basic information about users, such as: - user_id - username - account creation time - profile metadata (e.g., bio or status)\nThis table represents the identities of users and serves as a reference for other tables in the system.\n2. Post Table\nThe post table stores content created by users, such as:\n\npost_id\nauthor_id (foreign key referencing the User table)\ncontent\ntimestamp\n\nEach post is associated with a specific user, forming a one-to-many relationship between users and posts.\n3. Comment Table\nThe comment table stores replies to posts (or other comments), such as:\n\ncomment_id\npost_id (foreign key referencing the Post table)\nauthor_id\ncontent\ntimestamp\n\nThis table supports threaded discussions and allows multiple users to participate in conversations under the same post.\nThese tables are separated to support relational queries, maintain data consistency, and enable efficient retrieval of users, posts, and discussion threads.\n\n\n\n\n\n\nWhat are the differences between relation schema, relation, and instance? Give an example using the university database to illustrate.\n\nRelation Schema = The logical structure of a relation: a list of attribute names and their domains. It does not change over time.\nExample: instructor(ID, name, dept_name, salary)\nRelation = Informally used to refer to both the schema and instance together.\nExample: “The department relation” can refer to either the schema department(dept_name, building, budget) or the actual data it currently holds.\nInstance = A snapshot of the actual data in a relation at a given point in time. It changes as tuples are inserted, updated, or deleted.\nExample: The department relation instance in Figure 2.5 contains 7 tuples. If the university adds a “Data Science” department, the instance grows to 8 tuples, but the schema remains department(dept_name, building, budget).\n\n\n\n\nDraw a schema diagram for the following bank database. Identify primary keys (underlined) and foreign keys.\nThe bank database consists of the following relations:\n\nbranch(branch_name, branch_city, assets)\ncustomer(ID, customer_name, customer_street, customer_city)\nloan(loan_number, branch_name, amount)\nborrower(ID, loan_number)\naccount(account_number, branch_name, balance)\ndepositor(ID, account_number)\n\n\n\n\nBank Database Schema Diagram\n\n\n\n\n\nDescribe two ways artificial intelligence or LLM can assist in managing or querying a database. In your answer, briefly explain how each method improves efficiency or accuracy compared to traditional (non-AI) approaches. (3–5 sentences)\n\nNatural Language to SQL (Querying): LLMs can translate plain language questions directly into executable SQL queries, lowering the barrier for non-technical users and reducing syntax errors compared to writing SQL manually.\nAI-Driven Database Tuning (Managing): LLMs can automatically analyze slow queries and recommend index optimizations, replacing the traditionally time-consuming process of a DBA manually examining query logs and execution plans.\n\nOverall, both approaches reduce the need for specialized expertise and allow faster, more accurate database operations compared to traditional manual methods.\n\n\n\n\n\n\nOpen the Online SQL interpreter and load the university database.\n\n\n\nWrite SQL codes to get a list of: i. Student IDs, ii. Instructors, iii. Departments\n\n\n\nQ2 — Student IDs (from takes), Instructors, and Departments\n\n\n\n\n\nWrite SQL codes to do the following queries:\ni. Find the ID and name of each student who has taken at least one Comp. Sci. course; make sure there are no duplicate names in the result.\n\n\n\nQ3i — Students who took at least one Comp. Sci. course\n\n\nii. Add grades to the list\n\n\n\nQ3ii — Add grades to the result\n\n\niii. Find the ID and name of each student who has not taken any course offered before 2017.\n\n\n\nQ3iii — Students who have not taken any course before 2017\n\n\niv. For each department, find the maximum salary of instructors in that department.\n\n\n\nQ3iv — Maximum instructor salary per department\n\n\nv. Find the lowest, across all departments, of the per-department maximum salary computed by the preceding query.\n\n\n\nQ3v — Lowest of the per-department maximum salaries\n\n\nvi. Add names to the list\n\n\n\nQ3vi — Add instructor names to the result\n\n\n\n\n\nFind instructor (with name and ID) who has never given an A grade in any course she or he has taught. (Instructors who have never taught a course trivially satisfy this condition.)\n\n\n\nQ4 — Instructors who have never given an A grade"
  },
  {
    "objectID": "EPPS6354.html#assignments",
    "href": "EPPS6354.html#assignments",
    "title": "Information Management",
    "section": "",
    "text": "Name and describe three applications you have used that employed a database system to store and access persistent data. (e.g. airlines, online trade, banking, university system)\nFor the first question, one example that comes to mind is video games. In video games, a player’s level and experience points, as well as the items and equipment they have obtained, are recorded, so the player can still access them the next time they log in. Another example is online shopping. For instance, Amazon records information such as the price of each product, the catalog it belongs to, whether it is eligible for free shipping, and whether it is in stock. A third example is a streaming platform, such as Netflix, which records a user’s region and subscription level. All of this data is stored persistently and can be accessed at a later time.\n\n\n\nPropose three applications in domain projects (e.g. criminology, economics, brain science, etc.) Be sure you include: i. Purpose ii. Functions iii. Simple interface design\n\n\n\n\nThe main purpose of this wardrobe management database is to minimize the time spent choosing outfits before going out.\nFor many people, the difficulty in daily outfit selection is not a lack of clothing, but the need to simultaneously consider colors, styles, occasions, and overall coordination, which leads to a high decision-making cost.\nTherefore, I model the wardrobe as a relational database, which not only records individual clothing items but also describes the relationships between items, allowing outfit selection to be handled in a systematic way.\nBy structuring clothing data, this system aims to transform “rethinking what to wear every day” into “quickly selecting optimal combinations from a database.”\n\n\n\nIn this system, each clothing item is treated as a data entity and described using a set of attributes, such as:\n\ncategory (T-shirts, jeans, outerwear, shoes),\ncolor (including the proportion of each color),\nstyle (clean-fit, formal, vintage, sports, etc.),\nmaterial (denim, linen, cotton).\n\nThese attributes are normalized into multiple tables, and many-to-many relationships are used to represent that a single item can belong to multiple styles or be suitable for different occasions.\nThe core function of this database is not only to store items, but to describe the compatibility between items.\nThe system uses compatibility rules to define:\n\nVisual aesthetic constraints, such as avoiding more than three colors in a single outfit and limiting the number of style tags to maintain overall consistency\nClimate adaptability, where combinations are evaluated based on insulation-related variables to ensure balanced warmth between upper and lower body layers, and higher overall insulation is preferred as the temperature decreases\n\nWhen a user selects a specific item (for example, a pink T-shirt), the system can immediately recommend other highly compatible items (such as light blue jeans and white sneakers) based on database relationships and rules, and rank these combinations by compatibility score to help the user make decisions more efficiently.\nIn addition, as data accumulates, the system can analyze the overall structure of the wardrobe, such as:\n\nWhether certain styles or clothing categories are lacking\nWhether colors or item types are overly concentrated\nWhether newly purchased items overlap in function with existing ones\nWhich older items have not been used for a long time and could be considered for removal\n\nThis allows the wardrobe to function not just as an item list, but as a system that can be queried, analyzed, and optimized, and that can be extended to daily life applications such as outfit recommendations and purchase decision support.\nThis problem is particularly well suited for a relational database, because outfit selection inherently involves structured data and many-to-many relationships (such as items, styles, and compatibility rules), which can be efficiently combined and analyzed through relational queries.\n\n\n\nWhen users enter the system, the home page displays a table view of all items in the wardrobe, including basic information such as category, color, style, material, and seasonality. The interface supports multi-select functionality.\nUsers can select one or more items they plan to wear and submit their selection to generate outfit results.\nBased on the selected items and the compatibility rules stored in the database, the system generates multiple outfit candidates.\nThe outfit results page provides different sorting options, such as sorting by comfort score, aesthetic score, or climate fit score.\nEach outfit displays its corresponding numerical scores, allowing users to quickly compare options and select the most suitable combination without repeatedly trying on clothes or overthinking the decision.\nThe interface supports fast decision-making: select items → generate outfits → sort by scores → pick the best match.\n\n\n\n\n\n\nThe purpose of this 3D printing farm database is to systematize the entire workflow—from customer order intake to automated estimation, machine scheduling, and progress tracking—so the farm can operate efficiently as order volume grows. The goals are to shorten turnaround time, reduce human scheduling errors, improve machine utilization, and maximize profitability.\nIn practice, 3D printing orders vary widely (model size, material, resolution, multi-color requirements, and post-processing such as painting). If pricing and scheduling rely on manual judgment, it is easy to underestimate time/cost, assign the wrong machine, or create bottlenecks in the order queue. Therefore, this system uses a relational database to store orders, machine capabilities, material usage, and scheduling states in a structured way, enabling fast and consistent decisions through rules and queries.\n\n\n\nOrder intake & requirement tagging (Order Intake & Requirement Tagging)\nWhen a customer submits an order, the system stores it as an order record with structured attributes, such as:\n\nModel size and volume (bounding box / volume)\nPrinting type (FDM / SLA)\nResolution settings (layer height / resolution)\nMulti-color requirement (multi-color)\nMaterial type (material type)\nPost-processing needs (post-processing, e.g., painting/sanding)\nOther customization requests (stored as tags)\n\nThese fields can be normalized into multiple tables, with many-to-many relationships used to represent that a single order can have multiple requirement tags.\nPer-machine estimation (Per-Machine Estimation)\nThe key is not only to calculate an overall price for the order, but to estimate how the same order would perform on different machines, since time, cost, and completion time may vary by machine. This supports better machine assignment and scheduling decisions.\nFor each candidate machine, the system applies pricing rules or an estimation model to perform per-machine estimation, including:\n\nEstimated print time (estimated print time)\nEstimated material usage (estimated material usage)\nMachine-specific estimated cost & quote (machine-specific estimated cost & quote)\nEstimated completion time (estimated completion time, considering current workload)\n\nThe system stores these “order × machine” estimates for querying and ranking using different objective functions, such as lowest cost, earliest completion, or the most stable option within a deadline.\nOrder queue & status tracking (Order Queue & Status Tracking)\nAll orders are automatically added to an order queue (order list), and each order maintains a clear status, such as:\n\npending\nqueued\nprinting\npost-processing\ncompleted\nfailed\n\nManagers can query:\n\nWhat is currently in the queue and its priority\nWhich orders are printing vs. waiting for machines\nWhich failed orders require reprinting or manual intervention\n\nMachine capability modeling & assignment recommendations (Machine Capability & Assignment)\nThe database stores each machine’s capabilities and constraints, such as: - Machine type: multi-color / single-color / SLA / FDM - Maximum build volume (max build volume) - Supported materials (supported materials) - Speed/quality profile (speed/quality profile) - Current workload and availability (workload & availability)\nWhen a new order arrives, the system first performs constraint filtering (e.g., size, material, multi-color requirements) to identify feasible machines, then uses per-machine estimation to generate recommended assignments, for example:\n\nEarliest completion time (earliest completion time)\nLowest estimated cost (lowest estimated cost)\nBalanced option (deadline + stability)\n\nThis turns scheduling into a decision-support process rather than manual guesswork.\n\n\n\nOn the customer side, the system provides a customer order page where users can upload a 3D model or specify printing requirements such as size, material, resolution, multi-color options, and post-processing needs. Based on this information, the system automatically returns an estimated price and an estimated delivery time.\nOn the admin side, the system offers an order dashboard that displays the current order queue and order statuses. Administrators can sort or filter orders by deadline, priority, or processing status to manage workflow more efficiently.\nThe system also includes a machine dashboard that lists all available machines along with their machine type, maximum build volume, supported materials, current workload, and estimated availability. This allows operators to quickly understand machine capacity and constraints.\nWhen an order is selected, the scheduling view presents a list of candidate machines that can fulfill the order. For each candidate machine, the system displays the estimated print time, estimated material usage, machine-specific cost and quote, and estimated completion time. The interface supports one-click sorting options, such as fastest, cheapest, or most stable, to assist administrators in making assignment decisions.\nThe interface supports efficient operations: submit order → per-machine estimation → queue order → recommend machines → schedule & track progress.\n\n\n\n\n\n\nThe purpose of this system is to manage the core information of a farm—such as fields, crop types, growth stages, and irrigation equipment—using a relational database.\nAt the same time, the system retrieves and stores weather data through APIs provided by weather forecast services, and combines this information with a set of irrigation rules to automatically generate a daily irrigation schedule.\nThe goal is to reduce manual decision-making costs while improving water-use efficiency and consistency in crop management.\n\n\n\nIn this system, the database is not used only for data storage. Its core function is to integrate internal farm information with external weather data and automatically generate irrigation decisions based on predefined rules.\nCore Data Management\nThe system uses a relational schema to manage the main entities of the farm, including:\n\nField: field ID, location, area, and the crop currently planted\nCrop: crop type and its basic water requirements\nGrowth Stage: stages such as germination, growth, flowering, and fruiting, each with different water needs\nIrrigation Equipment: equipment type (e.g., drip irrigation, sprinkler), flow rate or efficiency factor, and availability status\n\nThese entities are connected through relationships. For example, each field is associated with a specific crop and a current growth stage, and can be assigned available irrigation equipment.\nWeather Data Integration\nThe system retrieves weather information through external weather forecast APIs, such as:\n\nPredicted rainfall amount\nProbability of precipitation\nTemperature range\n\nThis weather data is stored in the database and used as an important input for daily irrigation decisions, without requiring manual input from users.\nIrrigation Rules and Schedule Generation\nThe system maintains a set of irrigation rules that describe irrigation requirements under different conditions, such as:\n\nCrop type × growth stage → recommended baseline irrigation amount\nIf predicted rainfall exceeds a certain threshold → automatically reduce or cancel irrigation for the day\nDifferences in irrigation equipment efficiency → adjust actual irrigation duration\n\nWhen the daily scheduling process runs, the system combines:\n\nThe crop type and growth stage of each field\nThe weather forecast for the day\nThe availability and efficiency of irrigation equipment\n\nBased on this information, the system automatically generates a daily irrigation schedule, indicating whether each field requires irrigation and the recommended water amount or irrigation time.\n\n\n\nWhen users enter the system, the home page displays a table view of all fields on the farm, including the current crop type, growth stage, and the system’s irrigation recommendation for the day.\nUsers can generate the daily irrigation schedule with a single action. Based on field information, weather forecasts, and irrigation rules, the system lists which fields require irrigation and provides recommended water amounts or irrigation durations.\nThe schedule is presented in a simple list format, allowing users to quickly review and execute irrigation tasks. After completion, users can mark irrigation status for record-keeping and future reference.\n\n\n\n\n\nWhat are the things current database system cannot do?\nCurrent database systems are not capable of understanding the semantics behind data. As a result, in more complex applications, they often rely on manually defined rules or continuously adjusted weights to produce reasonable outputs. In addition, databases are limited in handling cross-context decision-making, where multiple competing objectives must be balanced simultaneously.\nFor example, in a wardrobe management database, the system can evaluate outfits based on structured criteria such as color combinations, style tags, material properties, and weather conditions. It can assign scores for factors like aesthetic quality, comfort, and climate suitability, and generate multiple candidate outfits that satisfy predefined rules. However, the database cannot determine which outfit represents the optimal balance among being visually appealing, comfortable, and suitable for the weather.\nThis limitation arises because preferences such as “looking good” or “feeling comfortable” are inherently subjective and context-dependent, and there is no single optimal solution that applies to all users or situations. Therefore, the role of the database is not to make the final decision, but to support decision-making by filtering infeasible options, structuring relevant information, and presenting comparable alternatives with transparent evaluation metrics.\nUltimately, the final choice must be made by the user, who can decide whether to prioritize comfort, aesthetics, or climate suitability in a given context. This highlights a fundamental limitation of current database systems: they are effective at decision support, but they cannot replace human judgment in complex, value-driven decisions.\n\n\n\nDescribe at least three tables that might be used to store information in a social-network/social media system such as Twitter or Reddit.\nA social-network or social media system such as Twitter or Reddit may be supported by at least the following three core tables:\n1. User Table\nThe user table stores basic information about users, such as: - user_id - username - account creation time - profile metadata (e.g., bio or status)\nThis table represents the identities of users and serves as a reference for other tables in the system.\n2. Post Table\nThe post table stores content created by users, such as:\n\npost_id\nauthor_id (foreign key referencing the User table)\ncontent\ntimestamp\n\nEach post is associated with a specific user, forming a one-to-many relationship between users and posts.\n3. Comment Table\nThe comment table stores replies to posts (or other comments), such as:\n\ncomment_id\npost_id (foreign key referencing the Post table)\nauthor_id\ncontent\ntimestamp\n\nThis table supports threaded discussions and allows multiple users to participate in conversations under the same post.\nThese tables are separated to support relational queries, maintain data consistency, and enable efficient retrieval of users, posts, and discussion threads.\n\n\n\n\n\n\nWhat are the differences between relation schema, relation, and instance? Give an example using the university database to illustrate.\n\nRelation Schema = The logical structure of a relation: a list of attribute names and their domains. It does not change over time.\nExample: instructor(ID, name, dept_name, salary)\nRelation = Informally used to refer to both the schema and instance together.\nExample: “The department relation” can refer to either the schema department(dept_name, building, budget) or the actual data it currently holds.\nInstance = A snapshot of the actual data in a relation at a given point in time. It changes as tuples are inserted, updated, or deleted.\nExample: The department relation instance in Figure 2.5 contains 7 tuples. If the university adds a “Data Science” department, the instance grows to 8 tuples, but the schema remains department(dept_name, building, budget).\n\n\n\n\nDraw a schema diagram for the following bank database. Identify primary keys (underlined) and foreign keys.\nThe bank database consists of the following relations:\n\nbranch(branch_name, branch_city, assets)\ncustomer(ID, customer_name, customer_street, customer_city)\nloan(loan_number, branch_name, amount)\nborrower(ID, loan_number)\naccount(account_number, branch_name, balance)\ndepositor(ID, account_number)\n\n\n\n\nBank Database Schema Diagram\n\n\n\n\n\nDescribe two ways artificial intelligence or LLM can assist in managing or querying a database. In your answer, briefly explain how each method improves efficiency or accuracy compared to traditional (non-AI) approaches. (3–5 sentences)\n\nNatural Language to SQL (Querying): LLMs can translate plain language questions directly into executable SQL queries, lowering the barrier for non-technical users and reducing syntax errors compared to writing SQL manually.\nAI-Driven Database Tuning (Managing): LLMs can automatically analyze slow queries and recommend index optimizations, replacing the traditionally time-consuming process of a DBA manually examining query logs and execution plans.\n\nOverall, both approaches reduce the need for specialized expertise and allow faster, more accurate database operations compared to traditional manual methods.\n\n\n\n\n\n\nOpen the Online SQL interpreter and load the university database.\n\n\n\nWrite SQL codes to get a list of: i. Student IDs, ii. Instructors, iii. Departments\n\n\n\nQ2 — Student IDs (from takes), Instructors, and Departments\n\n\n\n\n\nWrite SQL codes to do the following queries:\ni. Find the ID and name of each student who has taken at least one Comp. Sci. course; make sure there are no duplicate names in the result.\n\n\n\nQ3i — Students who took at least one Comp. Sci. course\n\n\nii. Add grades to the list\n\n\n\nQ3ii — Add grades to the result\n\n\niii. Find the ID and name of each student who has not taken any course offered before 2017.\n\n\n\nQ3iii — Students who have not taken any course before 2017\n\n\niv. For each department, find the maximum salary of instructors in that department.\n\n\n\nQ3iv — Maximum instructor salary per department\n\n\nv. Find the lowest, across all departments, of the per-department maximum salary computed by the preceding query.\n\n\n\nQ3v — Lowest of the per-department maximum salaries\n\n\nvi. Add names to the list\n\n\n\nQ3vi — Add instructor names to the result\n\n\n\n\n\nFind instructor (with name and ID) who has never given an A grade in any course she or he has taught. (Instructors who have never taught a course trivially satisfy this condition.)\n\n\n\nQ4 — Instructors who have never given an A grade"
  },
  {
    "objectID": "EPPS6354.html#final-project",
    "href": "EPPS6354.html#final-project",
    "title": "Information Management",
    "section": "Final Project",
    "text": "Final Project\n\nWeather- and Occasion-Aware Wardrobe Database with Rule-Based Outfit Recommendation\n\nProject Overview\nA single-user wardrobe management system. Each morning, the system reads the user’s calendar (to determine the occasion) and the current weather, filters out unwearable items (dirty or archived), and ranks candidate outfits using a rule-based scoring engine built on color theory, fabric compatibility, and style coherence. The output is the best recommended outfit plus 3–5 ranked alternatives, each with a score and explanation.\nCore workflow: Wake up → Read calendar (occasion) + weather → Filter wearable items → Score outfit combinations → Output recommendation\nThree scoring dimensions:\n\nColor — 60/30/10 color theory; each item carries a primary / secondary / accent color role\nFabric — same-fabric bonus, mixed-fabric reasonableness, warmth adequacy relative to weather\nStyle — style-tag consistency across items, and style–occasion fit scores\n\n\n\nDatabase Tables\n\nReference Tables\n\n\n\n\n\n\n\n\nTable\nKey Columns\nDescription\n\n\n\n\nUser\nuser_id PK, name\nStores user identity\n\n\nCategory\ncategory_id PK, name\nClothing categories: top, bottom, shoes, outerwear, accessory\n\n\nColor\ncolor_id PK, name\nColor names (black, white, navy, grey, beige…)\n\n\nFabric\nfabric_id PK, name, warmth_weight, breathability\nFabric type with warmth (0–100) and breathability (0–100) ratings\n\n\nStyleTag\nstyle_id PK, name\nStyle labels: street, formal, clean fit, simple, blokecore…\n\n\nOccasion\noccasion_id PK, name, target_formality_min, target_formality_max\nOccasion with required formality range\n\n\n\n\n\nStyle & Calendar\n\n\n\n\n\n\n\n\nTable\nKey Columns\nDescription\n\n\n\n\nStyleOccasionFit\nstyle_id FK, occasion_id FK, fit_score\nFit score (0–100) between a style and an occasion (PK: style_id + occasion_id)\n\n\nCalendarEvent\nevent_id PK, user_id FK, occasion_id FK, event_date, start_time\nCalendar entry mapped to an occasion; drives automatic occasion detection\n\n\n\n\n\nClothing Item & Tags\n\n\n\n\n\n\n\n\nTable\nKey Columns\nDescription\n\n\n\n\nClothingItem\nitem_id PK, user_id FK, category_id FK, fabric_id FK, formality_score, warmth_score, clean_score, status\nMain clothing table; recommendable only if clean_score &gt; 0 and status = 'active'\n\n\nItemStyle\nitem_id FK, style_id FK\nMany-to-many: clothing item to style tags\n\n\nItemColor\nitem_id FK, color_id FK, role (primary/secondary/accent)\nColor role assignment per item; max 3 colors per item\n\n\n\n\n\nWeather\n\n\n\n\n\n\n\n\nTable\nKey Columns\nDescription\n\n\n\n\nWeatherSnapshot\nweather_id PK, location, temp_c, feels_like_c, humidity, wind_speed, precip_mm\nReal-time weather data snapshot\n\n\nWeatherCondition\ncondition_id PK, name, temp/precip/wind/humidity ranges\nNamed weather condition (e.g. cold_rainy, hot_humid) used for rule matching\n\n\n\n\n\nOutfit\n\n\n\n\n\n\n\n\nTable\nKey Columns\nDescription\n\n\n\n\nOutfit\noutfit_id PK, user_id FK, occasion_id FK, weather_id FK, total_score, explanation\nGenerated outfit with total score and explanation\n\n\nOutfitItem\nPK(outfit_id, slot, layer_order), item_id FK\nOutfit detail; supports multi-layer dressing within the same slot\n\n\n\n\n\nScoring Rules\n\n\n\n\n\n\n\n\nTable\nKey Columns\nDescription\n\n\n\n\nOutfitScoringRule\nscore_rule_id PK, rule_type, score_delta, condition_json, valid_from, valid_until\nScoring rule with optional expiry date to support trend-based rules\n\n\nWearDegradeRule\ndegrade_rule_id PK, category_id FK, occasion_id FK (nullable), condition_id FK (nullable), delta_clean_score\nClean score deduction rule; multiple matching rules are stacked per wear event\n\n\n\n\n\nWear Tracking & Laundry\n\n\n\n\n\n\n\n\nTable\nKey Columns\nDescription\n\n\n\n\nWearEvent\nevent_id PK, user_id FK, outfit_id FK, occasion_id FK, weather_id FK, worn_at\nRecords each time an outfit is worn\n\n\nWearEventItem\nPK(event_id, item_id), delta_applied, clean_score_after\nPer-item deduction detail and resulting clean score after wear\n\n\nLaundryBatch\nbatch_id PK, user_id FK, laundry_type (dark/light)\nLaundry batch grouped by color type\n\n\nLaundryBatchItem\nPK(batch_id, item_id), reset_to_score = 100\nResets clean score to 100 upon laundering"
  },
  {
    "objectID": "SDAR.html",
    "href": "SDAR.html",
    "title": "MS of Social Data Analytics & Research",
    "section": "",
    "text": "學歷： 德州大學達拉斯分校 社會數據分析與研究碩士 (Master of Science in Social Data Analytics and Research, UTD)。"
  },
  {
    "objectID": "SDAR.html#about-the-program",
    "href": "SDAR.html#about-the-program",
    "title": "MS of Social Data Analytics & Research",
    "section": "About the Program",
    "text": "About the Program\nThe Master of Science in Social Data Analytics and Research (SDAR) program at The University of Texas at Dallas trains students to collect, manage, analyze, and visualize social and behavioral data. The curriculum spans GIS, data visualization, information management, and quantitative methods — equipping graduates with skills for data-driven decision-making in public policy, social science, and industry."
  },
  {
    "objectID": "SDAR.html#coursework",
    "href": "SDAR.html#coursework",
    "title": "MS of Social Data Analytics & Research",
    "section": "Coursework",
    "text": "Coursework\n\n\nGIS & Python Programming\nGeographic information systems, spatial data analysis, and Python programming — from basic scripting and GUI applications to web scraping and SVD image compression.\n\n\nData Visualization\nPrinciples of effective data visualization, grammar of graphics, and practical chart design — with assignments in R (ggplot2) and critical reviews of visualization literature.\n\n\nInformation Management\nDatabase design, SQL, information systems architecture, and data management strategies for social science research."
  },
  {
    "objectID": "EPPS6356.html",
    "href": "EPPS6356.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Assignment 1\n\n\n\nAssignment 2\n\n\n\nAssignment 3\n\n\n\n\ndata(iris)\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.1     ✔ stringr   1.5.2\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# 1. Divide the dataset into three rectangles based on species.\n# The average of Petal.Length and Petal.Width is the length and width.\n# Draw three rectangles arranged horizontally.\n\n#1\n\nplot_data &lt;- iris %&gt;%\n  mutate(\n    sepal_length_group = cut(\n      Sepal.Length,\n      breaks = c(4, 5.5, 7.0, 8.0),\n      labels = c(\"Small (4.0-5.5)\", \"Medium (5.6-7.0)\", \"Large (7.1-8.0)\"),\n      include.lowest = TRUE\n    )\n  ) %&gt;%\n  group_by(sepal_length_group) %&gt;%\n  summarise(\n    count = n(),\n    avg_petal_length = mean(Petal.Length)\n  ) %&gt;%\n  mutate(\n    xmax = cumsum(count),\n    xmin = xmax - count,\n    x_label_pos = (xmin + xmax) / 2\n  )\n\nggplot(plot_data, aes(ymin = 0)) +\n  geom_rect(\n    aes(\n      xmin = xmin,\n      xmax = xmax,\n      ymax = avg_petal_length,\n      fill = sepal_length_group\n    ),\n    color = \"white\"\n  ) +\n  scale_x_continuous(\n    breaks = plot_data$x_label_pos,\n    labels = plot_data$sepal_length_group,\n    expand = c(0, 0)\n  ) +\n  scale_fill_viridis_d(option = \"D\", direction = -1) +\n  labs(\n    title = \"Average Petal Length by Sepal Length Group\",\n    subtitle = \"Column width is proportional to the number of flowers in each group\",\n    x = \"Count of Flowers in Group\",\n    y = \"Average Petal Length (cm)\",\n    fill = \"Sepal Length Group\"\n  ) +\n  # Apply a clean theme\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 18),\n    legend.position = \"bottom\",\n    panel.grid.major.x = element_blank(), # Remove vertical grid lines\n    panel.grid.minor.x = element_blank()\n  )\n\n\n\n\n\n\n\n\n\n# 2. table with embedded charts\niris_long &lt;- iris %&gt;%\n  pivot_longer(cols = -Species, names_to = \"Measurement\", values_to = \"Value\")\n\nggplot(iris_long, aes(x = Value, fill = Species)) +\n  geom_histogram(color = \"white\", bins = 15) +\n  facet_grid(Species ~ Measurement, scales = \"free\") +\n  scale_fill_manual( #coloring each species\n    values = c(\n      \"setosa\" = \"steelblue\", \n      \"versicolor\" = \"orange\",   \n      \"virginica\" = \"seagreen\"     \n    ) \n    ) + #labels\n      labs(\n        title = \"Distribution of Iris Measurements by Species\",\n        x = \"Measurement Value (cm)\",\n        y = \"Count\"\n      ) +\n  theme_bw() +\n    theme(\n      plot.title = element_text(hjust = 0.5, face = \"bold\"),\n      strip.text.x = element_text(face = \"bold\"),\n      strip.text.y = element_text(face = \"bold\"),\n      panel.border = element_rect(color = \"grey80\", fill = NA),\n      legend.position = \"bottom\"\n    )\n\n\n\n\n\n\n\n\n\n# 3. Extract setona and versicolor from species.\n# Then create df_2 and df_3. Draw a bar plot using petal.width: p1 p2.\n# Finally, use gridExtra to combine the plots.'\nlibrary(\"gridExtra\")\n\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\ndf_2 &lt;- subset(iris, Species %in% \"setosa\")\ndf_3 &lt;- subset(iris, Species %in% \"versicolor\")\ndf_2$id &lt;- 1:nrow(df_2)\ndf_3$id &lt;- 1:nrow(df_3)\n\n\n\np1 = ggplot(df_2, aes(x = factor(id), y = Petal.Width)) +\n  geom_bar(stat = \"identity\", fill = 'red', color = \"black\") +\n  coord_flip() +\n  labs(title = \"setosa\") +\n  theme(\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank() #this was by GPT\n  )\n\n\np2 = ggplot(df_3, aes(x = factor(id), y = Petal.Width)) +\n  geom_bar(stat = \"identity\", fill = \"blue\", color = \"black\") +\n  coord_flip() +\n  labs(title = \"versicolor\")+\n  theme(\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank() #this was by GPT\n  )\n\n\n\ngridExtra::grid.arrange(p1, p2, ncol = 2)\n\n\n\n\n\n\n\n\n\n# 4 Column Chart\n# getting means of Petal length and width for each species\n# and mean sepal length and sepal width\niris_means &lt;- iris %&gt;%\n  group_by(Species) %&gt;%\n  summarise(\n    mean_sepal_length = mean(Sepal.Length),\n    mean_sepal_width = mean(Sepal.Width),\n    mean_petal_length = mean(Petal.Length),\n    mean_petal_width = mean(Petal.Width)\n  ) %&gt;%\n  pivot_longer(\n    cols = -Species,\n    names_to = \"Measurement\",\n    values_to = \"MeanValue\"\n    )\n\nggplot(iris_means, aes(x = Measurement, y = MeanValue, fill = Species)) +\n  geom_col(position = position_dodge(width = 0.8)) + \n  labs(title = \"Mean Iris Measurements by Species\",\n       x = \"Measurement\", y = \"Mean Value\") + \n  theme_minimal(base_size = 12) +\n  scale_fill_manual(values = c(\"steelblue\", \"orange\", \"seagreen\"))\n\n\n\n\n\n\n\n\n\nClass coding competition\n\n\nlibrary(ggplot2)\nmpg &lt;- as.data.frame(mpg)\n#2seater, compact, midsize, minivan, pickup, subcompact, suv scatterplots in one view\nggplot(mpg, aes(x=displ, y=hwy)) +\n  geom_point(color = \"black\") +\n  facet_wrap(~ class) +\n  labs(x=\"displ\",\n       y=\"hwy\") +\n  theme_gray()\n\n\n\n\n\n\n\n\n\n#improving the chart\nggplot(mpg, aes(x=displ, y=hwy)) +\n  geom_point(color = \"blue\", size=2, alpha=0.3) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"#E65100\", linewidth = 0.8) +\n  facet_wrap(~ class) +\n  labs(title=\"Engine Displacement vs Highway MPG by Vehicle Class\",\n       x=\"Engine Displacement (liters)\",\n       y=\"Highway Miles per Gallon (MPG)\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size=16, face=\"bold\"),\n    axis.title.x = element_text(size=12),\n    axis.title.y = element_text(size=12)\n  )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n# GPT was used for picking colors and family.\n# GPT was used for adjusting the format of the code.\nlibrary(ggplot2)\nlibrary(scales)   # for alpha()\n\n\nAttaching package: 'scales'\n\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\ndata(iris)\ncolor1 &lt;- \"#18A3A380\"\ncolor2 &lt;- \"#FF4D8DCC\"\ncolor3 &lt;- \"#7A7A7A\"\ncolor4 &lt;- \"#000000\"\nbase_family &lt;- \"sans\"\n\n# custom theme used across plots\ntheme1 &lt;- function() {\n  theme_minimal(base_family = base_family) +\n    theme(\n      text        = element_text(family = base_family, colour = color4),\n      plot.title  = element_text(face = \"bold\", colour = color4, size = 13),\n      axis.title  = element_text(colour = color4),\n      axis.text   = element_text(colour = color3),\n      panel.grid.major = element_line(color = scales::alpha(color3, 0.3), linetype = \"dotted\"),\n      panel.grid.minor = element_blank()\n    )\n}\n\n\nHisto &lt;- function(){\n  hist(iris$Sepal.Length,\n       main=\"Distribution of Sepal Length (iris)\",\n       col=color1, border=color3)\n}\n\nBar1 &lt;- function(){\n  barplot(table(iris$Species),\n          col=c(color1,color2,color3),\n          border=color4,\n          main=\"Count by Species\",\n          xlab=\"Species\", ylab=\"Count\")\n}\n\nBar2 &lt;- function(){\n  barplot(table(iris$Species),\n          horiz=TRUE,\n          col=c(color1,color2,color3),\n          border=color4,\n          main=\"Count by Species (Horizontal)\",\n          xlab=\"Count\", ylab=\"Species\")\n}\n\nPie &lt;- function(){\n  pie(table(iris$Species),\n      col=c(color1,color2,color3),\n      main=\"Species Composition\",\n      clockwise=TRUE)\n}\n\nBox &lt;- function(){\n  boxplot(Sepal.Length~Species, data=iris,\n          col=c(color1,color2,color3),\n          main=\"Sepal Length by Species\",\n          xlab=\"Species\", ylab=\"Sepal Length (cm)\")\n}\n\nScat &lt;- function(){\n  plot(iris$Petal.Length, iris$Sepal.Length,\n       main=\"Sepal vs Petal Length\",\n       xlab=\"Petal Length (cm)\", ylab=\"Sepal Length (cm)\",\n       pch=19, col=color1)\n}\n\n\nlibrary(gridExtra)\n\npar(mfrow=c(2,3), mar=c(4,4,2.5,1), family=\"sans\")\nHisto(); Bar1(); Bar2(); Pie(); Box(); Scat()\n\n\n\n\n\n\n\n\n\ndraw6 &lt;- function(){\n  par(mfrow=c(2,3), mar=c(4,4,2.5,1), family=base_family)\n  Histo(); Bar1(); Bar2(); Pie(); Box(); Scat()\n}\n\nsave_plot &lt;- function(fmt, file){\n  switch(fmt,\n    pdf  = pdf(file, width=10, height=7, family=base_family),\n    jpg  = jpeg(file, width=2400, height=1600, res=300, quality=95),\n    svg  = svg(file, width=2400, height=1600),\n    tiff = tiff(file, width=2400, height=1600, res=300),\n    bmp  = bmp(file, width=2400, height=1600, res=300), # cannot find bmg, and was told it might be .bmp by GPT\n  )\n  draw6(); invisible(dev.off())\n}\n\n\nsave_plot(\"pdf\",  \"/Users/buttegg/Documents/IAmTryingToUseQuartoDoSthCool/buttegggggggg.github.io/baseR_6plots.pdf\")\nsave_plot(\"jpg\",  \"/Users/buttegg/Documents/IAmTryingToUseQuartoDoSthCool/buttegggggggg.github.io/baseR_6plots.jpg\")\nsave_plot(\"svg\",  \"/Users/buttegg/Documents/IAmTryingToUseQuartoDoSthCool/buttegggggggg.github.io/baseR_6plots.svg\")\nsave_plot(\"tiff\", \"/Users/buttegg/Documents/IAmTryingToUseQuartoDoSthCool/buttegggggggg.github.io/baseR_6plots.tiff\")\nsave_plot(\"bmp\",  \"/Users/buttegg/Documents/IAmTryingToUseQuartoDoSthCool/buttegggggggg.github.io/baseR_6plots.bmp\")\n\nPDF of base R plots\nJPG of base R plots\nSVG of base R plots\nTIFF of base R plots\nBMP of base R plots\n\nggHisto &lt;- ggplot(iris, aes(x=Sepal.Length)) +\n  geom_histogram(fill=color1, color=color3, bins=20) +\n  labs(title=\"Distribution of Sepal Length (iris)\") +\n  theme1()\n\nggBar1 &lt;- ggplot(iris, aes(x=Species, fill=Species)) +\n  geom_bar(color=color4) +\n  scale_fill_manual(values=c(color1,color2,color3)) +\n  labs(title=\"Count by Species\") +\n  theme1() + ggplot2::theme(legend.position=\"none\")\n\nggBar2 &lt;- ggplot(iris, aes(x=Species, fill=Species)) +\n  geom_bar(color=color4) + coord_flip() +\n  scale_fill_manual(values=c(color1,color2,color3)) +\n  labs(title=\"Count by Species (Horizontal)\") +\n  theme1() + ggplot2::theme(legend.position=\"none\")\n\ndf &lt;- as.data.frame(prop.table(table(iris$Species)))\ncolnames(df) &lt;- c(\"Species\",\"prop\")\nggPie &lt;- ggplot(df, aes(x=\"\", y=prop, fill=Species)) +\n  geom_col(width=1, color=NA) + coord_polar(theta=\"y\") +\n  scale_fill_manual(values=c(color1,color2,color3)) +\n  labs(title=\"Species Composition\") +\n  theme1() + ggplot2::theme(axis.text=ggplot2::element_blank(),\n                            axis.title=ggplot2::element_blank(),\n                            panel.grid=ggplot2::element_blank(),\n                            legend.position=\"right\")\n\nggBox &lt;- ggplot(iris, aes(x=Species, y=Sepal.Length, fill=Species)) +\n  geom_boxplot(color=color4) +\n  scale_fill_manual(values=c(color1,color2,color3)) +\n  labs(title=\"Sepal Length by Species\") +\n  theme1() + ggplot2::theme(legend.position=\"none\")\n\nggScat &lt;- ggplot(iris, aes(x=Petal.Length, y=Sepal.Length)) +\n  geom_point(color=color1, size=2) +\n  labs(title=\"Sepal vs Petal Length\") +\n  theme1()\n\n\ngridExtra::grid.arrange(ggHisto, ggBar1, ggBar2, ggPie, ggBox, ggScat, ncol = 3)\n\n\n\n\n\n\n\n\n\ncombo &lt;- gridExtra::arrangeGrob(ggHisto, ggBar1, ggBar2, ggPie, ggBox, ggScat, ncol = 3)\noutdir &lt;- \"/Users/buttegg/Documents/IAmTryingToUseQuartoDoSthCool/buttegggggggg.github.io\"\nformats &lt;- c(\"pdf\", \"jpg\", \"svg\", \"tiff\", \"bmp\")\n\nfor (fmt in formats) {\n  outpath &lt;- file.path(outdir, paste0(\"ggplot_6plots.\", fmt))\n  ggplot2::ggsave(\n    filename = outpath,\n    plot = combo,\n    width = 10, height = 7, dpi = 300\n  )\n}\n\nPDF of ggplot2 plots\nJPG of ggplot2 plots\nSVG of ggplot2 plots\nTIFF of ggplot2 plots\nBMP of ggplot2 plots"
  },
  {
    "objectID": "EPPS6356.html#assignments",
    "href": "EPPS6356.html#assignments",
    "title": "Data Visualization",
    "section": "",
    "text": "Assignment 1\n\n\n\nAssignment 2\n\n\n\nAssignment 3\n\n\n\n\ndata(iris)\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.1     ✔ stringr   1.5.2\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# 1. Divide the dataset into three rectangles based on species.\n# The average of Petal.Length and Petal.Width is the length and width.\n# Draw three rectangles arranged horizontally.\n\n#1\n\nplot_data &lt;- iris %&gt;%\n  mutate(\n    sepal_length_group = cut(\n      Sepal.Length,\n      breaks = c(4, 5.5, 7.0, 8.0),\n      labels = c(\"Small (4.0-5.5)\", \"Medium (5.6-7.0)\", \"Large (7.1-8.0)\"),\n      include.lowest = TRUE\n    )\n  ) %&gt;%\n  group_by(sepal_length_group) %&gt;%\n  summarise(\n    count = n(),\n    avg_petal_length = mean(Petal.Length)\n  ) %&gt;%\n  mutate(\n    xmax = cumsum(count),\n    xmin = xmax - count,\n    x_label_pos = (xmin + xmax) / 2\n  )\n\nggplot(plot_data, aes(ymin = 0)) +\n  geom_rect(\n    aes(\n      xmin = xmin,\n      xmax = xmax,\n      ymax = avg_petal_length,\n      fill = sepal_length_group\n    ),\n    color = \"white\"\n  ) +\n  scale_x_continuous(\n    breaks = plot_data$x_label_pos,\n    labels = plot_data$sepal_length_group,\n    expand = c(0, 0)\n  ) +\n  scale_fill_viridis_d(option = \"D\", direction = -1) +\n  labs(\n    title = \"Average Petal Length by Sepal Length Group\",\n    subtitle = \"Column width is proportional to the number of flowers in each group\",\n    x = \"Count of Flowers in Group\",\n    y = \"Average Petal Length (cm)\",\n    fill = \"Sepal Length Group\"\n  ) +\n  # Apply a clean theme\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 18),\n    legend.position = \"bottom\",\n    panel.grid.major.x = element_blank(), # Remove vertical grid lines\n    panel.grid.minor.x = element_blank()\n  )\n\n\n\n\n\n\n\n\n\n# 2. table with embedded charts\niris_long &lt;- iris %&gt;%\n  pivot_longer(cols = -Species, names_to = \"Measurement\", values_to = \"Value\")\n\nggplot(iris_long, aes(x = Value, fill = Species)) +\n  geom_histogram(color = \"white\", bins = 15) +\n  facet_grid(Species ~ Measurement, scales = \"free\") +\n  scale_fill_manual( #coloring each species\n    values = c(\n      \"setosa\" = \"steelblue\", \n      \"versicolor\" = \"orange\",   \n      \"virginica\" = \"seagreen\"     \n    ) \n    ) + #labels\n      labs(\n        title = \"Distribution of Iris Measurements by Species\",\n        x = \"Measurement Value (cm)\",\n        y = \"Count\"\n      ) +\n  theme_bw() +\n    theme(\n      plot.title = element_text(hjust = 0.5, face = \"bold\"),\n      strip.text.x = element_text(face = \"bold\"),\n      strip.text.y = element_text(face = \"bold\"),\n      panel.border = element_rect(color = \"grey80\", fill = NA),\n      legend.position = \"bottom\"\n    )\n\n\n\n\n\n\n\n\n\n# 3. Extract setona and versicolor from species.\n# Then create df_2 and df_3. Draw a bar plot using petal.width: p1 p2.\n# Finally, use gridExtra to combine the plots.'\nlibrary(\"gridExtra\")\n\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\ndf_2 &lt;- subset(iris, Species %in% \"setosa\")\ndf_3 &lt;- subset(iris, Species %in% \"versicolor\")\ndf_2$id &lt;- 1:nrow(df_2)\ndf_3$id &lt;- 1:nrow(df_3)\n\n\n\np1 = ggplot(df_2, aes(x = factor(id), y = Petal.Width)) +\n  geom_bar(stat = \"identity\", fill = 'red', color = \"black\") +\n  coord_flip() +\n  labs(title = \"setosa\") +\n  theme(\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank() #this was by GPT\n  )\n\n\np2 = ggplot(df_3, aes(x = factor(id), y = Petal.Width)) +\n  geom_bar(stat = \"identity\", fill = \"blue\", color = \"black\") +\n  coord_flip() +\n  labs(title = \"versicolor\")+\n  theme(\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank() #this was by GPT\n  )\n\n\n\ngridExtra::grid.arrange(p1, p2, ncol = 2)\n\n\n\n\n\n\n\n\n\n# 4 Column Chart\n# getting means of Petal length and width for each species\n# and mean sepal length and sepal width\niris_means &lt;- iris %&gt;%\n  group_by(Species) %&gt;%\n  summarise(\n    mean_sepal_length = mean(Sepal.Length),\n    mean_sepal_width = mean(Sepal.Width),\n    mean_petal_length = mean(Petal.Length),\n    mean_petal_width = mean(Petal.Width)\n  ) %&gt;%\n  pivot_longer(\n    cols = -Species,\n    names_to = \"Measurement\",\n    values_to = \"MeanValue\"\n    )\n\nggplot(iris_means, aes(x = Measurement, y = MeanValue, fill = Species)) +\n  geom_col(position = position_dodge(width = 0.8)) + \n  labs(title = \"Mean Iris Measurements by Species\",\n       x = \"Measurement\", y = \"Mean Value\") + \n  theme_minimal(base_size = 12) +\n  scale_fill_manual(values = c(\"steelblue\", \"orange\", \"seagreen\"))\n\n\n\n\n\n\n\n\n\nClass coding competition\n\n\nlibrary(ggplot2)\nmpg &lt;- as.data.frame(mpg)\n#2seater, compact, midsize, minivan, pickup, subcompact, suv scatterplots in one view\nggplot(mpg, aes(x=displ, y=hwy)) +\n  geom_point(color = \"black\") +\n  facet_wrap(~ class) +\n  labs(x=\"displ\",\n       y=\"hwy\") +\n  theme_gray()\n\n\n\n\n\n\n\n\n\n#improving the chart\nggplot(mpg, aes(x=displ, y=hwy)) +\n  geom_point(color = \"blue\", size=2, alpha=0.3) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"#E65100\", linewidth = 0.8) +\n  facet_wrap(~ class) +\n  labs(title=\"Engine Displacement vs Highway MPG by Vehicle Class\",\n       x=\"Engine Displacement (liters)\",\n       y=\"Highway Miles per Gallon (MPG)\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size=16, face=\"bold\"),\n    axis.title.x = element_text(size=12),\n    axis.title.y = element_text(size=12)\n  )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n# GPT was used for picking colors and family.\n# GPT was used for adjusting the format of the code.\nlibrary(ggplot2)\nlibrary(scales)   # for alpha()\n\n\nAttaching package: 'scales'\n\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\ndata(iris)\ncolor1 &lt;- \"#18A3A380\"\ncolor2 &lt;- \"#FF4D8DCC\"\ncolor3 &lt;- \"#7A7A7A\"\ncolor4 &lt;- \"#000000\"\nbase_family &lt;- \"sans\"\n\n# custom theme used across plots\ntheme1 &lt;- function() {\n  theme_minimal(base_family = base_family) +\n    theme(\n      text        = element_text(family = base_family, colour = color4),\n      plot.title  = element_text(face = \"bold\", colour = color4, size = 13),\n      axis.title  = element_text(colour = color4),\n      axis.text   = element_text(colour = color3),\n      panel.grid.major = element_line(color = scales::alpha(color3, 0.3), linetype = \"dotted\"),\n      panel.grid.minor = element_blank()\n    )\n}\n\n\nHisto &lt;- function(){\n  hist(iris$Sepal.Length,\n       main=\"Distribution of Sepal Length (iris)\",\n       col=color1, border=color3)\n}\n\nBar1 &lt;- function(){\n  barplot(table(iris$Species),\n          col=c(color1,color2,color3),\n          border=color4,\n          main=\"Count by Species\",\n          xlab=\"Species\", ylab=\"Count\")\n}\n\nBar2 &lt;- function(){\n  barplot(table(iris$Species),\n          horiz=TRUE,\n          col=c(color1,color2,color3),\n          border=color4,\n          main=\"Count by Species (Horizontal)\",\n          xlab=\"Count\", ylab=\"Species\")\n}\n\nPie &lt;- function(){\n  pie(table(iris$Species),\n      col=c(color1,color2,color3),\n      main=\"Species Composition\",\n      clockwise=TRUE)\n}\n\nBox &lt;- function(){\n  boxplot(Sepal.Length~Species, data=iris,\n          col=c(color1,color2,color3),\n          main=\"Sepal Length by Species\",\n          xlab=\"Species\", ylab=\"Sepal Length (cm)\")\n}\n\nScat &lt;- function(){\n  plot(iris$Petal.Length, iris$Sepal.Length,\n       main=\"Sepal vs Petal Length\",\n       xlab=\"Petal Length (cm)\", ylab=\"Sepal Length (cm)\",\n       pch=19, col=color1)\n}\n\n\nlibrary(gridExtra)\n\npar(mfrow=c(2,3), mar=c(4,4,2.5,1), family=\"sans\")\nHisto(); Bar1(); Bar2(); Pie(); Box(); Scat()\n\n\n\n\n\n\n\n\n\ndraw6 &lt;- function(){\n  par(mfrow=c(2,3), mar=c(4,4,2.5,1), family=base_family)\n  Histo(); Bar1(); Bar2(); Pie(); Box(); Scat()\n}\n\nsave_plot &lt;- function(fmt, file){\n  switch(fmt,\n    pdf  = pdf(file, width=10, height=7, family=base_family),\n    jpg  = jpeg(file, width=2400, height=1600, res=300, quality=95),\n    svg  = svg(file, width=2400, height=1600),\n    tiff = tiff(file, width=2400, height=1600, res=300),\n    bmp  = bmp(file, width=2400, height=1600, res=300), # cannot find bmg, and was told it might be .bmp by GPT\n  )\n  draw6(); invisible(dev.off())\n}\n\n\nsave_plot(\"pdf\",  \"/Users/buttegg/Documents/IAmTryingToUseQuartoDoSthCool/buttegggggggg.github.io/baseR_6plots.pdf\")\nsave_plot(\"jpg\",  \"/Users/buttegg/Documents/IAmTryingToUseQuartoDoSthCool/buttegggggggg.github.io/baseR_6plots.jpg\")\nsave_plot(\"svg\",  \"/Users/buttegg/Documents/IAmTryingToUseQuartoDoSthCool/buttegggggggg.github.io/baseR_6plots.svg\")\nsave_plot(\"tiff\", \"/Users/buttegg/Documents/IAmTryingToUseQuartoDoSthCool/buttegggggggg.github.io/baseR_6plots.tiff\")\nsave_plot(\"bmp\",  \"/Users/buttegg/Documents/IAmTryingToUseQuartoDoSthCool/buttegggggggg.github.io/baseR_6plots.bmp\")\n\nPDF of base R plots\nJPG of base R plots\nSVG of base R plots\nTIFF of base R plots\nBMP of base R plots\n\nggHisto &lt;- ggplot(iris, aes(x=Sepal.Length)) +\n  geom_histogram(fill=color1, color=color3, bins=20) +\n  labs(title=\"Distribution of Sepal Length (iris)\") +\n  theme1()\n\nggBar1 &lt;- ggplot(iris, aes(x=Species, fill=Species)) +\n  geom_bar(color=color4) +\n  scale_fill_manual(values=c(color1,color2,color3)) +\n  labs(title=\"Count by Species\") +\n  theme1() + ggplot2::theme(legend.position=\"none\")\n\nggBar2 &lt;- ggplot(iris, aes(x=Species, fill=Species)) +\n  geom_bar(color=color4) + coord_flip() +\n  scale_fill_manual(values=c(color1,color2,color3)) +\n  labs(title=\"Count by Species (Horizontal)\") +\n  theme1() + ggplot2::theme(legend.position=\"none\")\n\ndf &lt;- as.data.frame(prop.table(table(iris$Species)))\ncolnames(df) &lt;- c(\"Species\",\"prop\")\nggPie &lt;- ggplot(df, aes(x=\"\", y=prop, fill=Species)) +\n  geom_col(width=1, color=NA) + coord_polar(theta=\"y\") +\n  scale_fill_manual(values=c(color1,color2,color3)) +\n  labs(title=\"Species Composition\") +\n  theme1() + ggplot2::theme(axis.text=ggplot2::element_blank(),\n                            axis.title=ggplot2::element_blank(),\n                            panel.grid=ggplot2::element_blank(),\n                            legend.position=\"right\")\n\nggBox &lt;- ggplot(iris, aes(x=Species, y=Sepal.Length, fill=Species)) +\n  geom_boxplot(color=color4) +\n  scale_fill_manual(values=c(color1,color2,color3)) +\n  labs(title=\"Sepal Length by Species\") +\n  theme1() + ggplot2::theme(legend.position=\"none\")\n\nggScat &lt;- ggplot(iris, aes(x=Petal.Length, y=Sepal.Length)) +\n  geom_point(color=color1, size=2) +\n  labs(title=\"Sepal vs Petal Length\") +\n  theme1()\n\n\ngridExtra::grid.arrange(ggHisto, ggBar1, ggBar2, ggPie, ggBox, ggScat, ncol = 3)\n\n\n\n\n\n\n\n\n\ncombo &lt;- gridExtra::arrangeGrob(ggHisto, ggBar1, ggBar2, ggPie, ggBox, ggScat, ncol = 3)\noutdir &lt;- \"/Users/buttegg/Documents/IAmTryingToUseQuartoDoSthCool/buttegggggggg.github.io\"\nformats &lt;- c(\"pdf\", \"jpg\", \"svg\", \"tiff\", \"bmp\")\n\nfor (fmt in formats) {\n  outpath &lt;- file.path(outdir, paste0(\"ggplot_6plots.\", fmt))\n  ggplot2::ggsave(\n    filename = outpath,\n    plot = combo,\n    width = 10, height = 7, dpi = 300\n  )\n}\n\nPDF of ggplot2 plots\nJPG of ggplot2 plots\nSVG of ggplot2 plots\nTIFF of ggplot2 plots\nBMP of ggplot2 plots"
  },
  {
    "objectID": "EPPS6356.html#reviews",
    "href": "EPPS6356.html#reviews",
    "title": "Data Visualization",
    "section": "Reviews",
    "text": "Reviews\n\nReview: Inge Druckrey – Teaching to See\nReview: Journalism in the Age of Data\nReview: The Future of Data Analysis\nReview: Data Visualization and Data Science\nReview: The Week in Charts & 2024 The Year in Charts"
  },
  {
    "objectID": "EPPS6356.html#notes",
    "href": "EPPS6356.html#notes",
    "title": "Data Visualization",
    "section": "Notes",
    "text": "Notes\n\nNotes: Big Data Pitfalls"
  },
  {
    "objectID": "EPPS6356.html#checklist",
    "href": "EPPS6356.html#checklist",
    "title": "Data Visualization",
    "section": "Checklist",
    "text": "Checklist\n\nTzu-Yuan’s Data Guide Checklist"
  },
  {
    "objectID": "DL.html",
    "href": "DL.html",
    "title": "Deep Learning",
    "section": "",
    "text": "Task / 任務： Classify industrial component images into 6 defect categories using a fine-tuned ResNet-18, trained on the AOI (Automated Optical Inspection) dataset. 使用 ResNet-18 對工業元件影像進行 6 類缺陷分類（AOI 自動光學檢測資料集）。\nDataset / 資料集： AOI Dataset — 2,530 training images, 10,144 test images, 6 classes (normal, void, horizontal defect, vertical defect, edge defect, particle)\nMethod / 方法： ResNet-18 (ImageNet pretrained) — frozen backbone, fine-tuned classifier head\n\n\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35}}}%%\nflowchart LR\n    A[\"Pretrained ResNet-18     \"] --&gt; B[\"Freeze Backbone     \"] --&gt; C[\"Replace fc → 6 cls     \"] --&gt; D[\"Train Classifier     \"] --&gt; E[\"Defect Prediction     \"]\n\n    style A fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style B fill:#F5F5F5,color:#424242,stroke:#BDBDBD,stroke-width:2px\n    style C fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style D fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n    style E fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n\n\n\n\n\n\n\n遷移學習策略： 凍結 ResNet-18 全部預訓練參數，僅替換最後一層 fc → 6 輸出，以 Adam optimizer (lr=0.001) 訓練分類頭。資料前處理：224x224 RGB，ImageNet mean/std 正規化。\n\n\n\n\nmodel = models.resnet18(pretrained=True)\n\n# Freeze all parameters\nfor param in model.parameters():\n    param.requires_grad = False\n\n# Replace final layer for 6-class output\nnum_ftrs = model.fc.in_features\nmodel.fc = nn.Linear(num_ftrs, 6)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)\n\n\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.fc.parameters(), lr=0.001)\nnum_epochs = 10\nbatch_size = 32\n# Input: 224x224 RGB, normalized to ImageNet mean/std\n\n\n\n\n\n\nEpoch\nTrain Loss\nVal Accuracy\n\n\n\n\n1\n0.8943\n95.26%\n\n\n2\n0.4654\n96.25%\n\n\n6\n0.2635\n96.44%\n\n\n10\n0.2381\n95.85%\n\n\n\nBest Validation Accuracy: 96.44%\n\n\n\n\n\n\n\n\n\n\nTraining Curve 分析： 粉色 Train Loss 在前 3 個 epoch 急遽下降，之後趨於平穩。青色 Val Accuracy 在 Epoch 6 達到最高 96.44%（橘色菱形），之後出現輕微 overfitting（accuracy 微幅下降）。橘色虛線 標示 best epoch 位置。\n\n\n\n\n\n\nTask / 任務： Perform binary semantic segmentation of blood vessels in retinal fundus images using a custom U-Net architecture trained on the DRIVE dataset. 使用自製 U-Net 對 DRIVE 資料集的眼底影像進行視網膜血管二元語意分割。\nDataset / 資料集： DRIVE (Digital Retinal Images for Vessel Extraction) — 22 training, 20 test images, 512x512\nMethod / 方法： U-Net (5-level encoder-decoder with skip connections) + Focal Tversky Loss\n\n\n\n\n\n\n\n\nSample 1 — fundus image\n\n\n\n\n\n\n\nSample 2 — different vessel pattern\n\n\n\n\n\n\n\nSample 3 — optic disc visible\n\n\n\n\n\n\n\n\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35}}}%%\nflowchart TD\n    IN[\"Input 1ch 512x512     \"] --&gt; E1[\"Conv1: 1→64     \"]\n    E1 --&gt;|Pool 2x| E2[\"Conv2: 64→128     \"]\n    E2 --&gt;|Pool 2x| E3[\"Conv3: 128→256     \"]\n    E3 --&gt;|Pool 2x| E4[\"Conv4: 256→512     \"]\n    E4 --&gt;|Pool 2x| BN[\"Bottleneck: 512→1024     \"]\n    BN --&gt; U1[\"Up: 1024→512     \"]\n    U1 --&gt;|+skip E4| U2[\"Up: 512→256     \"]\n    U2 --&gt;|+skip E3| U3[\"Up: 256→128     \"]\n    U3 --&gt;|+skip E2| U4[\"Up: 128→64     \"]\n    U4 --&gt;|+skip E1| OUT[\"Mask Output     \"]\n\n    E4 -.-&gt;|\"skip\"| U1\n    E3 -.-&gt;|\"skip\"| U2\n    E2 -.-&gt;|\"skip\"| U3\n    E1 -.-&gt;|\"skip\"| U4\n\n    style IN  fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style E1  fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style E2  fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style E3  fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style E4  fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style BN  fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style U1  fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n    style U2  fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n    style U3  fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n    style U4  fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n    style OUT fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n\n\n\n\n\n\n\nU-Net 架構色碼： 淺藍色 = Encoder（收縮路徑），淺灰色 = MaxPool 下採樣，淺橘色 = Bottleneck（最底層 512→1024），淺綠色 = Decoder（擴展路徑）+ skip connection。每層 skip connection 把 encoder 的空間細節傳遞給 decoder，保留高解析度的血管邊緣資訊。\n\nclass UNet(torch.nn.Module):\n    def __init__(self, inchannel, outchannel):\n        super(UNet, self).__init__()\n        # Encoder\n        self.conv1 = Conv(inchannel, 64)\n        self.conv2 = Conv(64, 128)\n        self.conv3 = Conv(128, 256)\n        self.conv4 = Conv(256, 512)\n        self.conv5 = Conv(512, 1024)\n        self.pool  = torch.nn.MaxPool2d(2)\n        # Decoder\n        self.up1   = torch.nn.ConvTranspose2d(1024, 512, 2, 2)\n        self.conv6 = Conv(1024, 512)\n        self.up2   = torch.nn.ConvTranspose2d(512, 256, 2, 2)\n        self.conv7 = Conv(512, 256)\n        self.up3   = torch.nn.ConvTranspose2d(256, 128, 2, 2)\n        self.conv8 = Conv(256, 128)\n        self.up4   = torch.nn.ConvTranspose2d(128, 64, 2, 2)\n        self.conv9 = Conv(128, 64)\n        self.conv10 = torch.nn.Conv2d(64, outchannel, 3, 1, 1)\n\n\n\n# Focal Tversky Loss — handles class imbalance in vessel vs background\ncriterion = lambda y_pred, y_true: focal_tversky_loss(\n    y_pred, y_true, alpha=0.5, beta=0.5, gamma=0.75\n)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, mode='min', factor=0.5, patience=5\n)\ndevice = torch.device(\"mps\")  # Apple Silicon\nnum_epochs = 100\n\n\n\nEach row shows: Original fundus image → Predicted segmentation mask → Ground truth mask 每列依序為：原始眼底影像 → 預測分割遮罩 → 真實標記遮罩\n\n\n\nSegmentation output — Original / Segmentation / Ground Truth (all 20 test images)\n\n\n\n分割結果觀察： 模型成功識別主要血管走向與分佈，但在微血管（fine capillaries）的辨識上仍有提升空間。Ground truth 中可見許多極細的毛細血管，模型傾向於只預測較粗的血管結構。\n\n\n\n\n\n\n\nMetric\nValue\n\n\n\n\nMean IoU (mIoU)\n0.3510\n\n\nTraining epochs\n100\n\n\nInput resolution\n512 x 512\n\n\n\n\n\n\n\n\n\n\n\n\n\nmIoU 分析： 左圖比較三者：隨機 baseline（0.05）、本模型 U-Net（0.351）、DRIVE 資料集 SOTA（~0.82）。右圖以 Venn 散點示意 IoU 概念 — 青色 為 Ground Truth、粉色 為 Prediction，重疊區域即 Intersection。mIoU 0.351 表示預測與標記的重疊程度約 35%，仍有提升空間（可嘗試更深網路、更多 data augmentation、class-weighted loss）。\n\n\n\n\n\n\nTask / 任務： Train a convolutional autoencoder to reconstruct retinal fundus images in an unsupervised manner, evaluated by Peak Signal-to-Noise Ratio (PSNR). 以無監督方式訓練卷積自動編碼器重建眼底影像，以 PSNR 作為評估指標。\nDataset / 資料集： DRIVE — 21 training, 20 test images, 512x512 RGB\nMethod / 方法： Convolutional Autoencoder (Encoder-Decoder with skip connections) + MSE Loss\n\n\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35}}}%%\nflowchart TD\n    IN[\"Input 3ch 512x512     \"] --&gt; C1[\"Conv1: 3→64     \"]\n    C1 --&gt;|Pool 2x| C2[\"Conv2: 64→128     \"]\n    C2 --&gt;|Pool 2x| C3[\"Conv3: 128→256     \"]\n    C3 --&gt;|Pool 2x| C4[\"Bottleneck: 256→512     \"]\n    C4 --&gt; U1[\"Up: 512→256     \"]\n    U1 --&gt;|+skip C3| U2[\"Up: 256→128     \"]\n    U2 --&gt;|+skip C2| U3[\"Up: 128→64     \"]\n    U3 --&gt;|+skip C1| OUT[\"Output: 64→3ch     \"]\n\n    C3 -.-&gt;|\"skip\"| U1\n    C2 -.-&gt;|\"skip\"| U2\n    C1 -.-&gt;|\"skip\"| U3\n\n    style IN  fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style C1  fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style C2  fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style C3  fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style C4  fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style U1  fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n    style U2  fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n    style U3  fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n    style OUT fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n\n\n\n\n\n\n\nAutoencoder 架構色碼： 淺藍色 = Encoder，淺橘色 = Bottleneck（256→512 壓縮表示），淺綠色 = Decoder + skip connections。與 U-Net 相同的 encoder-decoder 結構，但目標是重建輸入影像（自監督學習），而非分割。\n\nclass AutoEncoder(nn.Module):\n    def __init__(self, inchannel=3, outchannel=3):\n        super(AutoEncoder, self).__init__()\n        # Encoder\n        self.conv1 = Conv(inchannel, 64)\n        self.conv2 = Conv(64, 128)\n        self.conv3 = Conv(128, 256)\n        self.conv4 = Conv(256, 512)\n        self.pool  = nn.MaxPool2d(2)\n        # Decoder (with skip connections)\n        self.up1   = nn.ConvTranspose2d(512, 256, 2, 2)\n        self.conv5 = Conv(512, 256)\n        self.up2   = nn.ConvTranspose2d(256, 128, 2, 2)\n        self.conv6 = Conv(256, 128)\n        self.up3   = nn.ConvTranspose2d(128, 64, 2, 2)\n        self.conv7 = Conv(128, 64)\n        self.conv8 = nn.Conv2d(64, outchannel, 3, 1, 1)\n\n\n\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\nnum_epochs = 20\nbatch_size = 1\n# Normalization: mean=0.5, std=0.5\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n\n\n\n\n\n\nEpoch\nTrain Loss\nTest Loss\nPSNR (dB)\n\n\n\n\n1\n0.0979\n0.1152\n16.29\n\n\n3\n0.0323\n0.0067\n27.98\n\n\n10\n0.0309\n0.0055\n29.06\n\n\n13\n0.0263\n0.0043\n30.16\n\n\n18\n0.0280\n0.0037\n30.84\n\n\n20\n0.0268\n0.0048\n29.50\n\n\n\nBest PSNR: 30.84 dB at Epoch 18\n\n\nWarning in annotate(\"label\", x = best_ep - 1, y = 32, label = paste0(\"Best: \",\n: Ignoring unknown parameters: `label.size`\n\n\n\n\n\n\n\n\n\n\nTraining Curve 分析： 左圖 — 粉色 Train Loss 與 青色虛線 Test Loss 都在前 3 epoch 急速下降，之後趨於平穩。右圖 — PSNR 在 Epoch 18 達到峰值 30.84 dB（超過 30 dB 門檻，灰色虛線），之後微幅下降（Epoch 20 為 29.50 dB），顯示 Epoch 18 為最佳停止點。\n\n\n\n\n\n\nTask / 任務： Train a conditional GAN to generate Western blot images from two template images, learning the mapping from template patterns to realistic blot patterns. 訓練條件式 GAN，從兩張模板影像生成 Western blot 影像，學習模板圖案到真實條帶紋路的映射。\nDataset / 資料集： Western Blot Dataset — 402 template pairs + 402 target images, 64x64 grayscale\nMethod / 方法： Conditional GAN — Encoder-Decoder Generator + PatchGAN-style Discriminator\n\n\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35}}}%%\nflowchart TD\n    T1[\"Template 1: 64x64     \"] --&gt; CAT[\"Concat 2ch     \"]\n    T2[\"Template 2: 64x64     \"] --&gt; CAT\n    CAT --&gt; G[\"Generator     \"]\n    G --&gt; FAKE[\"Generated Image     \"]\n    REAL[\"Real Image     \"] --&gt; D[\"Discriminator     \"]\n    FAKE --&gt; D\n    D --&gt;|G loss| UG[\"Update G     \"]\n    D --&gt;|D loss| UD[\"Update D     \"]\n\n    style T1   fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style T2   fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style CAT  fill:#F5F5F5,color:#424242,stroke:#BDBDBD,stroke-width:2px\n    style G    fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style FAKE fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style REAL fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n    style D    fill:#FCE4EC,color:#C62828,stroke:#F48FB1,stroke-width:2px\n    style UG   fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style UD   fill:#FCE4EC,color:#C62828,stroke:#F48FB1,stroke-width:2px\n\n\n\n\n\n\n\nGAN 訓練流程： 兩張 template 圖 concat 成 2-channel 輸入，經過 藍色 Generator 生成假 blot 影像（橘色）。粉紅色 Discriminator 判斷輸入是 真（綠色） 還是 假（橘色），並分別回傳 G loss / D loss 更新各自的參數。\n\n\n\n\nclass TemplateToImageGenerator(nn.Module):\n    def __init__(self):\n        super(TemplateToImageGenerator, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Conv2d(1, 64, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(64), nn.ReLU(),\n            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(128), nn.ReLU(),\n            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(256), nn.ReLU(),\n        )\n        self.decoder = nn.Sequential(\n            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(128), nn.ReLU(),\n            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(64), nn.ReLU(),\n            nn.ConvTranspose2d(64, 1, kernel_size=4, stride=2, padding=1),\n            nn.Tanh(),\n        )\n\n\n\nclass TemplateToImageDiscriminator(nn.Module):\n    def __init__(self):\n        super(TemplateToImageDiscriminator, self).__init__()\n        self.model = nn.Sequential(\n            nn.Conv2d(2, 64, kernel_size=4, stride=2, padding=1),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(128), nn.LeakyReLU(0.2),\n            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(256), nn.LeakyReLU(0.2),\n            nn.AdaptiveAvgPool2d((1, 1)),\n            nn.Flatten(),\n            nn.Linear(256, 1),\n            nn.Sigmoid()\n        )\n\n\n\ng_optimizer = optim.Adam(generator.parameters(),     lr=0.0002)\nd_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002)\ncriterion   = nn.BCELoss()\nnum_epochs  = 200    # trained on CPU, stopped at epoch 118\nbatch_size  = 1\n\n\n\nTraining ran on CPU and was recorded up to epoch 118/200. By that point the Discriminator had begun to dominate (D Loss &lt; 0.1 in some steps), causing G Loss to climb — a classic sign the generator needs more capacity or learning rate balancing. 訓練在 CPU 上進行，記錄至第 118 個 epoch。此時判別器開始主導訓練（D Loss 低至 0.03），導致 G Loss 攀升，為典型的判別器過強問題。\n\n\n\nEpoch\nD Loss (sample)\nG Loss (sample)\n\n\n\n\n1 / step 10\n1.3715\n0.7412\n\n\n1 / step 40\n1.3699\n0.6840\n\n\n118 / step 200\n0.4921\n2.2424\n\n\n118 / step 230\n0.0263\n4.2039\n\n\n118 / step 270\n0.0683\n3.5220\n\n\n\n\n\n\n\n\n\n\n\n\n\nGAN 訓練動態分析： 灰色虛線 ln(2)=0.693 為 GAN 理想均衡點（D 分不出真假時的 BCE loss）。左圖原始 log 可見早期 D Loss 接近 ln(2)（D/G 接近均衡），後期 粉色 D Loss 快速下降至接近 0，青色 G Loss 攀升至 3-4，表示 Discriminator 過強（D 能輕鬆分辨真假）。右圖 per-epoch 平均趨勢更清楚呈現此分歧。可考慮降低 D 的學習率、增加 G 的容量、或加入 label smoothing 來緩解。"
  },
  {
    "objectID": "DL.html#assignments-作業",
    "href": "DL.html#assignments-作業",
    "title": "Deep Learning",
    "section": "",
    "text": "Task / 任務： Classify industrial component images into 6 defect categories using a fine-tuned ResNet-18, trained on the AOI (Automated Optical Inspection) dataset. 使用 ResNet-18 對工業元件影像進行 6 類缺陷分類（AOI 自動光學檢測資料集）。\nDataset / 資料集： AOI Dataset — 2,530 training images, 10,144 test images, 6 classes (normal, void, horizontal defect, vertical defect, edge defect, particle)\nMethod / 方法： ResNet-18 (ImageNet pretrained) — frozen backbone, fine-tuned classifier head\n\n\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35}}}%%\nflowchart LR\n    A[\"Pretrained ResNet-18     \"] --&gt; B[\"Freeze Backbone     \"] --&gt; C[\"Replace fc → 6 cls     \"] --&gt; D[\"Train Classifier     \"] --&gt; E[\"Defect Prediction     \"]\n\n    style A fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style B fill:#F5F5F5,color:#424242,stroke:#BDBDBD,stroke-width:2px\n    style C fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style D fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n    style E fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n\n\n\n\n\n\n\n遷移學習策略： 凍結 ResNet-18 全部預訓練參數，僅替換最後一層 fc → 6 輸出，以 Adam optimizer (lr=0.001) 訓練分類頭。資料前處理：224x224 RGB，ImageNet mean/std 正規化。\n\n\n\n\nmodel = models.resnet18(pretrained=True)\n\n# Freeze all parameters\nfor param in model.parameters():\n    param.requires_grad = False\n\n# Replace final layer for 6-class output\nnum_ftrs = model.fc.in_features\nmodel.fc = nn.Linear(num_ftrs, 6)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)\n\n\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.fc.parameters(), lr=0.001)\nnum_epochs = 10\nbatch_size = 32\n# Input: 224x224 RGB, normalized to ImageNet mean/std\n\n\n\n\n\n\nEpoch\nTrain Loss\nVal Accuracy\n\n\n\n\n1\n0.8943\n95.26%\n\n\n2\n0.4654\n96.25%\n\n\n6\n0.2635\n96.44%\n\n\n10\n0.2381\n95.85%\n\n\n\nBest Validation Accuracy: 96.44%\n\n\n\n\n\n\n\n\n\n\nTraining Curve 分析： 粉色 Train Loss 在前 3 個 epoch 急遽下降，之後趨於平穩。青色 Val Accuracy 在 Epoch 6 達到最高 96.44%（橘色菱形），之後出現輕微 overfitting（accuracy 微幅下降）。橘色虛線 標示 best epoch 位置。\n\n\n\n\n\n\nTask / 任務： Perform binary semantic segmentation of blood vessels in retinal fundus images using a custom U-Net architecture trained on the DRIVE dataset. 使用自製 U-Net 對 DRIVE 資料集的眼底影像進行視網膜血管二元語意分割。\nDataset / 資料集： DRIVE (Digital Retinal Images for Vessel Extraction) — 22 training, 20 test images, 512x512\nMethod / 方法： U-Net (5-level encoder-decoder with skip connections) + Focal Tversky Loss\n\n\n\n\n\n\n\n\nSample 1 — fundus image\n\n\n\n\n\n\n\nSample 2 — different vessel pattern\n\n\n\n\n\n\n\nSample 3 — optic disc visible\n\n\n\n\n\n\n\n\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35}}}%%\nflowchart TD\n    IN[\"Input 1ch 512x512     \"] --&gt; E1[\"Conv1: 1→64     \"]\n    E1 --&gt;|Pool 2x| E2[\"Conv2: 64→128     \"]\n    E2 --&gt;|Pool 2x| E3[\"Conv3: 128→256     \"]\n    E3 --&gt;|Pool 2x| E4[\"Conv4: 256→512     \"]\n    E4 --&gt;|Pool 2x| BN[\"Bottleneck: 512→1024     \"]\n    BN --&gt; U1[\"Up: 1024→512     \"]\n    U1 --&gt;|+skip E4| U2[\"Up: 512→256     \"]\n    U2 --&gt;|+skip E3| U3[\"Up: 256→128     \"]\n    U3 --&gt;|+skip E2| U4[\"Up: 128→64     \"]\n    U4 --&gt;|+skip E1| OUT[\"Mask Output     \"]\n\n    E4 -.-&gt;|\"skip\"| U1\n    E3 -.-&gt;|\"skip\"| U2\n    E2 -.-&gt;|\"skip\"| U3\n    E1 -.-&gt;|\"skip\"| U4\n\n    style IN  fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style E1  fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style E2  fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style E3  fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style E4  fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style BN  fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style U1  fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n    style U2  fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n    style U3  fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n    style U4  fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n    style OUT fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n\n\n\n\n\n\n\nU-Net 架構色碼： 淺藍色 = Encoder（收縮路徑），淺灰色 = MaxPool 下採樣，淺橘色 = Bottleneck（最底層 512→1024），淺綠色 = Decoder（擴展路徑）+ skip connection。每層 skip connection 把 encoder 的空間細節傳遞給 decoder，保留高解析度的血管邊緣資訊。\n\nclass UNet(torch.nn.Module):\n    def __init__(self, inchannel, outchannel):\n        super(UNet, self).__init__()\n        # Encoder\n        self.conv1 = Conv(inchannel, 64)\n        self.conv2 = Conv(64, 128)\n        self.conv3 = Conv(128, 256)\n        self.conv4 = Conv(256, 512)\n        self.conv5 = Conv(512, 1024)\n        self.pool  = torch.nn.MaxPool2d(2)\n        # Decoder\n        self.up1   = torch.nn.ConvTranspose2d(1024, 512, 2, 2)\n        self.conv6 = Conv(1024, 512)\n        self.up2   = torch.nn.ConvTranspose2d(512, 256, 2, 2)\n        self.conv7 = Conv(512, 256)\n        self.up3   = torch.nn.ConvTranspose2d(256, 128, 2, 2)\n        self.conv8 = Conv(256, 128)\n        self.up4   = torch.nn.ConvTranspose2d(128, 64, 2, 2)\n        self.conv9 = Conv(128, 64)\n        self.conv10 = torch.nn.Conv2d(64, outchannel, 3, 1, 1)\n\n\n\n# Focal Tversky Loss — handles class imbalance in vessel vs background\ncriterion = lambda y_pred, y_true: focal_tversky_loss(\n    y_pred, y_true, alpha=0.5, beta=0.5, gamma=0.75\n)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, mode='min', factor=0.5, patience=5\n)\ndevice = torch.device(\"mps\")  # Apple Silicon\nnum_epochs = 100\n\n\n\nEach row shows: Original fundus image → Predicted segmentation mask → Ground truth mask 每列依序為：原始眼底影像 → 預測分割遮罩 → 真實標記遮罩\n\n\n\nSegmentation output — Original / Segmentation / Ground Truth (all 20 test images)\n\n\n\n分割結果觀察： 模型成功識別主要血管走向與分佈，但在微血管（fine capillaries）的辨識上仍有提升空間。Ground truth 中可見許多極細的毛細血管，模型傾向於只預測較粗的血管結構。\n\n\n\n\n\n\n\nMetric\nValue\n\n\n\n\nMean IoU (mIoU)\n0.3510\n\n\nTraining epochs\n100\n\n\nInput resolution\n512 x 512\n\n\n\n\n\n\n\n\n\n\n\n\n\nmIoU 分析： 左圖比較三者：隨機 baseline（0.05）、本模型 U-Net（0.351）、DRIVE 資料集 SOTA（~0.82）。右圖以 Venn 散點示意 IoU 概念 — 青色 為 Ground Truth、粉色 為 Prediction，重疊區域即 Intersection。mIoU 0.351 表示預測與標記的重疊程度約 35%，仍有提升空間（可嘗試更深網路、更多 data augmentation、class-weighted loss）。\n\n\n\n\n\n\nTask / 任務： Train a convolutional autoencoder to reconstruct retinal fundus images in an unsupervised manner, evaluated by Peak Signal-to-Noise Ratio (PSNR). 以無監督方式訓練卷積自動編碼器重建眼底影像，以 PSNR 作為評估指標。\nDataset / 資料集： DRIVE — 21 training, 20 test images, 512x512 RGB\nMethod / 方法： Convolutional Autoencoder (Encoder-Decoder with skip connections) + MSE Loss\n\n\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35}}}%%\nflowchart TD\n    IN[\"Input 3ch 512x512     \"] --&gt; C1[\"Conv1: 3→64     \"]\n    C1 --&gt;|Pool 2x| C2[\"Conv2: 64→128     \"]\n    C2 --&gt;|Pool 2x| C3[\"Conv3: 128→256     \"]\n    C3 --&gt;|Pool 2x| C4[\"Bottleneck: 256→512     \"]\n    C4 --&gt; U1[\"Up: 512→256     \"]\n    U1 --&gt;|+skip C3| U2[\"Up: 256→128     \"]\n    U2 --&gt;|+skip C2| U3[\"Up: 128→64     \"]\n    U3 --&gt;|+skip C1| OUT[\"Output: 64→3ch     \"]\n\n    C3 -.-&gt;|\"skip\"| U1\n    C2 -.-&gt;|\"skip\"| U2\n    C1 -.-&gt;|\"skip\"| U3\n\n    style IN  fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style C1  fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style C2  fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style C3  fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style C4  fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style U1  fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n    style U2  fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n    style U3  fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n    style OUT fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n\n\n\n\n\n\n\nAutoencoder 架構色碼： 淺藍色 = Encoder，淺橘色 = Bottleneck（256→512 壓縮表示），淺綠色 = Decoder + skip connections。與 U-Net 相同的 encoder-decoder 結構，但目標是重建輸入影像（自監督學習），而非分割。\n\nclass AutoEncoder(nn.Module):\n    def __init__(self, inchannel=3, outchannel=3):\n        super(AutoEncoder, self).__init__()\n        # Encoder\n        self.conv1 = Conv(inchannel, 64)\n        self.conv2 = Conv(64, 128)\n        self.conv3 = Conv(128, 256)\n        self.conv4 = Conv(256, 512)\n        self.pool  = nn.MaxPool2d(2)\n        # Decoder (with skip connections)\n        self.up1   = nn.ConvTranspose2d(512, 256, 2, 2)\n        self.conv5 = Conv(512, 256)\n        self.up2   = nn.ConvTranspose2d(256, 128, 2, 2)\n        self.conv6 = Conv(256, 128)\n        self.up3   = nn.ConvTranspose2d(128, 64, 2, 2)\n        self.conv7 = Conv(128, 64)\n        self.conv8 = nn.Conv2d(64, outchannel, 3, 1, 1)\n\n\n\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\nnum_epochs = 20\nbatch_size = 1\n# Normalization: mean=0.5, std=0.5\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n\n\n\n\n\n\nEpoch\nTrain Loss\nTest Loss\nPSNR (dB)\n\n\n\n\n1\n0.0979\n0.1152\n16.29\n\n\n3\n0.0323\n0.0067\n27.98\n\n\n10\n0.0309\n0.0055\n29.06\n\n\n13\n0.0263\n0.0043\n30.16\n\n\n18\n0.0280\n0.0037\n30.84\n\n\n20\n0.0268\n0.0048\n29.50\n\n\n\nBest PSNR: 30.84 dB at Epoch 18\n\n\nWarning in annotate(\"label\", x = best_ep - 1, y = 32, label = paste0(\"Best: \",\n: Ignoring unknown parameters: `label.size`\n\n\n\n\n\n\n\n\n\n\nTraining Curve 分析： 左圖 — 粉色 Train Loss 與 青色虛線 Test Loss 都在前 3 epoch 急速下降，之後趨於平穩。右圖 — PSNR 在 Epoch 18 達到峰值 30.84 dB（超過 30 dB 門檻，灰色虛線），之後微幅下降（Epoch 20 為 29.50 dB），顯示 Epoch 18 為最佳停止點。\n\n\n\n\n\n\nTask / 任務： Train a conditional GAN to generate Western blot images from two template images, learning the mapping from template patterns to realistic blot patterns. 訓練條件式 GAN，從兩張模板影像生成 Western blot 影像，學習模板圖案到真實條帶紋路的映射。\nDataset / 資料集： Western Blot Dataset — 402 template pairs + 402 target images, 64x64 grayscale\nMethod / 方法： Conditional GAN — Encoder-Decoder Generator + PatchGAN-style Discriminator\n\n\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35}}}%%\nflowchart TD\n    T1[\"Template 1: 64x64     \"] --&gt; CAT[\"Concat 2ch     \"]\n    T2[\"Template 2: 64x64     \"] --&gt; CAT\n    CAT --&gt; G[\"Generator     \"]\n    G --&gt; FAKE[\"Generated Image     \"]\n    REAL[\"Real Image     \"] --&gt; D[\"Discriminator     \"]\n    FAKE --&gt; D\n    D --&gt;|G loss| UG[\"Update G     \"]\n    D --&gt;|D loss| UD[\"Update D     \"]\n\n    style T1   fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style T2   fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style CAT  fill:#F5F5F5,color:#424242,stroke:#BDBDBD,stroke-width:2px\n    style G    fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style FAKE fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style REAL fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n    style D    fill:#FCE4EC,color:#C62828,stroke:#F48FB1,stroke-width:2px\n    style UG   fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style UD   fill:#FCE4EC,color:#C62828,stroke:#F48FB1,stroke-width:2px\n\n\n\n\n\n\n\nGAN 訓練流程： 兩張 template 圖 concat 成 2-channel 輸入，經過 藍色 Generator 生成假 blot 影像（橘色）。粉紅色 Discriminator 判斷輸入是 真（綠色） 還是 假（橘色），並分別回傳 G loss / D loss 更新各自的參數。\n\n\n\n\nclass TemplateToImageGenerator(nn.Module):\n    def __init__(self):\n        super(TemplateToImageGenerator, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Conv2d(1, 64, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(64), nn.ReLU(),\n            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(128), nn.ReLU(),\n            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(256), nn.ReLU(),\n        )\n        self.decoder = nn.Sequential(\n            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(128), nn.ReLU(),\n            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(64), nn.ReLU(),\n            nn.ConvTranspose2d(64, 1, kernel_size=4, stride=2, padding=1),\n            nn.Tanh(),\n        )\n\n\n\nclass TemplateToImageDiscriminator(nn.Module):\n    def __init__(self):\n        super(TemplateToImageDiscriminator, self).__init__()\n        self.model = nn.Sequential(\n            nn.Conv2d(2, 64, kernel_size=4, stride=2, padding=1),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(128), nn.LeakyReLU(0.2),\n            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(256), nn.LeakyReLU(0.2),\n            nn.AdaptiveAvgPool2d((1, 1)),\n            nn.Flatten(),\n            nn.Linear(256, 1),\n            nn.Sigmoid()\n        )\n\n\n\ng_optimizer = optim.Adam(generator.parameters(),     lr=0.0002)\nd_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002)\ncriterion   = nn.BCELoss()\nnum_epochs  = 200    # trained on CPU, stopped at epoch 118\nbatch_size  = 1\n\n\n\nTraining ran on CPU and was recorded up to epoch 118/200. By that point the Discriminator had begun to dominate (D Loss &lt; 0.1 in some steps), causing G Loss to climb — a classic sign the generator needs more capacity or learning rate balancing. 訓練在 CPU 上進行，記錄至第 118 個 epoch。此時判別器開始主導訓練（D Loss 低至 0.03），導致 G Loss 攀升，為典型的判別器過強問題。\n\n\n\nEpoch\nD Loss (sample)\nG Loss (sample)\n\n\n\n\n1 / step 10\n1.3715\n0.7412\n\n\n1 / step 40\n1.3699\n0.6840\n\n\n118 / step 200\n0.4921\n2.2424\n\n\n118 / step 230\n0.0263\n4.2039\n\n\n118 / step 270\n0.0683\n3.5220\n\n\n\n\n\n\n\n\n\n\n\n\n\nGAN 訓練動態分析： 灰色虛線 ln(2)=0.693 為 GAN 理想均衡點（D 分不出真假時的 BCE loss）。左圖原始 log 可見早期 D Loss 接近 ln(2)（D/G 接近均衡），後期 粉色 D Loss 快速下降至接近 0，青色 G Loss 攀升至 3-4，表示 Discriminator 過強（D 能輕鬆分辨真假）。右圖 per-epoch 平均趨勢更清楚呈現此分歧。可考慮降低 D 的學習率、增加 G 的容量、或加入 label smoothing 來緩解。"
  },
  {
    "objectID": "AM.html",
    "href": "AM.html",
    "title": "BS of Applied Mathematics",
    "section": "",
    "text": "學歷： 國立中興大學 應用數學系 學士 (Bachelor of Science in Applied Mathematics, NCHU)。"
  },
  {
    "objectID": "AM.html#about-the-program",
    "href": "AM.html#about-the-program",
    "title": "BS of Applied Mathematics",
    "section": "About the Program",
    "text": "About the Program\nThe Department of Applied Mathematics (應用數學系) at National Chung Hsing University provides a solid foundation in mathematical theory and its applications. The curriculum covers calculus, linear algebra, differential equations, probability and statistics, numerical analysis, and optimization — preparing students for advanced studies and careers in data science, finance, engineering, and scientific computing.\n\n本頁將陸續補充大學期間的課程作業與專題成果。Content coming soon."
  },
  {
    "objectID": "ML.html",
    "href": "ML.html",
    "title": "Machine Learning & Data Science",
    "section": "",
    "text": "Task / 任務： Predict the onset of diabetes using the Pima Indians Diabetes dataset, with a focus on handling missing values through regression-based imputation, feature engineering, and resampling before training a deep neural network. 以 Pima Indians 糖尿病資料集為基礎，先用迴歸填補缺失值、進行特徵工程與資料重抽樣，再訓練深層神經網路進行二元分類。\nDataset / 資料集： Pima Indians Diabetes Dataset — 768 samples, 8 features\nMethod / 方法： Regression imputation → Feature engineering → Resampling → DNN\n\n\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35, \"nodeSpacing\": 25, \"rankSpacing\": 40}}}%%\nflowchart TD\n    A[\"Raw Data 768×8     \"] --&gt; B[\"Mark Zeros as Missing     \"]\n    B --&gt; C[\"Regression Imputation     \"]\n    C --&gt; D[\"Feature Eng. + Resample     \"]\n    D --&gt; E[\"Train / Val Split     \"]\n    E --&gt; F[\"DNN 64→32→16→1     \"]\n    F --&gt; G[\"90.04% acc     \"]\n\n    style A fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style B fill:#F5F5F5,color:#424242,stroke:#BDBDBD,stroke-width:2px\n    style C fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style D fill:#F3E5F5,color:#6A1B9A,stroke:#CE93D8,stroke-width:2px\n    style E fill:#F5F5F5,color:#424242,stroke:#BDBDBD,stroke-width:2px\n    style F fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style G fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n\n\n\n\n\n\n\nPipeline 說明： 原始資料中 Glucose、BMI 等欄位含有「零值」，實際上代表缺失。先以線性迴歸逐欄填補，再做特徵工程與重抽樣平衡正負樣本，最後以 4 層 DNN 進行二元分類，從 baseline 74% 提升至 90.04%。\n\n\n\n\nZero values in Glucose, BMI, BloodPressure, SkinThickness, and Insulin are treated as missing. After removing rows with missing values, 336 clean rows remain for fitting imputation models. 將 Glucose、BMI 等欄位的零值視為缺失。移除含缺失值的列後，336 筆乾淨資料用於訓練填補模型。\n\n\n\nFeature Correlation Matrix (after removing rows with NaN)\n\n\n\nKey observations / 重要觀察： Glucose → Outcome 相關係數 0.50，是糖尿病最強預測變數；SkinThickness ↔︎ BMI 達 0.71，可用 BMI 填補 SkinThickness；Insulin ↔︎ Glucose 達 0.70，可用 Glucose 填補 Insulin；Age ↔︎ Pregnancies 為 0.54，符合生物學預期。\n\n\n\n\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35}}}%%\nflowchart TD\n    A[\"Outcome     \"]\n    B[\"Glucose     \"]\n    C[\"BMI     \"]\n    D[\"Insulin     \"]\n    E[\"SkinThickness     \"]\n    F[\"Age     \"]\n    G[\"BloodPressure     \"]\n\n    A --&gt;|\"predicts\"| B\n    B --&gt;|\"predicts\"| C\n    B --&gt;|\"predicts\"| D\n    C --&gt;|\"predicts\"| E\n    B --&gt;|\"predicts\"| E\n    F --&gt;|\"predicts\"| G\n    C --&gt;|\"predicts\"| G\n\n    style A fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style B fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style C fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n    style D fill:#FCE4EC,color:#C62828,stroke:#F48FB1,stroke-width:2px\n    style E fill:#F3E5F5,color:#6A1B9A,stroke:#CE93D8,stroke-width:2px\n    style F fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style G fill:#E0F2F1,color:#00695C,stroke:#80CBC4,stroke-width:2px\n\n\n\n\n\n\n\n填補順序： 利用相關性最高的已知欄位作為自變數，以線性迴歸依序填補：Outcome → Glucose → BMI → Insulin / SkinThickness → BloodPressure。每一步都只使用「已經存在或已填補」的欄位當作 predictor。\n\n# Fill Glucose using Outcome\nX_train = df_non_missing[['Outcome']]\ny_train = df_non_missing['Glucose']\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Fill BMI using Glucose\nX_train = df_non_missing[['Glucose']]\ny_train = df_non_missing['BMI']\n\n# Fill Insulin using BMI + Glucose\nX_train = df_non_missing[['BMI', 'Glucose']]\ny_train = df_non_missing['Insulin']\n\n# Fill BloodPressure using Age + BMI\nX_train = df_non_missing[['Age', 'BMI']]\ny_train = df_non_missing['BloodPressure']\n\n\n\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35}}}%%\nflowchart LR\n    I[\"Input 8+ feat     \"] --&gt; L1[\"Dense 64 ReLU     \"] --&gt; L2[\"Dense 32 ReLU     \"] --&gt; L3[\"Dense 16 ReLU     \"] --&gt; O[\"Dense 1 Sigmoid     \"]\n\n    style I  fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style L1 fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style L2 fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style L3 fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style O  fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n\n\n\n\n\n\nmodel = Sequential()\nmodel.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(16, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n\n\n\nmodel.compile(\n    loss='binary_crossentropy',\n    optimizer='adam',\n    metrics=['accuracy']\n)\nmodel.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n\n\n\n\n\n\n\n\n\n\nMetric\nValue\n\n\n\n\nBaseline accuracy (no imputation)\n74.03%\n\n\nFinal accuracy (with imputation + feature engineering + resampling)\n90.04%\n\n\nImprovement\n+16.01 pp\n\n\n\n\n\n\n\n\n\n\n\n\n\nAccuracy 分析： Baseline（直接丟棄缺失值）僅有 74.03%。迴歸填補後提升至 77.92%，特徵工程再加 ~5 pp，最終搭配 resampling + DNN 達到 90.04%，共提升 +16 pp。灰色虛線為 baseline 參考線，橘色箭頭標示整體增幅。\n\n\n\n\n\n\nTask / 任務： Analyze 1.88 million US wildfire records to model annual frequency trends using Poisson regression, and predict wildfire causes using a multi-layer perceptron. 分析 188 萬筆美國野火紀錄，用 Poisson 迴歸建立年度頻率趨勢模型，並以 MLP 預測野火成因。\nDataset / 資料集： US Wildfires (1992–2015) — 1,880,465 records, Kaggle\nMethod / 方法： Poisson Regression (trend analysis) + MLP (cause classification)\n\n\nModels the annual count of wildfires as a function of year to estimate long-term trend. 建立野火年度數量對年份的 Poisson 迴歸，估計長期增長趨勢。\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\npoisson_model = smf.glm(\n    formula='Count ~ FIRE_YEAR',\n    data=fire_counts,\n    family=sm.families.Poisson()\n).fit()\n\nprint(poisson_model.summary())\n\n\n\n\n\n\n\n\n\n\nPoisson 趨勢： 以年份為自變數擬合 Poisson 迴歸，估計野火年度發生頻率以每年 +0.44% 的速度增長。橘色虛線 為迴歸擬合線，青色長條 為各年度實際數量。\n\n\n\n\nFeatures: FIRE_SIZE, LATITUDE, LONGITUDE, FIRE_YEAR, MONTH\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35}}}%%\nflowchart LR\n    F[\"5 Features     \"] --&gt; D1[\"Dense 64 ReLU     \"] --&gt; DR1[\"Dropout 0.3     \"] --&gt; D2[\"Dense 64 ReLU     \"] --&gt; DR2[\"Dropout 0.3     \"] --&gt; O[\"Softmax → N cls     \"]\n\n    style F   fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style D1  fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style DR1 fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style D2  fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style DR2 fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style O   fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n\n\n\n\n\n\n\nArchitecture 說明： 兩層 Dense 64 + Dropout 0.3 的簡單 MLP。藍色 = Dense 層，橘色 = Dropout 正則化，綠色 = Softmax 輸出（10 類野火成因）。\n\nmodel = Sequential()\nmodel.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(num_classes, activation='softmax'))\n\nmodel.compile(\n    loss='categorical_crossentropy',\n    optimizer='adam',\n    metrics=['accuracy']\n)\n\n\n\nmodel.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n# Train/Test Split: 70/30\n\n\n\n\n\n\nMetric\nValue\n\n\n\n\nWildfire cause prediction accuracy\n~45.6%\n\n\nPoisson regression trend\n+0.44% annual increase\n\n\nTotal records processed\n1,880,465\n\n\n\n\n\n\n\n\n\n\n\n\n\n結果分析： 10 類隨機猜測 baseline 為 10%，MLP 達到 ~45.6%，遠優於隨機但仍有提升空間。分類準確率偏低反映了僅依靠地理位置（經緯度）與時間（年份、月份）來判斷野火成因的固有難度 — 許多成因（人為 vs 雷擊）在空間上高度重疊。\n\n\n\n\n\n\nTask / 任務： Classify cervical cell images into three types (Type 1, 2, 3) corresponding to different levels of cervical transformation zone, using transfer learning with EfficientNet-B7 and Focal Loss to handle class imbalance. 將子宮頸細胞影像分類為三種類型（Type 1/2/3），對應不同程度的子宮頸轉化帶，採用 EfficientNet-B7 遷移學習並以 Focal Loss 處理類別不平衡。\nDataset / 資料集： Intel & MobileODT Cervical Cancer Screening (Kaggle) — 3-class image classification\nMethod / 方法： EfficientNet-B7 (ImageNet pretrained, fine-tuned) + Focal Loss + Data Augmentation\n\n\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35}}}%%\nflowchart LR\n    A[\"Pretrained EfficientNet-B7     \"] --&gt; B[\"Freeze Backbone     \"] --&gt; C[\"Classifier → 3 cls     \"] --&gt; D[\"Focal Loss γ=2     \"] --&gt; E[\"Type 1/2/3     \"]\n\n    style A fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style B fill:#F5F5F5,color:#424242,stroke:#BDBDBD,stroke-width:2px\n    style C fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style D fill:#F3E5F5,color:#6A1B9A,stroke:#CE93D8,stroke-width:2px\n    style E fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n\n\n\n\n\n\n\n遷移學習策略： 先凍結 EfficientNet-B7 的 ImageNet 預訓練 backbone 作為特徵提取器，僅訓練新增的分類頭。使用 Focal Loss 解決 Type 1/2/3 的樣本不平衡問題。\n\n\n\n\nfrom torchvision.models import efficientnet_b7, EfficientNet_B7_Weights\n\nmodel = efficientnet_b7(weights=EfficientNet_B7_Weights.IMAGENET1K_V1)\nnum_features = model.classifier[1].in_features\nmodel.classifier[1] = nn.Linear(num_features, num_classes)  # num_classes = 3\nmodel = model.to(device)\n\n\n\nFocal Loss down-weights easy examples and focuses training on hard, misclassified samples — especially useful for imbalanced class distributions. Focal Loss 降低簡單樣本的權重，讓訓練集中在難以分類的樣本，有效處理類別不平衡。\n\n\n\n\n\n\n\n\n\n\nFocal Loss 原理： 當 gamma=0 等同於標準 Cross-Entropy。gamma 越大，對「已經分對的 easy examples」（右側綠色區域）懲罰越小，讓模型集中學習 hard examples。本專案使用 gamma=2（橘色虛線）。\n\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=1, gamma=2, reduction=\"mean\"):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.reduction = reduction\n\n    def forward(self, inputs, targets):\n        ce_loss = nn.CrossEntropyLoss(reduction=\"none\")(inputs, targets)\n        pt = torch.exp(-ce_loss)\n        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n        if self.reduction == \"mean\":\n            return focal_loss.mean()\n        elif self.reduction == \"sum\":\n            return focal_loss.sum()\n        return focal_loss\n\ncriterion = FocalLoss(alpha=1, gamma=2)\n\n\n\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nnum_epochs = 20\nbatch_size = 32\n\nfor epoch in range(num_epochs):\n    model.train()\n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n\n\n\n\n\nClass\nDescription\nAccuracy\n\n\n\n\nType 1\nEctocervix (fully visible transformation zone)\n87.5%\n\n\nType 2\nPartially visible transformation zone\n92.3%\n\n\nType 3\nEndocervix (transformation zone not visible)\n78.5%\n\n\n\n\n\n\n\n\n\n\n\n\n\n分類結果分析： Type 2（部分可見轉化帶）準確率最高達 92.3%，因為特徵最明確。Type 3（轉化帶不可見）最低 78.5%，因為缺乏可辨識的表面結構特徵，分類難度最高。平均準確率 86.1%（灰色虛線）。Focal Loss 有效提升了少數類別的學習效果。"
  },
  {
    "objectID": "ML.html#assignments-作業",
    "href": "ML.html#assignments-作業",
    "title": "Machine Learning & Data Science",
    "section": "",
    "text": "Task / 任務： Predict the onset of diabetes using the Pima Indians Diabetes dataset, with a focus on handling missing values through regression-based imputation, feature engineering, and resampling before training a deep neural network. 以 Pima Indians 糖尿病資料集為基礎，先用迴歸填補缺失值、進行特徵工程與資料重抽樣，再訓練深層神經網路進行二元分類。\nDataset / 資料集： Pima Indians Diabetes Dataset — 768 samples, 8 features\nMethod / 方法： Regression imputation → Feature engineering → Resampling → DNN\n\n\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35, \"nodeSpacing\": 25, \"rankSpacing\": 40}}}%%\nflowchart TD\n    A[\"Raw Data 768×8     \"] --&gt; B[\"Mark Zeros as Missing     \"]\n    B --&gt; C[\"Regression Imputation     \"]\n    C --&gt; D[\"Feature Eng. + Resample     \"]\n    D --&gt; E[\"Train / Val Split     \"]\n    E --&gt; F[\"DNN 64→32→16→1     \"]\n    F --&gt; G[\"90.04% acc     \"]\n\n    style A fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style B fill:#F5F5F5,color:#424242,stroke:#BDBDBD,stroke-width:2px\n    style C fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style D fill:#F3E5F5,color:#6A1B9A,stroke:#CE93D8,stroke-width:2px\n    style E fill:#F5F5F5,color:#424242,stroke:#BDBDBD,stroke-width:2px\n    style F fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style G fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n\n\n\n\n\n\n\nPipeline 說明： 原始資料中 Glucose、BMI 等欄位含有「零值」，實際上代表缺失。先以線性迴歸逐欄填補，再做特徵工程與重抽樣平衡正負樣本，最後以 4 層 DNN 進行二元分類，從 baseline 74% 提升至 90.04%。\n\n\n\n\nZero values in Glucose, BMI, BloodPressure, SkinThickness, and Insulin are treated as missing. After removing rows with missing values, 336 clean rows remain for fitting imputation models. 將 Glucose、BMI 等欄位的零值視為缺失。移除含缺失值的列後，336 筆乾淨資料用於訓練填補模型。\n\n\n\nFeature Correlation Matrix (after removing rows with NaN)\n\n\n\nKey observations / 重要觀察： Glucose → Outcome 相關係數 0.50，是糖尿病最強預測變數；SkinThickness ↔︎ BMI 達 0.71，可用 BMI 填補 SkinThickness；Insulin ↔︎ Glucose 達 0.70，可用 Glucose 填補 Insulin；Age ↔︎ Pregnancies 為 0.54，符合生物學預期。\n\n\n\n\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35}}}%%\nflowchart TD\n    A[\"Outcome     \"]\n    B[\"Glucose     \"]\n    C[\"BMI     \"]\n    D[\"Insulin     \"]\n    E[\"SkinThickness     \"]\n    F[\"Age     \"]\n    G[\"BloodPressure     \"]\n\n    A --&gt;|\"predicts\"| B\n    B --&gt;|\"predicts\"| C\n    B --&gt;|\"predicts\"| D\n    C --&gt;|\"predicts\"| E\n    B --&gt;|\"predicts\"| E\n    F --&gt;|\"predicts\"| G\n    C --&gt;|\"predicts\"| G\n\n    style A fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style B fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style C fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n    style D fill:#FCE4EC,color:#C62828,stroke:#F48FB1,stroke-width:2px\n    style E fill:#F3E5F5,color:#6A1B9A,stroke:#CE93D8,stroke-width:2px\n    style F fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style G fill:#E0F2F1,color:#00695C,stroke:#80CBC4,stroke-width:2px\n\n\n\n\n\n\n\n填補順序： 利用相關性最高的已知欄位作為自變數，以線性迴歸依序填補：Outcome → Glucose → BMI → Insulin / SkinThickness → BloodPressure。每一步都只使用「已經存在或已填補」的欄位當作 predictor。\n\n# Fill Glucose using Outcome\nX_train = df_non_missing[['Outcome']]\ny_train = df_non_missing['Glucose']\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Fill BMI using Glucose\nX_train = df_non_missing[['Glucose']]\ny_train = df_non_missing['BMI']\n\n# Fill Insulin using BMI + Glucose\nX_train = df_non_missing[['BMI', 'Glucose']]\ny_train = df_non_missing['Insulin']\n\n# Fill BloodPressure using Age + BMI\nX_train = df_non_missing[['Age', 'BMI']]\ny_train = df_non_missing['BloodPressure']\n\n\n\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35}}}%%\nflowchart LR\n    I[\"Input 8+ feat     \"] --&gt; L1[\"Dense 64 ReLU     \"] --&gt; L2[\"Dense 32 ReLU     \"] --&gt; L3[\"Dense 16 ReLU     \"] --&gt; O[\"Dense 1 Sigmoid     \"]\n\n    style I  fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style L1 fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style L2 fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style L3 fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style O  fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n\n\n\n\n\n\nmodel = Sequential()\nmodel.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(16, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n\n\n\nmodel.compile(\n    loss='binary_crossentropy',\n    optimizer='adam',\n    metrics=['accuracy']\n)\nmodel.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n\n\n\n\n\n\n\n\n\n\nMetric\nValue\n\n\n\n\nBaseline accuracy (no imputation)\n74.03%\n\n\nFinal accuracy (with imputation + feature engineering + resampling)\n90.04%\n\n\nImprovement\n+16.01 pp\n\n\n\n\n\n\n\n\n\n\n\n\n\nAccuracy 分析： Baseline（直接丟棄缺失值）僅有 74.03%。迴歸填補後提升至 77.92%，特徵工程再加 ~5 pp，最終搭配 resampling + DNN 達到 90.04%，共提升 +16 pp。灰色虛線為 baseline 參考線，橘色箭頭標示整體增幅。\n\n\n\n\n\n\nTask / 任務： Analyze 1.88 million US wildfire records to model annual frequency trends using Poisson regression, and predict wildfire causes using a multi-layer perceptron. 分析 188 萬筆美國野火紀錄，用 Poisson 迴歸建立年度頻率趨勢模型，並以 MLP 預測野火成因。\nDataset / 資料集： US Wildfires (1992–2015) — 1,880,465 records, Kaggle\nMethod / 方法： Poisson Regression (trend analysis) + MLP (cause classification)\n\n\nModels the annual count of wildfires as a function of year to estimate long-term trend. 建立野火年度數量對年份的 Poisson 迴歸，估計長期增長趨勢。\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\npoisson_model = smf.glm(\n    formula='Count ~ FIRE_YEAR',\n    data=fire_counts,\n    family=sm.families.Poisson()\n).fit()\n\nprint(poisson_model.summary())\n\n\n\n\n\n\n\n\n\n\nPoisson 趨勢： 以年份為自變數擬合 Poisson 迴歸，估計野火年度發生頻率以每年 +0.44% 的速度增長。橘色虛線 為迴歸擬合線，青色長條 為各年度實際數量。\n\n\n\n\nFeatures: FIRE_SIZE, LATITUDE, LONGITUDE, FIRE_YEAR, MONTH\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35}}}%%\nflowchart LR\n    F[\"5 Features     \"] --&gt; D1[\"Dense 64 ReLU     \"] --&gt; DR1[\"Dropout 0.3     \"] --&gt; D2[\"Dense 64 ReLU     \"] --&gt; DR2[\"Dropout 0.3     \"] --&gt; O[\"Softmax → N cls     \"]\n\n    style F   fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style D1  fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style DR1 fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style D2  fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style DR2 fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style O   fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n\n\n\n\n\n\n\nArchitecture 說明： 兩層 Dense 64 + Dropout 0.3 的簡單 MLP。藍色 = Dense 層，橘色 = Dropout 正則化，綠色 = Softmax 輸出（10 類野火成因）。\n\nmodel = Sequential()\nmodel.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(num_classes, activation='softmax'))\n\nmodel.compile(\n    loss='categorical_crossentropy',\n    optimizer='adam',\n    metrics=['accuracy']\n)\n\n\n\nmodel.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n# Train/Test Split: 70/30\n\n\n\n\n\n\nMetric\nValue\n\n\n\n\nWildfire cause prediction accuracy\n~45.6%\n\n\nPoisson regression trend\n+0.44% annual increase\n\n\nTotal records processed\n1,880,465\n\n\n\n\n\n\n\n\n\n\n\n\n\n結果分析： 10 類隨機猜測 baseline 為 10%，MLP 達到 ~45.6%，遠優於隨機但仍有提升空間。分類準確率偏低反映了僅依靠地理位置（經緯度）與時間（年份、月份）來判斷野火成因的固有難度 — 許多成因（人為 vs 雷擊）在空間上高度重疊。\n\n\n\n\n\n\nTask / 任務： Classify cervical cell images into three types (Type 1, 2, 3) corresponding to different levels of cervical transformation zone, using transfer learning with EfficientNet-B7 and Focal Loss to handle class imbalance. 將子宮頸細胞影像分類為三種類型（Type 1/2/3），對應不同程度的子宮頸轉化帶，採用 EfficientNet-B7 遷移學習並以 Focal Loss 處理類別不平衡。\nDataset / 資料集： Intel & MobileODT Cervical Cancer Screening (Kaggle) — 3-class image classification\nMethod / 方法： EfficientNet-B7 (ImageNet pretrained, fine-tuned) + Focal Loss + Data Augmentation\n\n\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35}}}%%\nflowchart LR\n    A[\"Pretrained EfficientNet-B7     \"] --&gt; B[\"Freeze Backbone     \"] --&gt; C[\"Classifier → 3 cls     \"] --&gt; D[\"Focal Loss γ=2     \"] --&gt; E[\"Type 1/2/3     \"]\n\n    style A fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style B fill:#F5F5F5,color:#424242,stroke:#BDBDBD,stroke-width:2px\n    style C fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style D fill:#F3E5F5,color:#6A1B9A,stroke:#CE93D8,stroke-width:2px\n    style E fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n\n\n\n\n\n\n\n遷移學習策略： 先凍結 EfficientNet-B7 的 ImageNet 預訓練 backbone 作為特徵提取器，僅訓練新增的分類頭。使用 Focal Loss 解決 Type 1/2/3 的樣本不平衡問題。\n\n\n\n\nfrom torchvision.models import efficientnet_b7, EfficientNet_B7_Weights\n\nmodel = efficientnet_b7(weights=EfficientNet_B7_Weights.IMAGENET1K_V1)\nnum_features = model.classifier[1].in_features\nmodel.classifier[1] = nn.Linear(num_features, num_classes)  # num_classes = 3\nmodel = model.to(device)\n\n\n\nFocal Loss down-weights easy examples and focuses training on hard, misclassified samples — especially useful for imbalanced class distributions. Focal Loss 降低簡單樣本的權重，讓訓練集中在難以分類的樣本，有效處理類別不平衡。\n\n\n\n\n\n\n\n\n\n\nFocal Loss 原理： 當 gamma=0 等同於標準 Cross-Entropy。gamma 越大，對「已經分對的 easy examples」（右側綠色區域）懲罰越小，讓模型集中學習 hard examples。本專案使用 gamma=2（橘色虛線）。\n\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=1, gamma=2, reduction=\"mean\"):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.reduction = reduction\n\n    def forward(self, inputs, targets):\n        ce_loss = nn.CrossEntropyLoss(reduction=\"none\")(inputs, targets)\n        pt = torch.exp(-ce_loss)\n        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n        if self.reduction == \"mean\":\n            return focal_loss.mean()\n        elif self.reduction == \"sum\":\n            return focal_loss.sum()\n        return focal_loss\n\ncriterion = FocalLoss(alpha=1, gamma=2)\n\n\n\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nnum_epochs = 20\nbatch_size = 32\n\nfor epoch in range(num_epochs):\n    model.train()\n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n\n\n\n\n\nClass\nDescription\nAccuracy\n\n\n\n\nType 1\nEctocervix (fully visible transformation zone)\n87.5%\n\n\nType 2\nPartially visible transformation zone\n92.3%\n\n\nType 3\nEndocervix (transformation zone not visible)\n78.5%\n\n\n\n\n\n\n\n\n\n\n\n\n\n分類結果分析： Type 2（部分可見轉化帶）準確率最高達 92.3%，因為特徵最明確。Type 3（轉化帶不可見）最低 78.5%，因為缺乏可辨識的表面結構特徵，分類難度最高。平均準確率 86.1%（灰色虛線）。Focal Loss 有效提升了少數類別的學習效果。"
  },
  {
    "objectID": "GISPY.html",
    "href": "GISPY.html",
    "title": "GIS & Python Programming",
    "section": "",
    "text": "Topic / 主題： Financial planning with compound interest calculations 以複利計算為基礎的退休基金規劃工具。\nKey Concepts / 核心概念： Functions, loops, formatted output, compound interest\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35}}}%%\nflowchart LR\n    A[\"User Input     \"] --&gt; B[\"Monthly Contribution     \"] --&gt; C[\"Compound Interest     \"] --&gt; D[\"Year-by-Year Projection     \"] --&gt; E[\"Millionaire Calculator     \"]\n\n    style A fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style B fill:#F5F5F5,color:#424242,stroke:#BDBDBD,stroke-width:2px\n    style C fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style D fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n    style E fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n\n\n\n\n\n\n\n功能說明： 三種模式 — (1) 逐年基金預測（月複利），(2) 百萬富翁計算器（多少年達到 $1M），(3) 利率比較表（1-30% 年利率）。例：$500/月、10% 年利率、30 年 → $1.14M。\n\n# Year-by-year retirement fund projection\ndef calculate_fund(monthly, rate, years):\n    balance = 0\n    monthly_rate = rate / 12\n    for year in range(1, years + 1):\n        for month in range(12):\n            balance += monthly\n            balance *= (1 + monthly_rate)\n        print(f\"Year {year:3d}: ${balance:&gt;12,.2f}\")\n    return balance\n\n# Millionaire Calculator\ndef years_to_million(monthly, rate):\n    balance, months = 0, 0\n    monthly_rate = rate / 12\n    while balance &lt; 1_000_000:\n        balance += monthly\n        balance *= (1 + monthly_rate)\n        months += 1\n    return months // 12, months % 12\n\n\n\n\n\n\n\n\n\n\n複利效果： 前 15 年成長緩慢，後 15 年指數加速。$500/月在 10% 年利率下約 21 年達到百萬美元（橘色菱形）。複利是長期投資的核心驅動力。\n\n\n\n\n\nTopic / 主題： Interactive student grade management with CRUD operations 互動式學生成績管理系統，支援新增、刪除、修改、查詢。\nKey Concepts / 核心概念： Lists, nested data structures, menu-driven programming, input validation\n# Menu-driven gradebook\ndef main_menu():\n    while True:\n        print(\"\\n1. Add student\")\n        print(\"2. Remove student\")\n        print(\"3. Modify grade\")\n        print(\"4. Display gradebook\")\n        print(\"5. Find highest/lowest\")\n        print(\"6. Exit\")\n        choice = input(\"Select: \")\n        # ... handle each option\n\n設計重點： 使用巢狀 list 儲存學生資料 [name, grade]，搭配 while loop 實現持續互動選單。包含輸入驗證（成績範圍 0-100）與錯誤處理。\n\n\n\n\n\nTopic / 主題： Modular programming, reusable modules, and object-oriented design 模組化程式設計、可重用模組與物件導向設計。\nKey Concepts / 核心概念： Module imports, OOP (classes), CSV I/O, distance calculations\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35}}}%%\nflowchart TD\n    A[\"Lab 4 Modules     \"] --&gt; B[\"(a) CSV Field Counter     \"]\n    A --&gt; C[\"(b) Parcel Tax Calculator     \"]\n    A --&gt; D[\"(c) Distance Calculator     \"]\n\n    B --&gt; B1[\"mycount.py + callingscript.py     \"]\n    C --&gt; C1[\"parcelclass.py → OOP     \"]\n    D --&gt; D1[\"Euclidean + Great Circle     \"]\n\n    style A fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style B fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style C fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n    style D fill:#F3E5F5,color:#6A1B9A,stroke:#CE93D8,stroke-width:2px\n    style B1 fill:#F5F5F5,color:#424242,stroke:#BDBDBD,stroke-width:2px\n    style C1 fill:#F5F5F5,color:#424242,stroke:#BDBDBD,stroke-width:2px\n    style D1 fill:#F5F5F5,color:#424242,stroke:#BDBDBD,stroke-width:2px\n\n\n\n\n\n\n# (b) Parcel class with tax assessment\nclass Parcel:\n    def __init__(self, parcel_id, land_use, market_value):\n        self.parcel_id = parcel_id\n        self.land_use = land_use\n        self.market_value = market_value\n\n    def assess_tax(self):\n        rates = {\"SFR\": 0.05, \"MFR\": 0.04}\n        rate = rates.get(self.land_use, 0.02)\n        return self.market_value * rate\n\n# (c) Great Circle Distance (Haversine formula)\nimport math\ndef great_circle(lat1, lon1, lat2, lon2):\n    R = 6371  # Earth radius in km\n    dlat = math.radians(lat2 - lat1)\n    dlon = math.radians(lon2 - lon1)\n    a = (math.sin(dlat/2)**2 +\n         math.cos(math.radians(lat1)) *\n         math.cos(math.radians(lat2)) *\n         math.sin(dlon/2)**2)\n    return R * 2 * math.asin(math.sqrt(a))\n\n三個子任務： (a) 計算 CSV 各欄位空值數量，(b) 以 OOP 建立 Parcel 類別計算不動產稅（SFR 5%、MFR 4%、其他 2%），(c) 以 Haversine 公式計算地球表面兩點距離。\n\n\n\n\n\nTopic / 主題： DataFrame manipulation, grouping, pivoting, and multi-dataset merging Pandas DataFrame 操作、分組、樞紐分析與多資料集合併。\nDatasets / 資料集： MovieLens (100K ratings), COVID-19 global time series\nKey Concepts / 核心概念： pandas Series/DataFrame, boolean indexing, GroupBy, pivot tables, merge/join\n# COVID-19 fatality rate analysis\nfatality = (deaths_total / confirmed_total * 100).sort_values(ascending=False)\n# Peru: 9.17%, Mexico: 7.58%, South Africa: 2.68%\n\n# MovieLens: most rated movies\ntop_movies = ratings.groupby('movieId').size().sort_values(ascending=False).head(10)\n\n# Multi-DataFrame merge\nmerged = pd.merge(users, ratings, on='userId')\nmerged = pd.merge(merged, movies, on='movieId')\n\n分析重點： (1) MovieLens — 找出評分最多的電影、男女評分差異最大的電影，(2) COVID-19 — 確診數前 25 國、各國致死率排名（秘魯 9.17% 最高）、月度增量分析（美國 2021 年 9 月增 +430 萬例）。\n\n\n\n\n\nTopic / 主題： Publication-quality charts with matplotlib and Altair 使用 matplotlib 與 Altair 製作出版品質的圖表與互動式視覺化。\nDatasets / 資料集： MovieLens, COVID-19, Seattle weather (Vega)\n\n\n\n\n\n\nTop 10 Most-Reviewed Movies\n\n\n\n\n\n\n\nCOVID-19 Top 25 Countries\n\n\n\n\n\n\n\n\n\n\n\nGender Rating Differences\n\n\n\n\n\n\n\nSeattle Weather Distribution\n\n\n\n\n\n\n視覺化技巧： 水平長條圖比較電影評論數量、散點圖呈現男女評分差異、圓餅圖展示天氣類型分布、折線圖追蹤 COVID-19 趨勢。Altair 部分製作了互動式 linked chart（點選國家 → 顯示該國死亡趨勢）。\n\n\n\n\n\nTopic / 主題： Desktop application: Number guessing game with graphical interface 使用 PyQt5 開發數字猜謎遊戲桌面應用程式。\nKey Concepts / 核心概念： PyQt5 widgets, event-driven programming, signal/slot, Qt Designer\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35}}}%%\nflowchart LR\n    A[\"Qt Designer     \"] --&gt; B[\"frmGuess.py     \"] --&gt; C[\"lab7.py     \"] --&gt; D[\"Number Game GUI     \"]\n\n    style A fill:#F5F5F5,color:#424242,stroke:#BDBDBD,stroke-width:2px\n    style B fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style C fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style D fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n\n\n\n\n\n\n# Number guessing game with hint system\nclass GuessGame:\n    def __init__(self):\n        self.target = random.randint(1, 100)\n        self.guesses = 0\n\n    def make_guess(self, n):\n        self.guesses += 1\n        if n == self.target:\n            return \"Correct!\"\n        return \"Higher!\" if n &lt; self.target else \"Lower!\"\n\n    def use_hint(self):\n        \"\"\"Costs 5 guesses, reveals number within ±5\"\"\"\n        self.guesses += 5\n        return (self.target - 5, self.target + 5)\n\nGUI 功能： 隨機數字 (1-100) 猜謎遊戲。包含 (1) 猜測追蹤，(2) 提示系統（消耗 5 次機會，縮小範圍到 ±5），(3) 勝利偵測與重設。使用 Qt Designer 設計介面，frmGuess.py 為自動生成的 UI 程式碼。\n\n\n\n\n\nTopic / 主題： Programmatic map creation and spatial data querying with ArcGIS Online 使用 ArcGIS API 程式化建立地圖與查詢空間資料。\nKey Concepts / 核心概念： ArcGIS authentication, WebMap, FeatureLayer queries, basemap cycling\nfrom arcgis.gis import GIS\nfrom arcgis.mapping import WebMap\n\ngis = GIS(\"https://www.arcgis.com\", username, password)\nm = gis.map(\"University of Texas at Dallas\", zoomlevel=15)\n\n# Search and add feature layers\nitems = gis.content.search(\"UTD Buildings\", item_type=\"Feature Layer\")\nm.add_layer(items[0])\n\n# Query building attributes\nfl = items[0].layers[0]\nfl.properties.fields  # Inspect field schema\n\nGIS 操作： 透過 Python API 連接 ArcGIS Online，以 UTD 校園為中心建立互動式地圖，搜尋並疊加建築物圖層，查詢屬性欄位結構，切換不同底圖樣式。\n\n\n\n\n\nTopic / 主題： Data extraction from web sources using APIs and scraping tools 使用 API 與爬蟲工具從網頁來源擷取資料。\nKey Concepts / 核心概念： requests, JSON parsing, BeautifulSoup (HTML), Selenium (dynamic content)\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35}}}%%\nflowchart LR\n    A[\"requests GET     \"] --&gt; B[\"JSON / HTML     \"]\n    B --&gt; C[\"BeautifulSoup     \"]\n    B --&gt; D[\"json.loads()     \"]\n    C --&gt; E[\"Structured Data     \"]\n    D --&gt; E\n\n    style A fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style B fill:#F5F5F5,color:#424242,stroke:#BDBDBD,stroke-width:2px\n    style C fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style D fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n    style E fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n\n\n\n\n\n\nimport requests\nfrom bs4 import BeautifulSoup\n\n# RESTful API request\nresponse = requests.get(\"https://api.example.com/data\")\ndata = response.json()\n\n# HTML scraping\npage = requests.get(\"https://example.com\")\nsoup = BeautifulSoup(page.content, \"html.parser\")\nelements = soup.find_all(\"div\", class_=\"target\")\n\n兩種方法： (1) RESTful API — 發送 GET/POST 請求，解析 JSON 回應，(2) HTML 爬蟲 — 使用 BeautifulSoup 解析 DOM 結構，Selenium 處理動態載入內容。\n\n\n\n\n\nTopic / 主題： Parse structured text files with regex, create spatial visualizations with GeoPandas 使用正規表達式解析結構化文字，搭配 GeoPandas 製作地理空間視覺化。\nDataset / 資料集： worldcities.txt — city coordinates in degrees-minutes format\nimport re\nimport geopandas as gpd\nfrom shapely.geometry import Point\n\n# Parse DMS coordinates with regex\npattern = r\"^(.*)\\t(\\d+)\\t(\\d+) ([NS])\\t(\\d+)\\t(\\d+) ([EW])\\t(.*)$\"\n\nfor line in open(\"worldcities.txt\"):\n    match = re.match(pattern, line.strip())\n    if match:\n        city, lat_d, lat_m, ns, lon_d, lon_m, ew, country = match.groups()\n        lat = (int(lat_d) + int(lat_m)/60) * (-1 if ns == 'S' else 1)\n        lon = (int(lon_d) + int(lon_m)/60) * (-1 if ew == 'W' else 1)\n\n\n\nWorld Cities Map — GeoPandas with Natural Earth basemap\n\n\n\n處理流程： Regex 從 worldcities.txt 擷取城市名、經緯度（度分格式）與國家 → 轉換 DMS 為十進位度數 → 建立 Shapely Point 幾何 → GeoPandas GeoDataFrame → 疊加 Natural Earth 底圖繪製全球城市分布圖。"
  },
  {
    "objectID": "GISPY.html#assignments",
    "href": "GISPY.html#assignments",
    "title": "GIS & Python Programming",
    "section": "",
    "text": "Topic / 主題： Financial planning with compound interest calculations 以複利計算為基礎的退休基金規劃工具。\nKey Concepts / 核心概念： Functions, loops, formatted output, compound interest\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35}}}%%\nflowchart LR\n    A[\"User Input     \"] --&gt; B[\"Monthly Contribution     \"] --&gt; C[\"Compound Interest     \"] --&gt; D[\"Year-by-Year Projection     \"] --&gt; E[\"Millionaire Calculator     \"]\n\n    style A fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style B fill:#F5F5F5,color:#424242,stroke:#BDBDBD,stroke-width:2px\n    style C fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style D fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n    style E fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n\n\n\n\n\n\n\n功能說明： 三種模式 — (1) 逐年基金預測（月複利），(2) 百萬富翁計算器（多少年達到 $1M），(3) 利率比較表（1-30% 年利率）。例：$500/月、10% 年利率、30 年 → $1.14M。\n\n# Year-by-year retirement fund projection\ndef calculate_fund(monthly, rate, years):\n    balance = 0\n    monthly_rate = rate / 12\n    for year in range(1, years + 1):\n        for month in range(12):\n            balance += monthly\n            balance *= (1 + monthly_rate)\n        print(f\"Year {year:3d}: ${balance:&gt;12,.2f}\")\n    return balance\n\n# Millionaire Calculator\ndef years_to_million(monthly, rate):\n    balance, months = 0, 0\n    monthly_rate = rate / 12\n    while balance &lt; 1_000_000:\n        balance += monthly\n        balance *= (1 + monthly_rate)\n        months += 1\n    return months // 12, months % 12\n\n\n\n\n\n\n\n\n\n\n複利效果： 前 15 年成長緩慢，後 15 年指數加速。$500/月在 10% 年利率下約 21 年達到百萬美元（橘色菱形）。複利是長期投資的核心驅動力。\n\n\n\n\n\nTopic / 主題： Interactive student grade management with CRUD operations 互動式學生成績管理系統，支援新增、刪除、修改、查詢。\nKey Concepts / 核心概念： Lists, nested data structures, menu-driven programming, input validation\n# Menu-driven gradebook\ndef main_menu():\n    while True:\n        print(\"\\n1. Add student\")\n        print(\"2. Remove student\")\n        print(\"3. Modify grade\")\n        print(\"4. Display gradebook\")\n        print(\"5. Find highest/lowest\")\n        print(\"6. Exit\")\n        choice = input(\"Select: \")\n        # ... handle each option\n\n設計重點： 使用巢狀 list 儲存學生資料 [name, grade]，搭配 while loop 實現持續互動選單。包含輸入驗證（成績範圍 0-100）與錯誤處理。\n\n\n\n\n\nTopic / 主題： Modular programming, reusable modules, and object-oriented design 模組化程式設計、可重用模組與物件導向設計。\nKey Concepts / 核心概念： Module imports, OOP (classes), CSV I/O, distance calculations\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35}}}%%\nflowchart TD\n    A[\"Lab 4 Modules     \"] --&gt; B[\"(a) CSV Field Counter     \"]\n    A --&gt; C[\"(b) Parcel Tax Calculator     \"]\n    A --&gt; D[\"(c) Distance Calculator     \"]\n\n    B --&gt; B1[\"mycount.py + callingscript.py     \"]\n    C --&gt; C1[\"parcelclass.py → OOP     \"]\n    D --&gt; D1[\"Euclidean + Great Circle     \"]\n\n    style A fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style B fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style C fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n    style D fill:#F3E5F5,color:#6A1B9A,stroke:#CE93D8,stroke-width:2px\n    style B1 fill:#F5F5F5,color:#424242,stroke:#BDBDBD,stroke-width:2px\n    style C1 fill:#F5F5F5,color:#424242,stroke:#BDBDBD,stroke-width:2px\n    style D1 fill:#F5F5F5,color:#424242,stroke:#BDBDBD,stroke-width:2px\n\n\n\n\n\n\n# (b) Parcel class with tax assessment\nclass Parcel:\n    def __init__(self, parcel_id, land_use, market_value):\n        self.parcel_id = parcel_id\n        self.land_use = land_use\n        self.market_value = market_value\n\n    def assess_tax(self):\n        rates = {\"SFR\": 0.05, \"MFR\": 0.04}\n        rate = rates.get(self.land_use, 0.02)\n        return self.market_value * rate\n\n# (c) Great Circle Distance (Haversine formula)\nimport math\ndef great_circle(lat1, lon1, lat2, lon2):\n    R = 6371  # Earth radius in km\n    dlat = math.radians(lat2 - lat1)\n    dlon = math.radians(lon2 - lon1)\n    a = (math.sin(dlat/2)**2 +\n         math.cos(math.radians(lat1)) *\n         math.cos(math.radians(lat2)) *\n         math.sin(dlon/2)**2)\n    return R * 2 * math.asin(math.sqrt(a))\n\n三個子任務： (a) 計算 CSV 各欄位空值數量，(b) 以 OOP 建立 Parcel 類別計算不動產稅（SFR 5%、MFR 4%、其他 2%），(c) 以 Haversine 公式計算地球表面兩點距離。\n\n\n\n\n\nTopic / 主題： DataFrame manipulation, grouping, pivoting, and multi-dataset merging Pandas DataFrame 操作、分組、樞紐分析與多資料集合併。\nDatasets / 資料集： MovieLens (100K ratings), COVID-19 global time series\nKey Concepts / 核心概念： pandas Series/DataFrame, boolean indexing, GroupBy, pivot tables, merge/join\n# COVID-19 fatality rate analysis\nfatality = (deaths_total / confirmed_total * 100).sort_values(ascending=False)\n# Peru: 9.17%, Mexico: 7.58%, South Africa: 2.68%\n\n# MovieLens: most rated movies\ntop_movies = ratings.groupby('movieId').size().sort_values(ascending=False).head(10)\n\n# Multi-DataFrame merge\nmerged = pd.merge(users, ratings, on='userId')\nmerged = pd.merge(merged, movies, on='movieId')\n\n分析重點： (1) MovieLens — 找出評分最多的電影、男女評分差異最大的電影，(2) COVID-19 — 確診數前 25 國、各國致死率排名（秘魯 9.17% 最高）、月度增量分析（美國 2021 年 9 月增 +430 萬例）。\n\n\n\n\n\nTopic / 主題： Publication-quality charts with matplotlib and Altair 使用 matplotlib 與 Altair 製作出版品質的圖表與互動式視覺化。\nDatasets / 資料集： MovieLens, COVID-19, Seattle weather (Vega)\n\n\n\n\n\n\nTop 10 Most-Reviewed Movies\n\n\n\n\n\n\n\nCOVID-19 Top 25 Countries\n\n\n\n\n\n\n\n\n\n\n\nGender Rating Differences\n\n\n\n\n\n\n\nSeattle Weather Distribution\n\n\n\n\n\n\n視覺化技巧： 水平長條圖比較電影評論數量、散點圖呈現男女評分差異、圓餅圖展示天氣類型分布、折線圖追蹤 COVID-19 趨勢。Altair 部分製作了互動式 linked chart（點選國家 → 顯示該國死亡趨勢）。\n\n\n\n\n\nTopic / 主題： Desktop application: Number guessing game with graphical interface 使用 PyQt5 開發數字猜謎遊戲桌面應用程式。\nKey Concepts / 核心概念： PyQt5 widgets, event-driven programming, signal/slot, Qt Designer\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35}}}%%\nflowchart LR\n    A[\"Qt Designer     \"] --&gt; B[\"frmGuess.py     \"] --&gt; C[\"lab7.py     \"] --&gt; D[\"Number Game GUI     \"]\n\n    style A fill:#F5F5F5,color:#424242,stroke:#BDBDBD,stroke-width:2px\n    style B fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style C fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style D fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n\n\n\n\n\n\n# Number guessing game with hint system\nclass GuessGame:\n    def __init__(self):\n        self.target = random.randint(1, 100)\n        self.guesses = 0\n\n    def make_guess(self, n):\n        self.guesses += 1\n        if n == self.target:\n            return \"Correct!\"\n        return \"Higher!\" if n &lt; self.target else \"Lower!\"\n\n    def use_hint(self):\n        \"\"\"Costs 5 guesses, reveals number within ±5\"\"\"\n        self.guesses += 5\n        return (self.target - 5, self.target + 5)\n\nGUI 功能： 隨機數字 (1-100) 猜謎遊戲。包含 (1) 猜測追蹤，(2) 提示系統（消耗 5 次機會，縮小範圍到 ±5），(3) 勝利偵測與重設。使用 Qt Designer 設計介面，frmGuess.py 為自動生成的 UI 程式碼。\n\n\n\n\n\nTopic / 主題： Programmatic map creation and spatial data querying with ArcGIS Online 使用 ArcGIS API 程式化建立地圖與查詢空間資料。\nKey Concepts / 核心概念： ArcGIS authentication, WebMap, FeatureLayer queries, basemap cycling\nfrom arcgis.gis import GIS\nfrom arcgis.mapping import WebMap\n\ngis = GIS(\"https://www.arcgis.com\", username, password)\nm = gis.map(\"University of Texas at Dallas\", zoomlevel=15)\n\n# Search and add feature layers\nitems = gis.content.search(\"UTD Buildings\", item_type=\"Feature Layer\")\nm.add_layer(items[0])\n\n# Query building attributes\nfl = items[0].layers[0]\nfl.properties.fields  # Inspect field schema\n\nGIS 操作： 透過 Python API 連接 ArcGIS Online，以 UTD 校園為中心建立互動式地圖，搜尋並疊加建築物圖層，查詢屬性欄位結構，切換不同底圖樣式。\n\n\n\n\n\nTopic / 主題： Data extraction from web sources using APIs and scraping tools 使用 API 與爬蟲工具從網頁來源擷取資料。\nKey Concepts / 核心概念： requests, JSON parsing, BeautifulSoup (HTML), Selenium (dynamic content)\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35}}}%%\nflowchart LR\n    A[\"requests GET     \"] --&gt; B[\"JSON / HTML     \"]\n    B --&gt; C[\"BeautifulSoup     \"]\n    B --&gt; D[\"json.loads()     \"]\n    C --&gt; E[\"Structured Data     \"]\n    D --&gt; E\n\n    style A fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style B fill:#F5F5F5,color:#424242,stroke:#BDBDBD,stroke-width:2px\n    style C fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style D fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n    style E fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n\n\n\n\n\n\nimport requests\nfrom bs4 import BeautifulSoup\n\n# RESTful API request\nresponse = requests.get(\"https://api.example.com/data\")\ndata = response.json()\n\n# HTML scraping\npage = requests.get(\"https://example.com\")\nsoup = BeautifulSoup(page.content, \"html.parser\")\nelements = soup.find_all(\"div\", class_=\"target\")\n\n兩種方法： (1) RESTful API — 發送 GET/POST 請求，解析 JSON 回應，(2) HTML 爬蟲 — 使用 BeautifulSoup 解析 DOM 結構，Selenium 處理動態載入內容。\n\n\n\n\n\nTopic / 主題： Parse structured text files with regex, create spatial visualizations with GeoPandas 使用正規表達式解析結構化文字，搭配 GeoPandas 製作地理空間視覺化。\nDataset / 資料集： worldcities.txt — city coordinates in degrees-minutes format\nimport re\nimport geopandas as gpd\nfrom shapely.geometry import Point\n\n# Parse DMS coordinates with regex\npattern = r\"^(.*)\\t(\\d+)\\t(\\d+) ([NS])\\t(\\d+)\\t(\\d+) ([EW])\\t(.*)$\"\n\nfor line in open(\"worldcities.txt\"):\n    match = re.match(pattern, line.strip())\n    if match:\n        city, lat_d, lat_m, ns, lon_d, lon_m, ew, country = match.groups()\n        lat = (int(lat_d) + int(lat_m)/60) * (-1 if ns == 'S' else 1)\n        lon = (int(lon_d) + int(lon_m)/60) * (-1 if ew == 'W' else 1)\n\n\n\nWorld Cities Map — GeoPandas with Natural Earth basemap\n\n\n\n處理流程： Regex 從 worldcities.txt 擷取城市名、經緯度（度分格式）與國家 → 轉換 DMS 為十進位度數 → 建立 Shapely Point 幾何 → GeoPandas GeoDataFrame → 疊加 Natural Earth 底圖繪製全球城市分布圖。"
  },
  {
    "objectID": "GISPY.html#midterm",
    "href": "GISPY.html#midterm",
    "title": "GIS & Python Programming",
    "section": "Midterm / 期中考",
    "text": "Midterm / 期中考\n\nMidterm Project — Data Analysis Suite / 資料分析組合\n三個獨立的 Python 程式，展示檔案操作、資料分析與不動產查詢能力。\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35}}}%%\nflowchart TD\n    A[\"Midterm Project     \"] --&gt; B[\"Alumni Research     \"]\n    A --&gt; C[\"File Manipulation     \"]\n    A --&gt; D[\"Real Estate Search     \"]\n\n    B --&gt; B1[\"Income & Debt by Major     \"]\n    C --&gt; C1[\"Recursive CSV Processing     \"]\n    D --&gt; D1[\"Multi-criteria Property Filter     \"]\n\n    style A fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style B fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style C fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n    style D fill:#F3E5F5,color:#6A1B9A,stroke:#CE93D8,stroke-width:2px\n    style B1 fill:#F5F5F5,color:#424242,stroke:#BDBDBD,stroke-width:2px\n    style C1 fill:#F5F5F5,color:#424242,stroke:#BDBDBD,stroke-width:2px\n    style D1 fill:#F5F5F5,color:#424242,stroke:#BDBDBD,stroke-width:2px\n\n\n\n\n\n\n\n1. Alumni Research / 校友研究\n分析校友收入與學貸：按科系統計平均年齡、性別比例、收入排名、學貸還款計算器（以年收入 5% 為月還款）。\n# Loan payoff calculator: 5% annual income as monthly payment\ndef loan_payoff(debt, annual_income, interest_rate=0.05):\n    monthly_payment = annual_income * 0.05 / 12\n    balance, months = debt, 0\n    while balance &gt; 0:\n        balance *= (1 + interest_rate / 12)\n        balance -= monthly_payment\n        months += 1\n    return months // 12, months % 12\n\n\n2. File Manipulation / 檔案操作\n遞迴走訪目錄，偵測 CSV 檔案並以 tab 分隔格式美化輸出。\nimport os\ndef process_directory(path):\n    if os.path.isfile(path) and path.endswith('.csv'):\n        pretty_print_csv(path)\n    elif os.path.isdir(path):\n        for root, dirs, files in os.walk(path):\n            for f in files:\n                if f.endswith('.csv'):\n                    pretty_print_csv(os.path.join(root, f))\n\n\n3. Real Estate Search / 不動產搜尋\n多條件過濾地產：州別代碼、最小居住面積、市價範圍、指定學區。\n\n期中總結： 三個程式分別展示 (1) pandas 資料分析與合併（merge on ID），(2) os.walk() 遞迴檔案處理，(3) 多條件邏輯篩選。涵蓋資料科學、系統操作與實務應用三大面向。"
  },
  {
    "objectID": "GISPY.html#final-project",
    "href": "GISPY.html#final-project",
    "title": "GIS & Python Programming",
    "section": "Final Project / 期末專案",
    "text": "Final Project / 期末專案\n\nSVD Image Compression Application / SVD 影像壓縮應用程式\nTask / 任務： Build an interactive desktop application for image compression using Singular Value Decomposition (SVD), with real-time preview and quality metrics. 建立互動式桌面應用程式，使用奇異值分解 (SVD) 進行影像壓縮，具備即時預覽與品質指標。\nMethod / 方法： PyQt6 GUI + NumPy SVD + PIL image processing\nMathematical Foundation / 數學基礎： Eckart-Young Theorem — A_k = sum(sigma_i * u_i * v_i^T) for i=1 to k\n\nApplication Architecture / 應用程式架構\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35}}}%%\nflowchart TD\n    A[\"Image Input (drag & drop)     \"] --&gt; B[\"RGB Channel Split     \"]\n    B --&gt; C[\"np.linalg.svd per channel     \"]\n    C --&gt; D[\"Low-rank Approximation A_k     \"]\n    D --&gt; E[\"Reconstruct RGB     \"]\n    E --&gt; F[\"PSNR + File Size     \"]\n    F --&gt; G[\"Preview & Export     \"]\n\n    style A fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style B fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style C fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style D fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style E fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n    style F fill:#F3E5F5,color:#6A1B9A,stroke:#CE93D8,stroke-width:2px\n    style G fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n\n\n\n\n\n\n\nSVD 原理： 任何矩陣 A 可分解為 A = U Sigma V^T。取前 k 個奇異值重建 A_k，即為最佳 rank-k 近似（Eckart-Young 定理）。k 越小壓縮率越高，但品質越低。\n\n\n\nCore Algorithm / 核心演算法\nimport numpy as np\nfrom PIL import Image\n\ndef perform_svd(image_array):\n    \"\"\"SVD on each RGB channel separately\"\"\"\n    channels = {}\n    for i, name in enumerate(['R', 'G', 'B']):\n        U, S, Vt = np.linalg.svd(image_array[:, :, i], full_matrices=False)\n        channels[name] = (U, S, Vt)\n    return channels\n\ndef reconstruct(channels, k):\n    \"\"\"Low-rank approximation with k singular values\"\"\"\n    reconstructed = np.zeros_like(original)\n    for i, name in enumerate(['R', 'G', 'B']):\n        U, S, Vt = channels[name]\n        reconstructed[:, :, i] = np.clip(\n            U[:, :k] @ np.diag(S[:k]) @ Vt[:k, :], 0, 255\n        )\n    return reconstructed.astype(np.uint8)\n\ndef calculate_psnr(original, compressed):\n    mse = np.mean((original.astype(float) - compressed.astype(float)) ** 2)\n    return 10 * np.log10(255**2 / mse) if mse &gt; 0 else float('inf')\n\n\nGUI Features / 介面功能\n# PyQt6 GUI with dual slider control\nclass SVDCompressor(QMainWindow):\n    def __init__(self):\n        # Drag-and-drop image upload\n        # Dual slider: compression ratio ↔ target file size (linked)\n        # Smart presets:\n        #   Social media: 2 MB, ~35 dB PSNR\n        #   Email:        5 MB, ~40 dB PSNR\n        #   High quality: 80% compression, ~45 dB PSNR\n        pass\n\n\nApplication Demo / 應用程式展示\n\n\n\nSVD Compression App — PyQt6 Desktop Application\n\n\n\nApp 功能一覽：\n\n拖曳上傳 — 將圖片拖移至介面即可載入\n雙滑桿連動 — 壓縮比例 ↔︎ 目標檔案大小（拖動一條另一條自動調整）\n智慧建議 — 三種預設模板：社群媒體 (2 MB)、郵件附件 (5 MB)、高品質存檔 (PSNR &gt; 40 dB)\n即時預覽 — 左右對比原圖 vs 壓縮後，下方顯示 PSNR / 檔案大小 / 壓縮比\n品質警告 — PSNR 低於 40 dB 時自動彈出警告\n基於 Eckart-Young 定理 — 理論保證最佳低秩近似\n\n\n📎 Download： SVD_app.py (原始碼)\n\n\nSVD Quality Analysis / SVD 品質分析\n\n\n\n\n\n\nRGB Channel Decomposition\n\n\n\n\n\n\n\nSVD Metrics vs k (MSE, PSNR, norm, sigma)\n\n\n\n\n\n\n\nWarning in annotate(\"label\", x = 300, y = 34, label = \"k=300: 31.88 dB\", :\nIgnoring unknown parameters: `label.size`\n\n\n\n\n\n\n\n\n\n\n品質分析： 左圖 — PSNR 隨 k 增加而提升，k=300 時達 31.88 dB（超過 30 dB 門檻，灰色虛線），k=700 時接近原圖品質 (44.70 dB)。右圖 — MSE 呈指數下降，k=300 後改善趨緩。實務結論： k=200~400 是壓縮率與品質的最佳平衡區間。\n\n\n\n\nk\nPSNR (dB)\nMSE\nCompression Ratio\n\n\n\n\n1\n12.50\n3650\n99.9%\n\n\n50\n27.10\n127\n95.1%\n\n\n100\n29.50\n73\n90.3%\n\n\n300\n31.88\n42.29\n70.9%\n\n\n700\n44.70\n2.2\n32.0%\n\n\n\n\nEckart-Young 定理驗證： 實驗確認 ||A - A_k||₂ ≈ sigma_{k+1}，即低秩近似的誤差等於第 k+1 個奇異值。這為選擇最佳 k 值提供了理論依據 — 當 sigma_{k+1} 小於品質門檻時即可停止。"
  },
  {
    "objectID": "GISPY.html#course-skills-summary-課程技能總覽",
    "href": "GISPY.html#course-skills-summary-課程技能總覽",
    "title": "GIS & Python Programming",
    "section": "Course Skills Summary / 課程技能總覽",
    "text": "Course Skills Summary / 課程技能總覽\n\n\n\n\n\n\n\n\n\n\n課程總結： 從基礎 Python（函式、OOP、檔案操作）到資料科學（pandas、視覺化）、GIS 空間分析（ArcGIS、GeoPandas）、GUI 開發（PyQt5/6）、網頁爬蟲（requests、BeautifulSoup），最終以 SVD 影像壓縮專案整合數學理論與軟體工程能力。"
  },
  {
    "objectID": "DataAnalysis.html#hw1-svd-image-compression-svd-影像壓縮",
    "href": "DataAnalysis.html#hw1-svd-image-compression-svd-影像壓縮",
    "title": "Data Analysis Mathematics",
    "section": "HW1 — SVD Image Compression / SVD 影像壓縮",
    "text": "HW1 — SVD Image Compression / SVD 影像壓縮\nTopic / 主題： 以 SVD 分解驗證 Eckart-Young 定理：\\(\\|A - A_k\\|_2 = \\sigma_{k+1}\\)\nDataset： 彩色照片 1546×1029 pixels（RGB 三通道）\n\nPipeline Overview / 整體流程\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35}}}%%\nflowchart TD\n    A[\"Load Image 1546×1029×3     \"] --&gt; B[\"Split R, G, B     \"]\n    B --&gt; C[\"np.linalg.svd per channel     \"]\n    C --&gt; D[\"Reconstruct A_k for k = 1..1029     \"]\n    D --&gt; E[\"Compute MSE, PSNR, 2-norm     \"]\n    E --&gt; F[\"Compare 2-norm vs σ_k+1     \"]\n\n    style A fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style B fill:#F5F5F5,color:#424242,stroke:#BDBDBD,stroke-width:2px\n    style C fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style D fill:#F3E5F5,color:#6A1B9A,stroke:#CE93D8,stroke-width:2px\n    style E fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style F fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n\n\n\n\n\n\n\n方法說明： 以照片作為矩陣 A，對 RGB 三通道分別做 SVD。刻意不使用 np.linalg.norm(ord=2)（因為其內部實作就是 SVD，會造成循環論證），改用 Monte Carlo 隨機向量法 近似 operator 2-norm，再與 σ_{k+1} 比較趨勢。\n\n\n\nKey Formulas / 關鍵公式\nSVD 分解：\n\\[A = U \\Sigma V^T, \\quad A_k = \\sum_{i=1}^{k} \\sigma_i u_i v_i^T\\]\nEckart-Young 定理：\n\\[\\|A - A_k\\|_2 = \\sigma_{k+1}\\]\n\n\nResults / 結果\n\n\n\nk\n2-norm (approx)\nσ_{k+1}\nMSE\nPSNR (dB)\n\n\n\n\n1\n3,310\n34,072\n3,363\n12.95\n\n\n10\n1,556\n10,116\n1,403\n16.77\n\n\n50\n837\n3,645\n487\n21.35\n\n\n100\n577\n2,108\n241\n24.37\n\n\n300\n234\n699\n42\n31.88\n\n\n500\n116\n340\n10\n38.04\n\n\n700\n54\n172\n2.2\n44.70\n\n\n\n\n\nReconstruction Quality / 重建品質\n\n\n\nSVD 影像重建 — 不同 k 值的壓縮效果\n\n\n\n視覺化： k=1 時幾乎看不出原圖；k=50 開始可辨識主要輪廓；k=300 以上接近原圖。右下方 k=507 時 PSNR 已達 38 dB，肉眼幾乎無法分辨與原圖差異。\n\n\n\nMSE / PSNR / 2-norm Analysis\n\n\n\nMSE、PSNR、2-norm vs σ_{k+1} 隨 k 值變化\n\n\n\n結論： MSE 隨 k 遞減、PSNR 遞增，且 2-norm 與 σ_{k+1} 呈現相同遞減趨勢，間接驗證 Eckart-Young 定理。Monte Carlo 法因隨機抽樣會低估真實 2-norm，但趨勢一致。"
  },
  {
    "objectID": "DataAnalysis.html#hw2-handwritten-digit-recognition-手寫數字辨識",
    "href": "DataAnalysis.html#hw2-handwritten-digit-recognition-手寫數字辨識",
    "title": "Data Analysis Mathematics",
    "section": "HW2 — Handwritten Digit Recognition / 手寫數字辨識",
    "text": "HW2 — Handwritten Digit Recognition / 手寫數字辨識\nTopic / 主題： 比較 8 種分類方法在 USPS 手寫數字資料集上的表現\nDataset： USPS 手寫數字（16×16 灰階，0-9 共 10 類）\n\nDataset Samples / 資料集樣本\n\n\n\nUSPS 訓練集樣本 — 16×16 灰階手寫數字\n\n\n\n資料特徵： 每張圖片僅 16×16 = 256 pixels，解析度極低但保留了數字的基本結構。這使得矩陣分解方法（SVD / HOSVD）能夠有效捕捉低維特徵。\n\n\n\nMethods / 方法總覽\n\n\n\nCategory\nMethod\nDescription\n\n\n\n\nBaseline\nMean Method\n計算每類平均影像，以歐氏距離分類\n\n\n\nEnhanced Mean\n迭代修正，考慮類間混淆模式\n\n\n矩陣分解\nSVD (k=20)\n建立每類 SVD 基底，投影殘差分類\n\n\n\nHOSVD\nTucker 分解 (tensorly)，rank (10,10,20)\n\n\n傳統 ML\nSVM\nsklearn SVC\n\n\n\nKNN\nsklearn KNeighborsClassifier\n\n\n\nRandom Forest\nsklearn RandomForestClassifier\n\n\n深度學習\nCNN (PyTorch)\nConv2D → MaxPool → Dense，50 epochs\n\n\n\n\n\nAccuracy Comparison / 準確率比較\n\n\n\n\n\n\n\n\n\n\n結果分析： Mean 方法僅 81.42% 作為 baseline。SVD (k=20) 跳升至 94.12%，說明低秩近似能有效捕捉數字結構。傳統 ML（SVM 94.92%）與 SVD 系列表現接近。CNN 以 95.76% 勝出，但訓練時間為 98 秒，遠高於 KNN 的 0.45 秒。\n\n\n\nTraining Time Comparison / 訓練時間比較\n\n\n\n\n\n\n\n\n\n\n速度 vs 精度： KNN 以 0.45 秒達到 94.47%，是性價比最高的模型。CNN 雖然最準（95.76%）但耗時 98 秒。HOSVD（Tucker 分解）精度與 SVD 接近但慢了 30 倍。\n\n\n\nSVD Confusion Matrix\n\n\n\nSVD (k=20) Confusion Matrix — USPS 手寫數字分類\n\n\n\n混淆矩陣分析： SVD 方法對 0、1 的辨識最佳（353/264 正確），對 3、7 的混淆較高（3→5 有 13 筆誤判）。整體 94.12% 的準確率在無需訓練的矩陣分解方法中表現優異。\n\n\n\nMisclassified Samples / 誤判樣本\n\n\n\nMean Method 誤判案例 — True label vs Predicted label\n\n\n\n誤判分析： 這些樣本即使是人眼也不容易辨識。例如 True: 6 被判成 2（筆畫圓弧相似）、True: 3 被判成 8（形狀接近）。低解析度 16×16 下，部分數字的結構差異極小。\n\n\n\nReal Handwriting Test / 手寫實測\n\n\n\n外部手寫數字照片樣本\n\n\n\n\n\n8 種模型在外部手寫照片上的準確率\n\n\n\n外部驗證： 用手機拍攝手寫數字照片測試所有模型。CNN 達到 100% 完美辨識，Random Forest 95%，而 Enhanced Mean 僅 56%。這驗證了深度學習對真實世界手寫變異的泛化能力最強。"
  }
]