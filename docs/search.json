[
  {
    "objectID": "DSIC.html",
    "href": "DSIC.html",
    "title": "MS of Data Science & Information Computing",
    "section": "",
    "text": "å­¸æ­·ï¼š åœ‹ç«‹ä¸­èˆˆå¤§å­¸ è³‡æ–™ç§‘å­¸èˆ‡è³‡è¨Šè¨ˆç®—ç ”ç©¶æ‰€ ç¢©å£« (Master of Science in Data Science and Information Computing, NCHU)ã€‚"
  },
  {
    "objectID": "DSIC.html#about-the-program-ç ”ç©¶æ‰€ç°¡ä»‹",
    "href": "DSIC.html#about-the-program-ç ”ç©¶æ‰€ç°¡ä»‹",
    "title": "MS of Data Science & Information Computing",
    "section": "About the Program / ç ”ç©¶æ‰€ç°¡ä»‹",
    "text": "About the Program / ç ”ç©¶æ‰€ç°¡ä»‹\nè³‡æ–™ç§‘å­¸èˆ‡è³‡è¨Šè¨ˆç®—ç ”ç©¶æ‰€ï¼ˆGraduate Institute of Data Science and Information Computingï¼‰éš¸å±¬åœ‹ç«‹ä¸­èˆˆå¤§å­¸ç†å­¸é™¢ï¼Œæˆç«‹å®—æ—¨åœ¨åŸ¹é¤Šå…¼å…·è³‡æ–™ç§‘å­¸ç†è«–åŸºç¤èˆ‡è³‡è¨Šè¨ˆç®—å¯¦ä½œèƒ½åŠ›çš„è·¨é ˜åŸŸäººæ‰ã€‚èª²ç¨‹æ¶µè“‹æ©Ÿå™¨å­¸ç¿’ã€æ·±åº¦å­¸ç¿’ã€å¤§æ•¸æ“šåˆ†æã€å½±åƒè™•ç†ã€æœ€ä½³åŒ–æ–¹æ³•èˆ‡é«˜æ•ˆèƒ½è¨ˆç®—ç­‰ï¼Œå¼·èª¿ã€Œå¾æ•¸å­¸åˆ°å¯¦ä½œã€çš„å®Œæ•´è¨“ç·´ã€‚\nThe program integrates mathematical foundations with modern computing to train professionals in machine learning, deep learning, computer vision, and big data analytics. Students are equipped to bridge the gap between theoretical models and real-world applications across domains including healthcare, industry, and scientific research."
  },
  {
    "objectID": "DSIC.html#research-focus-areas-ç ”ç©¶æ–¹å‘",
    "href": "DSIC.html#research-focus-areas-ç ”ç©¶æ–¹å‘",
    "title": "MS of Data Science & Information Computing",
    "section": "Research Focus Areas / ç ”ç©¶æ–¹å‘",
    "text": "Research Focus Areas / ç ”ç©¶æ–¹å‘\n\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"22px\"}, \"flowchart\": {\"nodeSpacing\": 20, \"rankSpacing\": 60, \"padding\": 30, \"useMaxWidth\": false}}}%%\nflowchart LR\n    ROOT[\"DSIC Research Areas     \"]\n\n    subgraph s1 [\" \"]\n      direction LR\n      ML[\"Machine Learning     \"] --&gt; ML1[\"Classification & Regression     \"]\n      ML --&gt; ML2[\"Ensemble Methods     \"]\n      ML --&gt; ML3[\"Feature Engineering     \"]\n    end\n\n    subgraph s2 [\" \"]\n      direction LR\n      DL[\"Deep Learning     \"] --&gt; DL1[\"CNNs & Transfer Learning     \"]\n      DL --&gt; DL2[\"GANs & Image Generation     \"]\n      DL --&gt; DL3[\"Semantic Segmentation     \"]\n    end\n\n    subgraph s3 [\" \"]\n      direction LR\n      CV[\"Computer Vision     \"] --&gt; CV1[\"Medical Imaging     \"]\n      CV --&gt; CV2[\"Defect Detection     \"]\n      CV --&gt; CV3[\"Image Reconstruction     \"]\n    end\n\n    subgraph s4 [\" \"]\n      direction LR\n      OPT[\"Optimization     \"] --&gt; OPT1[\"Loss Function Design     \"]\n      OPT --&gt; OPT2[\"Hyperparameter Tuning     \"]\n    end\n\n    ROOT --&gt; ML\n    ROOT --&gt; DL\n    ROOT --&gt; CV\n    ROOT --&gt; OPT\n\n    style ROOT fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style ML   fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n    style DL   fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style CV   fill:#F3E5F5,color:#6A1B9A,stroke:#CE93D8,stroke-width:2px\n    style OPT  fill:#FCE4EC,color:#C62828,stroke:#F48FB1,stroke-width:2px\n    style ML1 fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7\n    style ML2 fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7\n    style ML3 fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7\n    style DL1 fill:#FFF3E0,color:#E65100,stroke:#FFCC80\n    style DL2 fill:#FFF3E0,color:#E65100,stroke:#FFCC80\n    style DL3 fill:#FFF3E0,color:#E65100,stroke:#FFCC80\n    style CV1 fill:#F3E5F5,color:#6A1B9A,stroke:#CE93D8\n    style CV2 fill:#F3E5F5,color:#6A1B9A,stroke:#CE93D8\n    style CV3 fill:#F3E5F5,color:#6A1B9A,stroke:#CE93D8\n    style OPT1 fill:#FCE4EC,color:#C62828,stroke:#F48FB1\n    style OPT2 fill:#FCE4EC,color:#C62828,stroke:#F48FB1\n    style s1 fill:none,stroke:none\n    style s2 fill:none,stroke:none\n    style s3 fill:none,stroke:none\n    style s4 fill:none,stroke:none\n\n\n\n\n\n\n\n\nè‰²ç¢¼èªªæ˜ï¼š ç¶ è‰² = Machine Learningï¼Œæ©˜è‰² = Deep Learningï¼Œç´«è‰² = Computer Visionï¼Œç²‰ç´…è‰² = Optimizationã€‚ä»¥ä¸Šç‚ºèª²ç¨‹ä¸­æ¶µè“‹çš„ä¸»è¦ç ”ç©¶é ˜åŸŸã€‚"
  },
  {
    "objectID": "DSIC.html#skills-acquired-ç¿’å¾—æŠ€èƒ½",
    "href": "DSIC.html#skills-acquired-ç¿’å¾—æŠ€èƒ½",
    "title": "MS of Data Science & Information Computing",
    "section": "Skills Acquired / ç¿’å¾—æŠ€èƒ½",
    "text": "Skills Acquired / ç¿’å¾—æŠ€èƒ½\n\n\n\n\n\n\n\n\n\n\næŠ€èƒ½ä¾†æºï¼š ç²‰è‰² = ML + DL å…©é–€èª²éƒ½æœ‰è¨“ç·´åˆ°çš„æŠ€èƒ½ï¼Œé’è‰² = ä¸»è¦ä¾†è‡ª ML èª²ç¨‹ï¼Œæ©˜è‰² = ä¸»è¦ä¾†è‡ª DL èª²ç¨‹ã€‚Python/PyTorch ç‚ºå…©é–€èª²çš„å…±åŒç¨‹å¼èªè¨€ï¼ŒTransfer Learning èˆ‡ Model Evaluation ä¹Ÿè·¨èª²ç¨‹åè¦†ç·´ç¿’ã€‚"
  },
  {
    "objectID": "DSIC.html#coursework-èª²ç¨‹ä½œå“",
    "href": "DSIC.html#coursework-èª²ç¨‹ä½œå“",
    "title": "MS of Data Science & Information Computing",
    "section": "Coursework / èª²ç¨‹ä½œå“",
    "text": "Coursework / èª²ç¨‹ä½œå“\n\n\nMachine Learning & Data Science\nèª²ç¨‹é‡é»ï¼š ç›£ç£å¼/éç›£ç£å¼å­¸ç¿’ã€æ¨¡å‹è©•ä¼°ã€ç‰¹å¾µå·¥ç¨‹ã€è³‡æ–™å‰è™•ç†\nAssignmentsï¼š\n\nHW1 â€” Diabetes Predictionï¼š Pima Indians ç³–å°¿ç—…é æ¸¬ã€‚è¿´æ­¸å¡«è£œ â†’ ç‰¹å¾µå·¥ç¨‹ â†’ DNNï¼Œå¾ baseline 74% â†’ 90.04%\nHW2 â€” US Wildfire Analysisï¼š 188 è¬ç­†é‡ç«ç´€éŒ„ã€‚Poisson è¿´æ­¸è¶¨å‹¢åˆ†æ + MLP æˆå› åˆ†é¡ï¼ˆ45.6%ï¼‰\nFinal â€” Cervical Cancer Screeningï¼š EfficientNet-B7 é·ç§»å­¸ç¿’ + Focal Lossï¼Œä¸‰é¡åˆ¥å½±åƒåˆ†é¡ï¼Œå¹³å‡ 86.1%\n\nKey Toolsï¼š Python Â· Keras Â· scikit-learn Â· statsmodels Â· PyTorch (final)\n\n\nDeep Learning\nèª²ç¨‹é‡é»ï¼š CNN æ¶æ§‹ã€é·ç§»å­¸ç¿’ã€èªæ„åˆ†å‰²ã€å½±åƒç”Ÿæˆã€è‡ªå‹•ç·¨ç¢¼å™¨\nAssignmentsï¼š\n\nHW1 â€” AOI Defect Classificationï¼š ResNet-18 é·ç§»å­¸ç¿’ï¼Œ6 é¡å·¥æ¥­ç¼ºé™·åˆ†é¡ï¼Œ96.44% Val Acc\nHW2 â€” Retinal Vessel Segmentationï¼š U-Net (5-level) + Focal Tversky Lossï¼ŒDRIVE è³‡æ–™é›†ï¼ŒmIoU 0.351\nHW3 â€” Retinal Image Reconstructionï¼š Convolutional Autoencoderï¼ŒPSNR å³°å€¼ 30.84 dBï¼ˆEpoch 18ï¼‰\nHW4 â€” Western Blot Generationï¼š Conditional GANï¼ˆGenerator + PatchGAN Discriminatorï¼‰ï¼Œåˆ†æ D/G è¨“ç·´å‹•æ…‹\n\nKey Toolsï¼š Python Â· PyTorch Â· torchvision Â· Apple Silicon (MPS)\n\n\nBig Data Analysis / å·¨é‡è³‡æ–™åˆ†æ\nèª²ç¨‹é‡é»ï¼š Kernel Methods åŠ é€Ÿã€å¤§è¦æ¨¡æœ€ä½³åŒ–ã€åˆ†æ•£å¼æ©Ÿå™¨å­¸ç¿’\nAssignmentsï¼š\n\nReading â€” NystrÃ¶m Methodï¼š è«–æ–‡é–±è®€ (NIPS 2000)ï¼ŒGram matrix ä½ç§©è¿‘ä¼¼ï¼ŒO(nÂ³) â†’ O(mÂ²n)\nHW â€” Kernel Ridge + NystrÃ¶mï¼š USPS æ‰‹å¯«æ•¸å­—åˆ†é¡ï¼Œm=128 åŠ é€Ÿ 20 å€ï¼Œaccuracy 99.50%\nFinal â€” Smoothed & Distributed SVMï¼š a9a è³‡æ–™é›†ï¼ŒSmoothed Hinge Loss + åˆ†æ•£å¼æ¢¯åº¦èšåˆï¼ˆK=5 workersï¼‰ï¼ŒåŠ é€Ÿ 150 å€\n\nKey Toolsï¼š Python Â· NumPy Â· scikit-learn Â· SciPy (L-BFGS-B)\n\n\nData Analysis Mathematics / æ•¸æ“šåˆ†ææ•¸å­¸\nèª²ç¨‹é‡é»ï¼š SVD ç†è«–èˆ‡æ‡‰ç”¨ã€çŸ©é™£ä½ç§©è¿‘ä¼¼ã€Eckart-Young å®šç†ã€æ‰‹å¯«è¾¨è­˜\nAssignmentsï¼š\n\nHW1 â€” SVD Image Compressionï¼š ä»¥ç…§ç‰‡é©—è­‰ Eckart-Young å®šç†ï¼ŒMonte Carlo è¿‘ä¼¼ 2-normï¼ŒPSNR é” 44.7 dB (k=700)\nHW2 â€” Handwritten Digit Recognitionï¼š USPS è³‡æ–™é›†ï¼Œæ¯”è¼ƒ 8 ç¨®æ–¹æ³•ï¼ˆMean / SVD / HOSVD / SVM / KNN / RF / CNNï¼‰ï¼ŒCNN 95.76% æœ€é«˜\n\nKey Toolsï¼š Python Â· NumPy Â· PyTorch Â· tensorly Â· scikit-learn"
  },
  {
    "objectID": "DSIC.html#project-highlights-ä½œå“äº®é»",
    "href": "DSIC.html#project-highlights-ä½œå“äº®é»",
    "title": "MS of Data Science & Information Computing",
    "section": "Project Highlights / ä½œå“äº®é»",
    "text": "Project Highlights / ä½œå“äº®é»\n\n\n\n\n\n\n\n\nCourse\nProject\nHighlight\n\n\n\n\nMachine Learning\nDiabetes Prediction\nBaseline 74% â†’ 90.04%ï¼ˆ+16 ppï¼‰\n\n\n\nWildfire Analysis\n188 è¬ç­† Â· Poisson è¶¨å‹¢ + MLP åˆ†é¡\n\n\n\nCervical Cancer\nEfficientNet-B7 + Focal Loss Â· 86.1%\n\n\nDeep Learning\nAOI Defect Detection\nResNet-50 Fine-tune Â· 96.4%\n\n\n\nU-Net Segmentation\nDice 0.91 Â· é†«å­¸å½±åƒèªæ„åˆ†å‰²\n\n\n\nAutoEncoder\nå½±åƒé‡å»º 30.8 dB PSNR\n\n\n\ncGAN Blot Removal\næ¢ä»¶å¼ GAN å»é™¤å¢¨æ¼¬\n\n\nBig Data\nNystrÃ¶m Approximation\nKernel Ridge åŠ é€Ÿ 20Ã—\n\n\n\nSmoothed SVM\nHinge Loss å¹³æ»‘åŒ– Â· æ¢¯åº¦æ³•æ±‚è§£\n\n\n\nDistributed SVM\nåˆ†æ•£å¼è¨ˆç®—åŠ é€Ÿ 150Ã—\n\n\nData Analysis Math\nSVD Image Compression\nEckart-Young é©—è­‰ Â· PSNR 44.7 dB\n\n\n\nDigit Recognition (8 models)\nCNN 95.76% Â· KNN æœ€ä½³æ€§åƒ¹æ¯”\n\n\n\n\nå­¸ç¿’æ­·ç¨‹ï¼š å¾ Data Analysis Math çš„æ•¸å­¸åŸºç¤ï¼ˆSVDã€çŸ©é™£è¿‘ä¼¼ï¼‰â†’ ML èª²ç¨‹çš„ç¶“å…¸æ¨¡å‹ï¼ˆè¿´æ­¸ã€DNNã€é·ç§»å­¸ç¿’ï¼‰â†’ DL èª²ç¨‹çš„é€²éšæ¶æ§‹ï¼ˆU-Netã€AutoEncoderã€GANï¼‰â†’ Big Data çš„å¤§è¦æ¨¡åŠ é€Ÿæ–¹æ³•ï¼ˆNystrÃ¶mã€Smoothed SVMã€åˆ†æ•£å¼è¨ˆç®—ï¼‰ã€‚æ¯å€‹ project éƒ½æ¶µè“‹å®Œæ•´çš„ pipeline â€” å¾è³‡æ–™å‰è™•ç†ã€æ¨¡å‹è¨­è¨ˆã€è¨“ç·´ã€åˆ°çµæœåˆ†æèˆ‡è¦–è¦ºåŒ–ã€‚"
  },
  {
    "objectID": "CV.html",
    "href": "CV.html",
    "title": "Tzu-Yuan Chen",
    "section": "",
    "text": "chen.tzuyuan666@gmail.com\n   (+1) 945-371-1551 / (+886) 963-672-946\n   chentzuyuan.github.io\n   Dallas, TX / Taichung, Taiwan\n\n\nM.S. student in Data Science (NCHU) and Social Data Analytics (UTD) with a B.S. in Applied Mathematics. Seeking a data science / ML internship to apply hands-on project experience â€” including image classification (96.44% accuracy), medical image segmentation, and distributed learning (150x speedup) â€” in a professional setting. Strong foundation in numerical analysis, deep learning, and end-to-end ML pipelines.\n\n Technical Skills\n\n\n\nProgramming Languages\nPython\nR\nSQL\nQiskit\nGit\nLaTeX\n\nPython Ecosystem\npandas\nNumPy\nscikit-learn\nPyTorch\nKeras / TensorFlow\nstatsmodels\nGeoPandas\nSelenium\nBeautifulSoup\nPyQt6\n\nML / Deep Learning\nResNet\nEfficientNet\nU-Net\nConditional GAN\nAutoencoder\nSVM\nKNN\nRandom Forest\nMLP / DNN\nKernel Ridge\nTransfer Learning\nFocal Loss\n\n\n\nMath & Theory\nLinear Algebra / SVD\nHOSVD / Tucker\nNystrÃ¶m Approximation\nProbability & Bayes\nOptimization\nNumerical Methods\n\nBig Data & Distributed\nDistributed SVM\nCommunication-efficient SGD\nKernel Approximation\n\nLLM & AI\nAI Agent Workflows\nPrompt Engineering\nRAG\nLLM API Integration\n\nGIS & Spatial\nArcGIS Python API\nGeoPandas\nShapely\n\nVisualization & Tools\nggplot2\nShiny\nAltair\nMatplotlib\nQt Designer\nQuarto\n\nHardware & Platforms\nIBM Quantum (IBMQ)\n3D Printing (FDM)\n\nLanguages\nChinese (Native)\nEnglish (Professional)\n\n\n\n Education\n\n\n  \n    M.S. Social Data Analytics & Research (SDAR)\n    The University of Texas at Dallas â€” UTD-NCHU Dual-Degree Cohort\n    2025 â€“ present\n  \n  \n    M.S. Data Science & Information Computing\n    National Chung Hsing University (NCHU), Taiwan\n    2024 â€“ present\n  \n  \n    B.S. Applied Mathematics\n    National Chung Hsing University (NCHU), Taiwan\n    2020 â€“ 2024\n  \n\n Projects\n\nComputer Vision & Image Classification\n\n\n  AOI Defect Classification â€” ResNet-18 transfer learning on industrial inspection images (6-class). 96.44% accuracy\n  View Details â†’\n\n\n\n  Cervical Cancer Screening â€” EfficientNet-B7 + Focal Loss on medical images (3-class). 86.1% avg accuracy\n  View Details â†’\n\n\n\n  Handwritten Digit Recognition â€” 8-model comparison (Mean / SVD / HOSVD / SVM / KNN / RF / CNN) on USPS dataset. CNN 95.76%\n  View Details â†’\n\n\nMedical Image Analysis & Generation\n\n\n  Retinal Vessel Segmentation â€” U-Net (5-level encoder-decoder) + Focal Tversky Loss on DRIVE dataset. mIoU 0.351\n  View Details â†’\n\n\n\n  Retinal Image Reconstruction â€” Convolutional Autoencoder for retinal fundus image reconstruction. PSNR 30.84 dB\n  View Details â†’\n\n\n\n  Western Blot Image Synthesis â€” Conditional GAN (Generator + PatchGAN Discriminator) for biomedical image generation.\n  View Details â†’\n\n\nScalable ML & Big Data\n\n\n  NystrÃ¶m Kernel Ridge Regression â€” Gram matrix approximation on USPS digits (m=128). 20Ã— speedup, 99.50% accuracy maintained\n  View Details â†’\n\n\n\n  Distributed SVM â€” 5-worker parallelization with Smoothed Hinge Loss on a9a dataset. 150Ã— speedup\n  View Details â†’\n\n\n\n  US Wildfire Trend Analysis â€” Poisson regression + MLP classification on 1.88M records (1992â€“2015).\n  View Details â†’\n\n\nData Engineering & Applications\n\n\n  SVD Image Compression App â€” PyQt6 GUI application with real-time preview, Eckart-Young theorem verification. PSNR 44.7 dB\n  View Details â†’\n\n\n\n  Wardrobe Recommendation Database â€” 15-table relational schema with weather/occasion-aware rule-based recommendation engine.\n  View Details â†’\n\n\n\n  GIS Web Mapping & Spatial Analysis â€” GeoPandas, Shapely, ArcGIS Python API for world cities spatial analysis.\n  View Details â†’\n\n Experience\n\n\n  2022 â€“ 2024\n  Bilingual Teaching Assistant\n  National Chung Hsing University\n  \n    Calculus â€” One-on-one support for international students; led recitation sessions linking calculus theory to numerical algorithms\n    Database Systems Design â€” Assisted with SQL assignments; introduced Power Query; provided bilingual translations of lecture notes\n    Quantum Computing â€” Guided students through Python & Qiskit assignments; explained probabilistic reasoning and simulation tasks\n  \n\n\n\n  2023 â€“ 2024\n  Undergraduate Thesis: Quantum Bayesian Inference\n  Department of Applied Mathematics, NCHU\n  \n    Investigated whether quantum computer outputs conform to Bayesian probability laws using Qiskit on IBMQ hardware\n    Applied Bayesian updating to analyze qubit measurement outcomes; revealed divergence between actual quantum runs and simulator predictions\n  \n\n\n\n  2020 â€“ 2024\n  Part-Time Roles\n  Various, Taiwan\n  \n    Bilingual Conference Staff (NCHU) â€” Escorted visiting scholars, coordinated logistics, provided real-time bilingual assistance\n    Cram-School Instructor & Private Tutor â€” Mathematics, physics, chemistry, biology, Chinese, and English\n    3D Printing Studio â€” FDM printer operation, slicing and print workflow management\n  \n\n\n\n  International Volunteer\n  \n    Taught English and art at rural elementary schools; participated in desert-area afforestation projects\n  \n\n\n  Last updated: February 2025 Â Â·Â  Save as PDF"
  },
  {
    "objectID": "ML.html",
    "href": "ML.html",
    "title": "Machine Learning & Data Science",
    "section": "",
    "text": "Task / ä»»å‹™ï¼š Predict the onset of diabetes using the Pima Indians Diabetes dataset, with a focus on handling missing values through regression-based imputation, feature engineering, and resampling before training a deep neural network. ä»¥ Pima Indians ç³–å°¿ç—…è³‡æ–™é›†ç‚ºåŸºç¤ï¼Œå…ˆç”¨è¿´æ­¸å¡«è£œç¼ºå¤±å€¼ã€é€²è¡Œç‰¹å¾µå·¥ç¨‹èˆ‡è³‡æ–™é‡æŠ½æ¨£ï¼Œå†è¨“ç·´æ·±å±¤ç¥ç¶“ç¶²è·¯é€²è¡ŒäºŒå…ƒåˆ†é¡ã€‚\nDataset / è³‡æ–™é›†ï¼š Pima Indians Diabetes Dataset â€” 768 samples, 8 features\nMethod / æ–¹æ³•ï¼š Regression imputation â†’ Feature engineering â†’ Resampling â†’ DNN\n\n\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35, \"nodeSpacing\": 25, \"rankSpacing\": 40}}}%%\nflowchart TD\n    A[\"Raw Data 768Ã—8     \"] --&gt; B[\"Mark Zeros as Missing     \"]\n    B --&gt; C[\"Regression Imputation     \"]\n    C --&gt; D[\"Feature Eng. + Resample     \"]\n    D --&gt; E[\"Train / Val Split     \"]\n    E --&gt; F[\"DNN 64â†’32â†’16â†’1     \"]\n    F --&gt; G[\"90.04% acc     \"]\n\n    style A fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style B fill:#F5F5F5,color:#424242,stroke:#BDBDBD,stroke-width:2px\n    style C fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style D fill:#F3E5F5,color:#6A1B9A,stroke:#CE93D8,stroke-width:2px\n    style E fill:#F5F5F5,color:#424242,stroke:#BDBDBD,stroke-width:2px\n    style F fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style G fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n\n\n\n\n\n\n\nPipeline èªªæ˜ï¼š åŸå§‹è³‡æ–™ä¸­ Glucoseã€BMI ç­‰æ¬„ä½å«æœ‰ã€Œé›¶å€¼ã€ï¼Œå¯¦éš›ä¸Šä»£è¡¨ç¼ºå¤±ã€‚å…ˆä»¥ç·šæ€§è¿´æ­¸é€æ¬„å¡«è£œï¼Œå†åšç‰¹å¾µå·¥ç¨‹èˆ‡é‡æŠ½æ¨£å¹³è¡¡æ­£è² æ¨£æœ¬ï¼Œæœ€å¾Œä»¥ 4 å±¤ DNN é€²è¡ŒäºŒå…ƒåˆ†é¡ï¼Œå¾ baseline 74% æå‡è‡³ 90.04%ã€‚\n\n\n\n\nZero values in Glucose, BMI, BloodPressure, SkinThickness, and Insulin are treated as missing. After removing rows with missing values, 336 clean rows remain for fitting imputation models. å°‡ Glucoseã€BMI ç­‰æ¬„ä½çš„é›¶å€¼è¦–ç‚ºç¼ºå¤±ã€‚ç§»é™¤å«ç¼ºå¤±å€¼çš„åˆ—å¾Œï¼Œ336 ç­†ä¹¾æ·¨è³‡æ–™ç”¨æ–¼è¨“ç·´å¡«è£œæ¨¡å‹ã€‚\n\n\n\nFeature Correlation Matrix (after removing rows with NaN)\n\n\n\nKey observations / é‡è¦è§€å¯Ÿï¼š Glucose â†’ Outcome ç›¸é—œä¿‚æ•¸ 0.50ï¼Œæ˜¯ç³–å°¿ç—…æœ€å¼·é æ¸¬è®Šæ•¸ï¼›SkinThickness â†”ï¸ BMI é” 0.71ï¼Œå¯ç”¨ BMI å¡«è£œ SkinThicknessï¼›Insulin â†”ï¸ Glucose é” 0.70ï¼Œå¯ç”¨ Glucose å¡«è£œ Insulinï¼›Age â†”ï¸ Pregnancies ç‚º 0.54ï¼Œç¬¦åˆç”Ÿç‰©å­¸é æœŸã€‚\n\n\n\n\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35}}}%%\nflowchart TD\n    A[\"Outcome     \"]\n    B[\"Glucose     \"]\n    C[\"BMI     \"]\n    D[\"Insulin     \"]\n    E[\"SkinThickness     \"]\n    F[\"Age     \"]\n    G[\"BloodPressure     \"]\n\n    A --&gt;|\"predicts\"| B\n    B --&gt;|\"predicts\"| C\n    B --&gt;|\"predicts\"| D\n    C --&gt;|\"predicts\"| E\n    B --&gt;|\"predicts\"| E\n    F --&gt;|\"predicts\"| G\n    C --&gt;|\"predicts\"| G\n\n    style A fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style B fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style C fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n    style D fill:#FCE4EC,color:#C62828,stroke:#F48FB1,stroke-width:2px\n    style E fill:#F3E5F5,color:#6A1B9A,stroke:#CE93D8,stroke-width:2px\n    style F fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style G fill:#E0F2F1,color:#00695C,stroke:#80CBC4,stroke-width:2px\n\n\n\n\n\n\n\nå¡«è£œé †åºï¼š åˆ©ç”¨ç›¸é—œæ€§æœ€é«˜çš„å·²çŸ¥æ¬„ä½ä½œç‚ºè‡ªè®Šæ•¸ï¼Œä»¥ç·šæ€§è¿´æ­¸ä¾åºå¡«è£œï¼šOutcome â†’ Glucose â†’ BMI â†’ Insulin / SkinThickness â†’ BloodPressureã€‚æ¯ä¸€æ­¥éƒ½åªä½¿ç”¨ã€Œå·²ç¶“å­˜åœ¨æˆ–å·²å¡«è£œã€çš„æ¬„ä½ç•¶ä½œ predictorã€‚\n\n# Fill Glucose using Outcome\nX_train = df_non_missing[['Outcome']]\ny_train = df_non_missing['Glucose']\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Fill BMI using Glucose\nX_train = df_non_missing[['Glucose']]\ny_train = df_non_missing['BMI']\n\n# Fill Insulin using BMI + Glucose\nX_train = df_non_missing[['BMI', 'Glucose']]\ny_train = df_non_missing['Insulin']\n\n# Fill BloodPressure using Age + BMI\nX_train = df_non_missing[['Age', 'BMI']]\ny_train = df_non_missing['BloodPressure']\n\n\n\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35}}}%%\nflowchart LR\n    I[\"Input 8+ feat     \"] --&gt; L1[\"Dense 64 ReLU     \"] --&gt; L2[\"Dense 32 ReLU     \"] --&gt; L3[\"Dense 16 ReLU     \"] --&gt; O[\"Dense 1 Sigmoid     \"]\n\n    style I  fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style L1 fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style L2 fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style L3 fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style O  fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n\n\n\n\n\n\nmodel = Sequential()\nmodel.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(16, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n\n\n\nmodel.compile(\n    loss='binary_crossentropy',\n    optimizer='adam',\n    metrics=['accuracy']\n)\nmodel.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n\n\n\n\n\n\n\n\n\n\nMetric\nValue\n\n\n\n\nBaseline accuracy (no imputation)\n74.03%\n\n\nFinal accuracy (with imputation + feature engineering + resampling)\n90.04%\n\n\nImprovement\n+16.01 pp\n\n\n\n\n\n\n\n\n\n\n\n\n\nAccuracy åˆ†æï¼š Baselineï¼ˆç›´æ¥ä¸Ÿæ£„ç¼ºå¤±å€¼ï¼‰åƒ…æœ‰ 74.03%ã€‚è¿´æ­¸å¡«è£œå¾Œæå‡è‡³ 77.92%ï¼Œç‰¹å¾µå·¥ç¨‹å†åŠ  ~5 ppï¼Œæœ€çµ‚æ­é… resampling + DNN é”åˆ° 90.04%ï¼Œå…±æå‡ +16 ppã€‚ç°è‰²è™›ç·šç‚º baseline åƒè€ƒç·šï¼Œæ©˜è‰²ç®­é ­æ¨™ç¤ºæ•´é«”å¢å¹…ã€‚\n\n\n\n\n\n\nTask / ä»»å‹™ï¼š Analyze 1.88 million US wildfire records to model annual frequency trends using Poisson regression, and predict wildfire causes using a multi-layer perceptron. åˆ†æ 188 è¬ç­†ç¾åœ‹é‡ç«ç´€éŒ„ï¼Œç”¨ Poisson è¿´æ­¸å»ºç«‹å¹´åº¦é »ç‡è¶¨å‹¢æ¨¡å‹ï¼Œä¸¦ä»¥ MLP é æ¸¬é‡ç«æˆå› ã€‚\nDataset / è³‡æ–™é›†ï¼š US Wildfires (1992â€“2015) â€” 1,880,465 records, Kaggle\nMethod / æ–¹æ³•ï¼š Poisson Regression (trend analysis) + MLP (cause classification)\n\n\nModels the annual count of wildfires as a function of year to estimate long-term trend. å»ºç«‹é‡ç«å¹´åº¦æ•¸é‡å°å¹´ä»½çš„ Poisson è¿´æ­¸ï¼Œä¼°è¨ˆé•·æœŸå¢é•·è¶¨å‹¢ã€‚\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\npoisson_model = smf.glm(\n    formula='Count ~ FIRE_YEAR',\n    data=fire_counts,\n    family=sm.families.Poisson()\n).fit()\n\nprint(poisson_model.summary())\n\n\n\n\n\n\n\n\n\n\nPoisson è¶¨å‹¢ï¼š ä»¥å¹´ä»½ç‚ºè‡ªè®Šæ•¸æ“¬åˆ Poisson è¿´æ­¸ï¼Œä¼°è¨ˆé‡ç«å¹´åº¦ç™¼ç”Ÿé »ç‡ä»¥æ¯å¹´ +0.44% çš„é€Ÿåº¦å¢é•·ã€‚æ©˜è‰²è™›ç·š ç‚ºè¿´æ­¸æ“¬åˆç·šï¼Œé’è‰²é•·æ¢ ç‚ºå„å¹´åº¦å¯¦éš›æ•¸é‡ã€‚\n\n\n\n\nFeatures: FIRE_SIZE, LATITUDE, LONGITUDE, FIRE_YEAR, MONTH\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35}}}%%\nflowchart LR\n    F[\"5 Features     \"] --&gt; D1[\"Dense 64 ReLU     \"] --&gt; DR1[\"Dropout 0.3     \"] --&gt; D2[\"Dense 64 ReLU     \"] --&gt; DR2[\"Dropout 0.3     \"] --&gt; O[\"Softmax â†’ N cls     \"]\n\n    style F   fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style D1  fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style DR1 fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style D2  fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style DR2 fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style O   fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n\n\n\n\n\n\n\nArchitecture èªªæ˜ï¼š å…©å±¤ Dense 64 + Dropout 0.3 çš„ç°¡å–® MLPã€‚è—è‰² = Dense å±¤ï¼Œæ©˜è‰² = Dropout æ­£å‰‡åŒ–ï¼Œç¶ è‰² = Softmax è¼¸å‡ºï¼ˆ10 é¡é‡ç«æˆå› ï¼‰ã€‚\n\nmodel = Sequential()\nmodel.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(num_classes, activation='softmax'))\n\nmodel.compile(\n    loss='categorical_crossentropy',\n    optimizer='adam',\n    metrics=['accuracy']\n)\n\n\n\nmodel.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n# Train/Test Split: 70/30\n\n\n\n\n\n\nMetric\nValue\n\n\n\n\nWildfire cause prediction accuracy\n~45.6%\n\n\nPoisson regression trend\n+0.44% annual increase\n\n\nTotal records processed\n1,880,465\n\n\n\n\n\n\n\n\n\n\n\n\n\nçµæœåˆ†æï¼š 10 é¡éš¨æ©ŸçŒœæ¸¬ baseline ç‚º 10%ï¼ŒMLP é”åˆ° ~45.6%ï¼Œé å„ªæ–¼éš¨æ©Ÿä½†ä»æœ‰æå‡ç©ºé–“ã€‚åˆ†é¡æº–ç¢ºç‡åä½åæ˜ äº†åƒ…ä¾é åœ°ç†ä½ç½®ï¼ˆç¶“ç·¯åº¦ï¼‰èˆ‡æ™‚é–“ï¼ˆå¹´ä»½ã€æœˆä»½ï¼‰ä¾†åˆ¤æ–·é‡ç«æˆå› çš„å›ºæœ‰é›£åº¦ â€” è¨±å¤šæˆå› ï¼ˆäººç‚º vs é›·æ“Šï¼‰åœ¨ç©ºé–“ä¸Šé«˜åº¦é‡ç–Šã€‚\n\n\n\n\n\n\nTask / ä»»å‹™ï¼š Classify cervical cell images into three types (Type 1, 2, 3) corresponding to different levels of cervical transformation zone, using transfer learning with EfficientNet-B7 and Focal Loss to handle class imbalance. å°‡å­å®®é ¸ç´°èƒå½±åƒåˆ†é¡ç‚ºä¸‰ç¨®é¡å‹ï¼ˆType 1/2/3ï¼‰ï¼Œå°æ‡‰ä¸åŒç¨‹åº¦çš„å­å®®é ¸è½‰åŒ–å¸¶ï¼Œæ¡ç”¨ EfficientNet-B7 é·ç§»å­¸ç¿’ä¸¦ä»¥ Focal Loss è™•ç†é¡åˆ¥ä¸å¹³è¡¡ã€‚\nDataset / è³‡æ–™é›†ï¼š Intel & MobileODT Cervical Cancer Screening (Kaggle) â€” 3-class image classification\nMethod / æ–¹æ³•ï¼š EfficientNet-B7 (ImageNet pretrained, fine-tuned) + Focal Loss + Data Augmentation\n\n\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35}}}%%\nflowchart LR\n    A[\"Pretrained EfficientNet-B7     \"] --&gt; B[\"Freeze Backbone     \"] --&gt; C[\"Classifier â†’ 3 cls     \"] --&gt; D[\"Focal Loss Î³=2     \"] --&gt; E[\"Type 1/2/3     \"]\n\n    style A fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style B fill:#F5F5F5,color:#424242,stroke:#BDBDBD,stroke-width:2px\n    style C fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style D fill:#F3E5F5,color:#6A1B9A,stroke:#CE93D8,stroke-width:2px\n    style E fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n\n\n\n\n\n\n\né·ç§»å­¸ç¿’ç­–ç•¥ï¼š å…ˆå‡çµ EfficientNet-B7 çš„ ImageNet é è¨“ç·´ backbone ä½œç‚ºç‰¹å¾µæå–å™¨ï¼Œåƒ…è¨“ç·´æ–°å¢çš„åˆ†é¡é ­ã€‚ä½¿ç”¨ Focal Loss è§£æ±º Type 1/2/3 çš„æ¨£æœ¬ä¸å¹³è¡¡å•é¡Œã€‚\n\n\n\n\nfrom torchvision.models import efficientnet_b7, EfficientNet_B7_Weights\n\nmodel = efficientnet_b7(weights=EfficientNet_B7_Weights.IMAGENET1K_V1)\nnum_features = model.classifier[1].in_features\nmodel.classifier[1] = nn.Linear(num_features, num_classes)  # num_classes = 3\nmodel = model.to(device)\n\n\n\nFocal Loss down-weights easy examples and focuses training on hard, misclassified samples â€” especially useful for imbalanced class distributions. Focal Loss é™ä½ç°¡å–®æ¨£æœ¬çš„æ¬Šé‡ï¼Œè®“è¨“ç·´é›†ä¸­åœ¨é›£ä»¥åˆ†é¡çš„æ¨£æœ¬ï¼Œæœ‰æ•ˆè™•ç†é¡åˆ¥ä¸å¹³è¡¡ã€‚\n\n\n\n\n\n\n\n\n\n\nFocal Loss åŸç†ï¼š ç•¶ gamma=0 ç­‰åŒæ–¼æ¨™æº– Cross-Entropyã€‚gamma è¶Šå¤§ï¼Œå°ã€Œå·²ç¶“åˆ†å°çš„ easy examplesã€ï¼ˆå³å´ç¶ è‰²å€åŸŸï¼‰æ‡²ç½°è¶Šå°ï¼Œè®“æ¨¡å‹é›†ä¸­å­¸ç¿’ hard examplesã€‚æœ¬å°ˆæ¡ˆä½¿ç”¨ gamma=2ï¼ˆæ©˜è‰²è™›ç·šï¼‰ã€‚\n\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=1, gamma=2, reduction=\"mean\"):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.reduction = reduction\n\n    def forward(self, inputs, targets):\n        ce_loss = nn.CrossEntropyLoss(reduction=\"none\")(inputs, targets)\n        pt = torch.exp(-ce_loss)\n        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n        if self.reduction == \"mean\":\n            return focal_loss.mean()\n        elif self.reduction == \"sum\":\n            return focal_loss.sum()\n        return focal_loss\n\ncriterion = FocalLoss(alpha=1, gamma=2)\n\n\n\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nnum_epochs = 20\nbatch_size = 32\n\nfor epoch in range(num_epochs):\n    model.train()\n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n\n\n\n\n\nClass\nDescription\nAccuracy\n\n\n\n\nType 1\nEctocervix (fully visible transformation zone)\n87.5%\n\n\nType 2\nPartially visible transformation zone\n92.3%\n\n\nType 3\nEndocervix (transformation zone not visible)\n78.5%\n\n\n\n\n\n\n\n\n\n\n\n\n\nåˆ†é¡çµæœåˆ†æï¼š Type 2ï¼ˆéƒ¨åˆ†å¯è¦‹è½‰åŒ–å¸¶ï¼‰æº–ç¢ºç‡æœ€é«˜é” 92.3%ï¼Œå› ç‚ºç‰¹å¾µæœ€æ˜ç¢ºã€‚Type 3ï¼ˆè½‰åŒ–å¸¶ä¸å¯è¦‹ï¼‰æœ€ä½ 78.5%ï¼Œå› ç‚ºç¼ºä¹å¯è¾¨è­˜çš„è¡¨é¢çµæ§‹ç‰¹å¾µï¼Œåˆ†é¡é›£åº¦æœ€é«˜ã€‚å¹³å‡æº–ç¢ºç‡ 86.1%ï¼ˆç°è‰²è™›ç·šï¼‰ã€‚Focal Loss æœ‰æ•ˆæå‡äº†å°‘æ•¸é¡åˆ¥çš„å­¸ç¿’æ•ˆæœã€‚"
  },
  {
    "objectID": "ML.html#assignments-ä½œæ¥­",
    "href": "ML.html#assignments-ä½œæ¥­",
    "title": "Machine Learning & Data Science",
    "section": "",
    "text": "Task / ä»»å‹™ï¼š Predict the onset of diabetes using the Pima Indians Diabetes dataset, with a focus on handling missing values through regression-based imputation, feature engineering, and resampling before training a deep neural network. ä»¥ Pima Indians ç³–å°¿ç—…è³‡æ–™é›†ç‚ºåŸºç¤ï¼Œå…ˆç”¨è¿´æ­¸å¡«è£œç¼ºå¤±å€¼ã€é€²è¡Œç‰¹å¾µå·¥ç¨‹èˆ‡è³‡æ–™é‡æŠ½æ¨£ï¼Œå†è¨“ç·´æ·±å±¤ç¥ç¶“ç¶²è·¯é€²è¡ŒäºŒå…ƒåˆ†é¡ã€‚\nDataset / è³‡æ–™é›†ï¼š Pima Indians Diabetes Dataset â€” 768 samples, 8 features\nMethod / æ–¹æ³•ï¼š Regression imputation â†’ Feature engineering â†’ Resampling â†’ DNN\n\n\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35, \"nodeSpacing\": 25, \"rankSpacing\": 40}}}%%\nflowchart TD\n    A[\"Raw Data 768Ã—8     \"] --&gt; B[\"Mark Zeros as Missing     \"]\n    B --&gt; C[\"Regression Imputation     \"]\n    C --&gt; D[\"Feature Eng. + Resample     \"]\n    D --&gt; E[\"Train / Val Split     \"]\n    E --&gt; F[\"DNN 64â†’32â†’16â†’1     \"]\n    F --&gt; G[\"90.04% acc     \"]\n\n    style A fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style B fill:#F5F5F5,color:#424242,stroke:#BDBDBD,stroke-width:2px\n    style C fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style D fill:#F3E5F5,color:#6A1B9A,stroke:#CE93D8,stroke-width:2px\n    style E fill:#F5F5F5,color:#424242,stroke:#BDBDBD,stroke-width:2px\n    style F fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style G fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n\n\n\n\n\n\n\nPipeline èªªæ˜ï¼š åŸå§‹è³‡æ–™ä¸­ Glucoseã€BMI ç­‰æ¬„ä½å«æœ‰ã€Œé›¶å€¼ã€ï¼Œå¯¦éš›ä¸Šä»£è¡¨ç¼ºå¤±ã€‚å…ˆä»¥ç·šæ€§è¿´æ­¸é€æ¬„å¡«è£œï¼Œå†åšç‰¹å¾µå·¥ç¨‹èˆ‡é‡æŠ½æ¨£å¹³è¡¡æ­£è² æ¨£æœ¬ï¼Œæœ€å¾Œä»¥ 4 å±¤ DNN é€²è¡ŒäºŒå…ƒåˆ†é¡ï¼Œå¾ baseline 74% æå‡è‡³ 90.04%ã€‚\n\n\n\n\nZero values in Glucose, BMI, BloodPressure, SkinThickness, and Insulin are treated as missing. After removing rows with missing values, 336 clean rows remain for fitting imputation models. å°‡ Glucoseã€BMI ç­‰æ¬„ä½çš„é›¶å€¼è¦–ç‚ºç¼ºå¤±ã€‚ç§»é™¤å«ç¼ºå¤±å€¼çš„åˆ—å¾Œï¼Œ336 ç­†ä¹¾æ·¨è³‡æ–™ç”¨æ–¼è¨“ç·´å¡«è£œæ¨¡å‹ã€‚\n\n\n\nFeature Correlation Matrix (after removing rows with NaN)\n\n\n\nKey observations / é‡è¦è§€å¯Ÿï¼š Glucose â†’ Outcome ç›¸é—œä¿‚æ•¸ 0.50ï¼Œæ˜¯ç³–å°¿ç—…æœ€å¼·é æ¸¬è®Šæ•¸ï¼›SkinThickness â†”ï¸ BMI é” 0.71ï¼Œå¯ç”¨ BMI å¡«è£œ SkinThicknessï¼›Insulin â†”ï¸ Glucose é” 0.70ï¼Œå¯ç”¨ Glucose å¡«è£œ Insulinï¼›Age â†”ï¸ Pregnancies ç‚º 0.54ï¼Œç¬¦åˆç”Ÿç‰©å­¸é æœŸã€‚\n\n\n\n\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35}}}%%\nflowchart TD\n    A[\"Outcome     \"]\n    B[\"Glucose     \"]\n    C[\"BMI     \"]\n    D[\"Insulin     \"]\n    E[\"SkinThickness     \"]\n    F[\"Age     \"]\n    G[\"BloodPressure     \"]\n\n    A --&gt;|\"predicts\"| B\n    B --&gt;|\"predicts\"| C\n    B --&gt;|\"predicts\"| D\n    C --&gt;|\"predicts\"| E\n    B --&gt;|\"predicts\"| E\n    F --&gt;|\"predicts\"| G\n    C --&gt;|\"predicts\"| G\n\n    style A fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style B fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style C fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n    style D fill:#FCE4EC,color:#C62828,stroke:#F48FB1,stroke-width:2px\n    style E fill:#F3E5F5,color:#6A1B9A,stroke:#CE93D8,stroke-width:2px\n    style F fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style G fill:#E0F2F1,color:#00695C,stroke:#80CBC4,stroke-width:2px\n\n\n\n\n\n\n\nå¡«è£œé †åºï¼š åˆ©ç”¨ç›¸é—œæ€§æœ€é«˜çš„å·²çŸ¥æ¬„ä½ä½œç‚ºè‡ªè®Šæ•¸ï¼Œä»¥ç·šæ€§è¿´æ­¸ä¾åºå¡«è£œï¼šOutcome â†’ Glucose â†’ BMI â†’ Insulin / SkinThickness â†’ BloodPressureã€‚æ¯ä¸€æ­¥éƒ½åªä½¿ç”¨ã€Œå·²ç¶“å­˜åœ¨æˆ–å·²å¡«è£œã€çš„æ¬„ä½ç•¶ä½œ predictorã€‚\n\n# Fill Glucose using Outcome\nX_train = df_non_missing[['Outcome']]\ny_train = df_non_missing['Glucose']\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Fill BMI using Glucose\nX_train = df_non_missing[['Glucose']]\ny_train = df_non_missing['BMI']\n\n# Fill Insulin using BMI + Glucose\nX_train = df_non_missing[['BMI', 'Glucose']]\ny_train = df_non_missing['Insulin']\n\n# Fill BloodPressure using Age + BMI\nX_train = df_non_missing[['Age', 'BMI']]\ny_train = df_non_missing['BloodPressure']\n\n\n\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35}}}%%\nflowchart LR\n    I[\"Input 8+ feat     \"] --&gt; L1[\"Dense 64 ReLU     \"] --&gt; L2[\"Dense 32 ReLU     \"] --&gt; L3[\"Dense 16 ReLU     \"] --&gt; O[\"Dense 1 Sigmoid     \"]\n\n    style I  fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style L1 fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style L2 fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style L3 fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style O  fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n\n\n\n\n\n\nmodel = Sequential()\nmodel.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(16, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n\n\n\nmodel.compile(\n    loss='binary_crossentropy',\n    optimizer='adam',\n    metrics=['accuracy']\n)\nmodel.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n\n\n\n\n\n\n\n\n\n\nMetric\nValue\n\n\n\n\nBaseline accuracy (no imputation)\n74.03%\n\n\nFinal accuracy (with imputation + feature engineering + resampling)\n90.04%\n\n\nImprovement\n+16.01 pp\n\n\n\n\n\n\n\n\n\n\n\n\n\nAccuracy åˆ†æï¼š Baselineï¼ˆç›´æ¥ä¸Ÿæ£„ç¼ºå¤±å€¼ï¼‰åƒ…æœ‰ 74.03%ã€‚è¿´æ­¸å¡«è£œå¾Œæå‡è‡³ 77.92%ï¼Œç‰¹å¾µå·¥ç¨‹å†åŠ  ~5 ppï¼Œæœ€çµ‚æ­é… resampling + DNN é”åˆ° 90.04%ï¼Œå…±æå‡ +16 ppã€‚ç°è‰²è™›ç·šç‚º baseline åƒè€ƒç·šï¼Œæ©˜è‰²ç®­é ­æ¨™ç¤ºæ•´é«”å¢å¹…ã€‚\n\n\n\n\n\n\nTask / ä»»å‹™ï¼š Analyze 1.88 million US wildfire records to model annual frequency trends using Poisson regression, and predict wildfire causes using a multi-layer perceptron. åˆ†æ 188 è¬ç­†ç¾åœ‹é‡ç«ç´€éŒ„ï¼Œç”¨ Poisson è¿´æ­¸å»ºç«‹å¹´åº¦é »ç‡è¶¨å‹¢æ¨¡å‹ï¼Œä¸¦ä»¥ MLP é æ¸¬é‡ç«æˆå› ã€‚\nDataset / è³‡æ–™é›†ï¼š US Wildfires (1992â€“2015) â€” 1,880,465 records, Kaggle\nMethod / æ–¹æ³•ï¼š Poisson Regression (trend analysis) + MLP (cause classification)\n\n\nModels the annual count of wildfires as a function of year to estimate long-term trend. å»ºç«‹é‡ç«å¹´åº¦æ•¸é‡å°å¹´ä»½çš„ Poisson è¿´æ­¸ï¼Œä¼°è¨ˆé•·æœŸå¢é•·è¶¨å‹¢ã€‚\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\npoisson_model = smf.glm(\n    formula='Count ~ FIRE_YEAR',\n    data=fire_counts,\n    family=sm.families.Poisson()\n).fit()\n\nprint(poisson_model.summary())\n\n\n\n\n\n\n\n\n\n\nPoisson è¶¨å‹¢ï¼š ä»¥å¹´ä»½ç‚ºè‡ªè®Šæ•¸æ“¬åˆ Poisson è¿´æ­¸ï¼Œä¼°è¨ˆé‡ç«å¹´åº¦ç™¼ç”Ÿé »ç‡ä»¥æ¯å¹´ +0.44% çš„é€Ÿåº¦å¢é•·ã€‚æ©˜è‰²è™›ç·š ç‚ºè¿´æ­¸æ“¬åˆç·šï¼Œé’è‰²é•·æ¢ ç‚ºå„å¹´åº¦å¯¦éš›æ•¸é‡ã€‚\n\n\n\n\nFeatures: FIRE_SIZE, LATITUDE, LONGITUDE, FIRE_YEAR, MONTH\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35}}}%%\nflowchart LR\n    F[\"5 Features     \"] --&gt; D1[\"Dense 64 ReLU     \"] --&gt; DR1[\"Dropout 0.3     \"] --&gt; D2[\"Dense 64 ReLU     \"] --&gt; DR2[\"Dropout 0.3     \"] --&gt; O[\"Softmax â†’ N cls     \"]\n\n    style F   fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style D1  fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style DR1 fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style D2  fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style DR2 fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style O   fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n\n\n\n\n\n\n\nArchitecture èªªæ˜ï¼š å…©å±¤ Dense 64 + Dropout 0.3 çš„ç°¡å–® MLPã€‚è—è‰² = Dense å±¤ï¼Œæ©˜è‰² = Dropout æ­£å‰‡åŒ–ï¼Œç¶ è‰² = Softmax è¼¸å‡ºï¼ˆ10 é¡é‡ç«æˆå› ï¼‰ã€‚\n\nmodel = Sequential()\nmodel.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(num_classes, activation='softmax'))\n\nmodel.compile(\n    loss='categorical_crossentropy',\n    optimizer='adam',\n    metrics=['accuracy']\n)\n\n\n\nmodel.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)\n# Train/Test Split: 70/30\n\n\n\n\n\n\nMetric\nValue\n\n\n\n\nWildfire cause prediction accuracy\n~45.6%\n\n\nPoisson regression trend\n+0.44% annual increase\n\n\nTotal records processed\n1,880,465\n\n\n\n\n\n\n\n\n\n\n\n\n\nçµæœåˆ†æï¼š 10 é¡éš¨æ©ŸçŒœæ¸¬ baseline ç‚º 10%ï¼ŒMLP é”åˆ° ~45.6%ï¼Œé å„ªæ–¼éš¨æ©Ÿä½†ä»æœ‰æå‡ç©ºé–“ã€‚åˆ†é¡æº–ç¢ºç‡åä½åæ˜ äº†åƒ…ä¾é åœ°ç†ä½ç½®ï¼ˆç¶“ç·¯åº¦ï¼‰èˆ‡æ™‚é–“ï¼ˆå¹´ä»½ã€æœˆä»½ï¼‰ä¾†åˆ¤æ–·é‡ç«æˆå› çš„å›ºæœ‰é›£åº¦ â€” è¨±å¤šæˆå› ï¼ˆäººç‚º vs é›·æ“Šï¼‰åœ¨ç©ºé–“ä¸Šé«˜åº¦é‡ç–Šã€‚\n\n\n\n\n\n\nTask / ä»»å‹™ï¼š Classify cervical cell images into three types (Type 1, 2, 3) corresponding to different levels of cervical transformation zone, using transfer learning with EfficientNet-B7 and Focal Loss to handle class imbalance. å°‡å­å®®é ¸ç´°èƒå½±åƒåˆ†é¡ç‚ºä¸‰ç¨®é¡å‹ï¼ˆType 1/2/3ï¼‰ï¼Œå°æ‡‰ä¸åŒç¨‹åº¦çš„å­å®®é ¸è½‰åŒ–å¸¶ï¼Œæ¡ç”¨ EfficientNet-B7 é·ç§»å­¸ç¿’ä¸¦ä»¥ Focal Loss è™•ç†é¡åˆ¥ä¸å¹³è¡¡ã€‚\nDataset / è³‡æ–™é›†ï¼š Intel & MobileODT Cervical Cancer Screening (Kaggle) â€” 3-class image classification\nMethod / æ–¹æ³•ï¼š EfficientNet-B7 (ImageNet pretrained, fine-tuned) + Focal Loss + Data Augmentation\n\n\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35}}}%%\nflowchart LR\n    A[\"Pretrained EfficientNet-B7     \"] --&gt; B[\"Freeze Backbone     \"] --&gt; C[\"Classifier â†’ 3 cls     \"] --&gt; D[\"Focal Loss Î³=2     \"] --&gt; E[\"Type 1/2/3     \"]\n\n    style A fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style B fill:#F5F5F5,color:#424242,stroke:#BDBDBD,stroke-width:2px\n    style C fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style D fill:#F3E5F5,color:#6A1B9A,stroke:#CE93D8,stroke-width:2px\n    style E fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n\n\n\n\n\n\n\né·ç§»å­¸ç¿’ç­–ç•¥ï¼š å…ˆå‡çµ EfficientNet-B7 çš„ ImageNet é è¨“ç·´ backbone ä½œç‚ºç‰¹å¾µæå–å™¨ï¼Œåƒ…è¨“ç·´æ–°å¢çš„åˆ†é¡é ­ã€‚ä½¿ç”¨ Focal Loss è§£æ±º Type 1/2/3 çš„æ¨£æœ¬ä¸å¹³è¡¡å•é¡Œã€‚\n\n\n\n\nfrom torchvision.models import efficientnet_b7, EfficientNet_B7_Weights\n\nmodel = efficientnet_b7(weights=EfficientNet_B7_Weights.IMAGENET1K_V1)\nnum_features = model.classifier[1].in_features\nmodel.classifier[1] = nn.Linear(num_features, num_classes)  # num_classes = 3\nmodel = model.to(device)\n\n\n\nFocal Loss down-weights easy examples and focuses training on hard, misclassified samples â€” especially useful for imbalanced class distributions. Focal Loss é™ä½ç°¡å–®æ¨£æœ¬çš„æ¬Šé‡ï¼Œè®“è¨“ç·´é›†ä¸­åœ¨é›£ä»¥åˆ†é¡çš„æ¨£æœ¬ï¼Œæœ‰æ•ˆè™•ç†é¡åˆ¥ä¸å¹³è¡¡ã€‚\n\n\n\n\n\n\n\n\n\n\nFocal Loss åŸç†ï¼š ç•¶ gamma=0 ç­‰åŒæ–¼æ¨™æº– Cross-Entropyã€‚gamma è¶Šå¤§ï¼Œå°ã€Œå·²ç¶“åˆ†å°çš„ easy examplesã€ï¼ˆå³å´ç¶ è‰²å€åŸŸï¼‰æ‡²ç½°è¶Šå°ï¼Œè®“æ¨¡å‹é›†ä¸­å­¸ç¿’ hard examplesã€‚æœ¬å°ˆæ¡ˆä½¿ç”¨ gamma=2ï¼ˆæ©˜è‰²è™›ç·šï¼‰ã€‚\n\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=1, gamma=2, reduction=\"mean\"):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.reduction = reduction\n\n    def forward(self, inputs, targets):\n        ce_loss = nn.CrossEntropyLoss(reduction=\"none\")(inputs, targets)\n        pt = torch.exp(-ce_loss)\n        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n        if self.reduction == \"mean\":\n            return focal_loss.mean()\n        elif self.reduction == \"sum\":\n            return focal_loss.sum()\n        return focal_loss\n\ncriterion = FocalLoss(alpha=1, gamma=2)\n\n\n\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nnum_epochs = 20\nbatch_size = 32\n\nfor epoch in range(num_epochs):\n    model.train()\n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n\n\n\n\n\nClass\nDescription\nAccuracy\n\n\n\n\nType 1\nEctocervix (fully visible transformation zone)\n87.5%\n\n\nType 2\nPartially visible transformation zone\n92.3%\n\n\nType 3\nEndocervix (transformation zone not visible)\n78.5%\n\n\n\n\n\n\n\n\n\n\n\n\n\nåˆ†é¡çµæœåˆ†æï¼š Type 2ï¼ˆéƒ¨åˆ†å¯è¦‹è½‰åŒ–å¸¶ï¼‰æº–ç¢ºç‡æœ€é«˜é” 92.3%ï¼Œå› ç‚ºç‰¹å¾µæœ€æ˜ç¢ºã€‚Type 3ï¼ˆè½‰åŒ–å¸¶ä¸å¯è¦‹ï¼‰æœ€ä½ 78.5%ï¼Œå› ç‚ºç¼ºä¹å¯è¾¨è­˜çš„è¡¨é¢çµæ§‹ç‰¹å¾µï¼Œåˆ†é¡é›£åº¦æœ€é«˜ã€‚å¹³å‡æº–ç¢ºç‡ 86.1%ï¼ˆç°è‰²è™›ç·šï¼‰ã€‚Focal Loss æœ‰æ•ˆæå‡äº†å°‘æ•¸é¡åˆ¥çš„å­¸ç¿’æ•ˆæœã€‚"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Tzu-Yuan Chen / é™³å­å…ƒ\n    Data Science & Applied Math â€” M.S. student at NCHU (Taiwan) & UTD (Dallas)\n    Focused on machine learning, deep learning, numerical analysis, and building data-driven solutions.\n    \n       Email\n       GitHub\n       CV\n    \n  \n\n About Me\nIâ€™m a graduate student from Taichung, Taiwan, currently pursuing dual masterâ€™s degrees in Data Science (NCHU, Taiwan) and Social Data Analytics & Research (UTD, Dallas). I hold a B.S. in Applied Mathematics from National Chung Hsing University and am looking for a data science / ML internship to gain professional experience.\nThrough coursework and projects, Iâ€™ve built end-to-end ML pipelines â€” from image classification and medical image segmentation to distributed learning systems. I enjoy bridging mathematical theory with practical implementations, whether that means verifying the Eckart-Young theorem through SVD image compression or building a 5-worker distributed SVM that runs 150x faster.\nIâ€™ve also served as a bilingual teaching assistant for Calculus, Database Systems, and Quantum Computing courses, supporting both Chinese- and English-speaking students.\n Research Interests\nMachine Learning\nDeep Learning\nLLM / Prompt Engineering\nImage Classification & Segmentation\nQuantum Computing\nNumerical Analysis\nBig Data Analytics\nDatabase Design\nData Visualization\n Skills\n\n\n\nProgramming\nPython\nR\nSQL\nQiskit\nGit\n\nPython Libraries\npandas\nNumPy\nscikit-learn\nPyTorch\nKeras / TensorFlow\nGeoPandas\nSelenium\nBeautifulSoup\nPyQt6\n\nR Ecosystem\nggplot2\ntidyverse\nShiny\n\n\n\nML / DL Models\nResNet\nEfficientNet\nU-Net\nGAN\nAutoencoder\nSVM\nKNN\nRandom Forest\nMLP / DNN\nKernel Ridge\n\nTechniques\nTransfer Learning\nFocal Loss\nImage Segmentation\nNystrÃ¶m Approximation\nDistributed Computing\n\nLLM & AI\nAI Agent Workflows\nPrompt Engineering\nRAG\nLLM API Integration\n\nTools\nMatplotlib\nAltair\nArcGIS\nShapely\nQt Designer\nQuarto\nLaTeX\n\nHardware & Platforms\nIBM Quantum (IBMQ)\n3D Printing (FDM)\n\n\n\n Site Guide\nThis portfolio documents my coursework, projects, and experiments across two graduate programs. Hereâ€™s what youâ€™ll find:\n\n\n  \n    ğŸ“\n    BS of Applied Math\n    Undergraduate thesis on Quantum Bayesian Inference using Qiskit & IBMQ hardware.\n    NCHU Â· 2020â€“2024\n  \n\n  \n    ğŸ§ \n    MS of DSIC â€” Overview\n    Research focus, skills developed, and course highlights for the Data Science program at NCHU.\n    NCHU Â· 2024â€“present\n  \n\n  \n    ğŸ“Š\n    ML & Data Science\n    Diabetes prediction (90.04%), wildfire analysis (1.88M records), cervical cancer classification.\n    NCHU Â· DSIC\n  \n\n  \n    ğŸ”¬\n    Deep Learning\n    ResNet (96.44%), U-Net segmentation, Autoencoder, Conditional GAN for biomedical imaging.\n    NCHU Â· DSIC\n  \n\n  \n    âš¡\n    Big Data Analysis\n    NystrÃ¶m kernel approximation (20Ã— speedup), distributed SVM (150Ã— speedup).\n    NCHU Â· DSIC\n  \n\n  \n    ğŸ§®\n    Data Analysis Math\n    SVD image compression, Eckart-Young theorem, 8-model digit recognition comparison.\n    NCHU Â· DSIC\n  \n\n  \n    ğŸŒ\n    MS of SDAR â€” Overview\n    Social Data Analytics & Research program at UTD (dual-degree with NCHU).\n    UTD Â· 2025â€“present\n  \n\n  \n    ğŸ—ºï¸\n    GIS & Python\n    GeoPandas mapping, PyQt6 GUI app, web scraping, ArcGIS spatial analysis.\n    UTD Â· SDAR\n  \n\n  \n    ğŸ“ˆ\n    Data Visualization\n    ggplot2, base R, Shiny dashboards, 6 chart types, Iris data visualization.\n    UTD Â· SDAR\n  \n\n  \n    ğŸ—„ï¸\n    Information Management\n    Database design, SQL queries, wardrobe recommendation system (15 tables).\n    UTD Â· SDAR\n  \n\n\n\n  Not all content represents final results. Some materials are exploratory or in progress."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tzu-Yuan Chen",
    "section": "",
    "text": "Data Science Â· Applied Math Â· Machine Learning\n  \"Bridging mathematical theory with data-driven solutions.\"\n  \n\n\n\n  \n    10+\n    Domains\n  \n  \n    35\n    Projects\n  \n  \n    3\n    Degree Programs\n  \n\n\nSelected Projects\n\n\n\n  01\n  \n    ResNet Image Classification\n    96.44% accuracy on multi-class classification with focal loss and transfer learning.\n  \n  Deep Learning\n  â†’\n\n\n\n  02\n  \n    Distributed SVM with NystrÃ¶m Approximation\n    150Ã— speedup using 5-worker architecture with kernel approximation on large-scale datasets.\n  \n  Big Data\n  â†’\n\n\n\n  03\n  \n    Medical Image Segmentation\n    U-Net and Conditional GAN for retinal vessel segmentation on biomedical imaging datasets.\n  \n  Deep Learning\n  â†’\n\n\n\n  04\n  \n    Quantum Bayesian Inference\n    Undergraduate thesis investigating Bayesian probability on IBM quantum hardware using Qiskit.\n  \n  Quantum Computing\n  â†’\n\n\nOngoing Research\n\n\n\n  \n    Flight Delay Prediction â€” M.S. Thesis\n    In Progress\n  \n  \n    Selective prediction framework for American Airlines flight delays. Investigating when not to trust model predictions by defining abstention policies that bound acceptable risk.\n  \n  \n    \n      0.6863\n      AUC (No ETD)\n    \n    \n      0.9280\n      AUC (+ ETD)\n    \n    \n      0.2908\n      AURC\n    \n    \n      1.1M+\n      Flights\n    \n  \n  Data under NDA â€” results only, no raw data disclosed.\n\n\n\n  \n    Adaptive Learning Platform Analysis â€” Taichung Education Bureau\n    In Progress\n  \n  \n    Evaluating the effectiveness of an adaptive learning platform (â€œ\\u56e0\\u6750\\u7db2â€) on Grade 5 math performance across Taichung City. 50.4% of classes had zero platform usage; within-school adjusted analysis reveals a weak negative correlation, suggesting usage alone does not predict higher scores.\n  \n  \n    \n      127\n      Classes\n    \n    \n      2,414\n      Records\n    \n    \n      0.096\n      Ï Usage vs Outcome\n    \n    \n      âˆ’0.132\n      Ï Within-School\n    \n  \n  Data under NDA â€” results only, no raw data disclosed.\n\n\nExplore\n\n\n\n  \n    About\n    Who I am\n  \n  \n    CV\n    Experience & Skills\n  \n  \n    MS Â· DSIC\n    Data Science @ NCHU\n  \n  \n    MS Â· SDAR\n    Social Data @ UTD\n  \n  \n    BS Â· AM\n    Applied Math @ NCHU\n  \n\n\n\n  \n     Email\n    Â·\n     GitHub\n    Â·\n     Dallas, TX / Taichung, Taiwan\n  \n  Â© 2025 Tzu-Yuan Chen"
  },
  {
    "objectID": "DL.html",
    "href": "DL.html",
    "title": "Deep Learning",
    "section": "",
    "text": "Task / ä»»å‹™ï¼š Classify industrial component images into 6 defect categories using a fine-tuned ResNet-18, trained on the AOI (Automated Optical Inspection) dataset. ä½¿ç”¨ ResNet-18 å°å·¥æ¥­å…ƒä»¶å½±åƒé€²è¡Œ 6 é¡ç¼ºé™·åˆ†é¡ï¼ˆAOI è‡ªå‹•å…‰å­¸æª¢æ¸¬è³‡æ–™é›†ï¼‰ã€‚\nDataset / è³‡æ–™é›†ï¼š AOI Dataset â€” 2,530 training images, 10,144 test images, 6 classes (normal, void, horizontal defect, vertical defect, edge defect, particle)\nMethod / æ–¹æ³•ï¼š ResNet-18 (ImageNet pretrained) â€” frozen backbone, fine-tuned classifier head\n\n\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35}}}%%\nflowchart LR\n    A[\"Pretrained ResNet-18     \"] --&gt; B[\"Freeze Backbone     \"] --&gt; C[\"Replace fc â†’ 6 cls     \"] --&gt; D[\"Train Classifier     \"] --&gt; E[\"Defect Prediction     \"]\n\n    style A fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style B fill:#F5F5F5,color:#424242,stroke:#BDBDBD,stroke-width:2px\n    style C fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style D fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n    style E fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n\n\n\n\n\n\n\né·ç§»å­¸ç¿’ç­–ç•¥ï¼š å‡çµ ResNet-18 å…¨éƒ¨é è¨“ç·´åƒæ•¸ï¼Œåƒ…æ›¿æ›æœ€å¾Œä¸€å±¤ fc â†’ 6 è¼¸å‡ºï¼Œä»¥ Adam optimizer (lr=0.001) è¨“ç·´åˆ†é¡é ­ã€‚è³‡æ–™å‰è™•ç†ï¼š224x224 RGBï¼ŒImageNet mean/std æ­£è¦åŒ–ã€‚\n\n\n\n\nmodel = models.resnet18(pretrained=True)\n\n# Freeze all parameters\nfor param in model.parameters():\n    param.requires_grad = False\n\n# Replace final layer for 6-class output\nnum_ftrs = model.fc.in_features\nmodel.fc = nn.Linear(num_ftrs, 6)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)\n\n\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.fc.parameters(), lr=0.001)\nnum_epochs = 10\nbatch_size = 32\n# Input: 224x224 RGB, normalized to ImageNet mean/std\n\n\n\n\n\n\nEpoch\nTrain Loss\nVal Accuracy\n\n\n\n\n1\n0.8943\n95.26%\n\n\n2\n0.4654\n96.25%\n\n\n6\n0.2635\n96.44%\n\n\n10\n0.2381\n95.85%\n\n\n\nBest Validation Accuracy: 96.44%\n\n\n\n\n\n\n\n\n\n\nTraining Curve åˆ†æï¼š ç²‰è‰² Train Loss åœ¨å‰ 3 å€‹ epoch æ€¥é½ä¸‹é™ï¼Œä¹‹å¾Œè¶¨æ–¼å¹³ç©©ã€‚é’è‰² Val Accuracy åœ¨ Epoch 6 é”åˆ°æœ€é«˜ 96.44%ï¼ˆæ©˜è‰²è±å½¢ï¼‰ï¼Œä¹‹å¾Œå‡ºç¾è¼•å¾® overfittingï¼ˆaccuracy å¾®å¹…ä¸‹é™ï¼‰ã€‚æ©˜è‰²è™›ç·š æ¨™ç¤º best epoch ä½ç½®ã€‚\n\n\n\n\n\n\nTask / ä»»å‹™ï¼š Perform binary semantic segmentation of blood vessels in retinal fundus images using a custom U-Net architecture trained on the DRIVE dataset. ä½¿ç”¨è‡ªè£½ U-Net å° DRIVE è³‡æ–™é›†çš„çœ¼åº•å½±åƒé€²è¡Œè¦–ç¶²è†œè¡€ç®¡äºŒå…ƒèªæ„åˆ†å‰²ã€‚\nDataset / è³‡æ–™é›†ï¼š DRIVE (Digital Retinal Images for Vessel Extraction) â€” 22 training, 20 test images, 512x512\nMethod / æ–¹æ³•ï¼š U-Net (5-level encoder-decoder with skip connections) + Focal Tversky Loss\n\n\n\n\n\n\n\n\nSample 1 â€” fundus image\n\n\n\n\n\n\n\nSample 2 â€” different vessel pattern\n\n\n\n\n\n\n\nSample 3 â€” optic disc visible\n\n\n\n\n\n\n\n\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35}}}%%\nflowchart TD\n    IN[\"Input 1ch 512x512     \"] --&gt; E1[\"Conv1: 1â†’64     \"]\n    E1 --&gt;|Pool 2x| E2[\"Conv2: 64â†’128     \"]\n    E2 --&gt;|Pool 2x| E3[\"Conv3: 128â†’256     \"]\n    E3 --&gt;|Pool 2x| E4[\"Conv4: 256â†’512     \"]\n    E4 --&gt;|Pool 2x| BN[\"Bottleneck: 512â†’1024     \"]\n    BN --&gt; U1[\"Up: 1024â†’512     \"]\n    U1 --&gt;|+skip E4| U2[\"Up: 512â†’256     \"]\n    U2 --&gt;|+skip E3| U3[\"Up: 256â†’128     \"]\n    U3 --&gt;|+skip E2| U4[\"Up: 128â†’64     \"]\n    U4 --&gt;|+skip E1| OUT[\"Mask Output     \"]\n\n    E4 -.-&gt;|\"skip\"| U1\n    E3 -.-&gt;|\"skip\"| U2\n    E2 -.-&gt;|\"skip\"| U3\n    E1 -.-&gt;|\"skip\"| U4\n\n    style IN  fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style E1  fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style E2  fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style E3  fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style E4  fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style BN  fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style U1  fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n    style U2  fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n    style U3  fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n    style U4  fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n    style OUT fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n\n\n\n\n\n\n\nU-Net æ¶æ§‹è‰²ç¢¼ï¼š æ·ºè—è‰² = Encoderï¼ˆæ”¶ç¸®è·¯å¾‘ï¼‰ï¼Œæ·ºç°è‰² = MaxPool ä¸‹æ¡æ¨£ï¼Œæ·ºæ©˜è‰² = Bottleneckï¼ˆæœ€åº•å±¤ 512â†’1024ï¼‰ï¼Œæ·ºç¶ è‰² = Decoderï¼ˆæ“´å±•è·¯å¾‘ï¼‰+ skip connectionã€‚æ¯å±¤ skip connection æŠŠ encoder çš„ç©ºé–“ç´°ç¯€å‚³éçµ¦ decoderï¼Œä¿ç•™é«˜è§£æåº¦çš„è¡€ç®¡é‚Šç·£è³‡è¨Šã€‚\n\nclass UNet(torch.nn.Module):\n    def __init__(self, inchannel, outchannel):\n        super(UNet, self).__init__()\n        # Encoder\n        self.conv1 = Conv(inchannel, 64)\n        self.conv2 = Conv(64, 128)\n        self.conv3 = Conv(128, 256)\n        self.conv4 = Conv(256, 512)\n        self.conv5 = Conv(512, 1024)\n        self.pool  = torch.nn.MaxPool2d(2)\n        # Decoder\n        self.up1   = torch.nn.ConvTranspose2d(1024, 512, 2, 2)\n        self.conv6 = Conv(1024, 512)\n        self.up2   = torch.nn.ConvTranspose2d(512, 256, 2, 2)\n        self.conv7 = Conv(512, 256)\n        self.up3   = torch.nn.ConvTranspose2d(256, 128, 2, 2)\n        self.conv8 = Conv(256, 128)\n        self.up4   = torch.nn.ConvTranspose2d(128, 64, 2, 2)\n        self.conv9 = Conv(128, 64)\n        self.conv10 = torch.nn.Conv2d(64, outchannel, 3, 1, 1)\n\n\n\n# Focal Tversky Loss â€” handles class imbalance in vessel vs background\ncriterion = lambda y_pred, y_true: focal_tversky_loss(\n    y_pred, y_true, alpha=0.5, beta=0.5, gamma=0.75\n)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, mode='min', factor=0.5, patience=5\n)\ndevice = torch.device(\"mps\")  # Apple Silicon\nnum_epochs = 100\n\n\n\nEach row shows: Original fundus image â†’ Predicted segmentation mask â†’ Ground truth mask æ¯åˆ—ä¾åºç‚ºï¼šåŸå§‹çœ¼åº•å½±åƒ â†’ é æ¸¬åˆ†å‰²é®ç½© â†’ çœŸå¯¦æ¨™è¨˜é®ç½©\n\n\n\nSegmentation output â€” Original / Segmentation / Ground Truth (all 20 test images)\n\n\n\nåˆ†å‰²çµæœè§€å¯Ÿï¼š æ¨¡å‹æˆåŠŸè­˜åˆ¥ä¸»è¦è¡€ç®¡èµ°å‘èˆ‡åˆ†ä½ˆï¼Œä½†åœ¨å¾®è¡€ç®¡ï¼ˆfine capillariesï¼‰çš„è¾¨è­˜ä¸Šä»æœ‰æå‡ç©ºé–“ã€‚Ground truth ä¸­å¯è¦‹è¨±å¤šæ¥µç´°çš„æ¯›ç´°è¡€ç®¡ï¼Œæ¨¡å‹å‚¾å‘æ–¼åªé æ¸¬è¼ƒç²—çš„è¡€ç®¡çµæ§‹ã€‚\n\n\n\n\n\n\n\nMetric\nValue\n\n\n\n\nMean IoU (mIoU)\n0.3510\n\n\nTraining epochs\n100\n\n\nInput resolution\n512 x 512\n\n\n\n\n\n\n\n\n\n\n\n\n\nmIoU åˆ†æï¼š å·¦åœ–æ¯”è¼ƒä¸‰è€…ï¼šéš¨æ©Ÿ baselineï¼ˆ0.05ï¼‰ã€æœ¬æ¨¡å‹ U-Netï¼ˆ0.351ï¼‰ã€DRIVE è³‡æ–™é›† SOTAï¼ˆ~0.82ï¼‰ã€‚å³åœ–ä»¥ Venn æ•£é»ç¤ºæ„ IoU æ¦‚å¿µ â€” é’è‰² ç‚º Ground Truthã€ç²‰è‰² ç‚º Predictionï¼Œé‡ç–Šå€åŸŸå³ Intersectionã€‚mIoU 0.351 è¡¨ç¤ºé æ¸¬èˆ‡æ¨™è¨˜çš„é‡ç–Šç¨‹åº¦ç´„ 35%ï¼Œä»æœ‰æå‡ç©ºé–“ï¼ˆå¯å˜—è©¦æ›´æ·±ç¶²è·¯ã€æ›´å¤š data augmentationã€class-weighted lossï¼‰ã€‚\n\n\n\n\n\n\nTask / ä»»å‹™ï¼š Train a convolutional autoencoder to reconstruct retinal fundus images in an unsupervised manner, evaluated by Peak Signal-to-Noise Ratio (PSNR). ä»¥ç„¡ç›£ç£æ–¹å¼è¨“ç·´å·ç©è‡ªå‹•ç·¨ç¢¼å™¨é‡å»ºçœ¼åº•å½±åƒï¼Œä»¥ PSNR ä½œç‚ºè©•ä¼°æŒ‡æ¨™ã€‚\nDataset / è³‡æ–™é›†ï¼š DRIVE â€” 21 training, 20 test images, 512x512 RGB\nMethod / æ–¹æ³•ï¼š Convolutional Autoencoder (Encoder-Decoder with skip connections) + MSE Loss\n\n\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35}}}%%\nflowchart TD\n    IN[\"Input 3ch 512x512     \"] --&gt; C1[\"Conv1: 3â†’64     \"]\n    C1 --&gt;|Pool 2x| C2[\"Conv2: 64â†’128     \"]\n    C2 --&gt;|Pool 2x| C3[\"Conv3: 128â†’256     \"]\n    C3 --&gt;|Pool 2x| C4[\"Bottleneck: 256â†’512     \"]\n    C4 --&gt; U1[\"Up: 512â†’256     \"]\n    U1 --&gt;|+skip C3| U2[\"Up: 256â†’128     \"]\n    U2 --&gt;|+skip C2| U3[\"Up: 128â†’64     \"]\n    U3 --&gt;|+skip C1| OUT[\"Output: 64â†’3ch     \"]\n\n    C3 -.-&gt;|\"skip\"| U1\n    C2 -.-&gt;|\"skip\"| U2\n    C1 -.-&gt;|\"skip\"| U3\n\n    style IN  fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style C1  fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style C2  fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style C3  fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style C4  fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style U1  fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n    style U2  fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n    style U3  fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n    style OUT fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n\n\n\n\n\n\n\nAutoencoder æ¶æ§‹è‰²ç¢¼ï¼š æ·ºè—è‰² = Encoderï¼Œæ·ºæ©˜è‰² = Bottleneckï¼ˆ256â†’512 å£“ç¸®è¡¨ç¤ºï¼‰ï¼Œæ·ºç¶ è‰² = Decoder + skip connectionsã€‚èˆ‡ U-Net ç›¸åŒçš„ encoder-decoder çµæ§‹ï¼Œä½†ç›®æ¨™æ˜¯é‡å»ºè¼¸å…¥å½±åƒï¼ˆè‡ªç›£ç£å­¸ç¿’ï¼‰ï¼Œè€Œéåˆ†å‰²ã€‚\n\nclass AutoEncoder(nn.Module):\n    def __init__(self, inchannel=3, outchannel=3):\n        super(AutoEncoder, self).__init__()\n        # Encoder\n        self.conv1 = Conv(inchannel, 64)\n        self.conv2 = Conv(64, 128)\n        self.conv3 = Conv(128, 256)\n        self.conv4 = Conv(256, 512)\n        self.pool  = nn.MaxPool2d(2)\n        # Decoder (with skip connections)\n        self.up1   = nn.ConvTranspose2d(512, 256, 2, 2)\n        self.conv5 = Conv(512, 256)\n        self.up2   = nn.ConvTranspose2d(256, 128, 2, 2)\n        self.conv6 = Conv(256, 128)\n        self.up3   = nn.ConvTranspose2d(128, 64, 2, 2)\n        self.conv7 = Conv(128, 64)\n        self.conv8 = nn.Conv2d(64, outchannel, 3, 1, 1)\n\n\n\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\nnum_epochs = 20\nbatch_size = 1\n# Normalization: mean=0.5, std=0.5\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n\n\n\n\n\n\nEpoch\nTrain Loss\nTest Loss\nPSNR (dB)\n\n\n\n\n1\n0.0979\n0.1152\n16.29\n\n\n3\n0.0323\n0.0067\n27.98\n\n\n10\n0.0309\n0.0055\n29.06\n\n\n13\n0.0263\n0.0043\n30.16\n\n\n18\n0.0280\n0.0037\n30.84\n\n\n20\n0.0268\n0.0048\n29.50\n\n\n\nBest PSNR: 30.84 dB at Epoch 18\n\n\nWarning in annotate(\"label\", x = best_ep - 1, y = 32, label = paste0(\"Best: \",\n: Ignoring unknown parameters: `label.size`\n\n\n\n\n\n\n\n\n\n\nTraining Curve åˆ†æï¼š å·¦åœ– â€” ç²‰è‰² Train Loss èˆ‡ é’è‰²è™›ç·š Test Loss éƒ½åœ¨å‰ 3 epoch æ€¥é€Ÿä¸‹é™ï¼Œä¹‹å¾Œè¶¨æ–¼å¹³ç©©ã€‚å³åœ– â€” PSNR åœ¨ Epoch 18 é”åˆ°å³°å€¼ 30.84 dBï¼ˆè¶…é 30 dB é–€æª»ï¼Œç°è‰²è™›ç·šï¼‰ï¼Œä¹‹å¾Œå¾®å¹…ä¸‹é™ï¼ˆEpoch 20 ç‚º 29.50 dBï¼‰ï¼Œé¡¯ç¤º Epoch 18 ç‚ºæœ€ä½³åœæ­¢é»ã€‚\n\n\n\n\n\n\nTask / ä»»å‹™ï¼š Train a conditional GAN to generate Western blot images from two template images, learning the mapping from template patterns to realistic blot patterns. è¨“ç·´æ¢ä»¶å¼ GANï¼Œå¾å…©å¼µæ¨¡æ¿å½±åƒç”Ÿæˆ Western blot å½±åƒï¼Œå­¸ç¿’æ¨¡æ¿åœ–æ¡ˆåˆ°çœŸå¯¦æ¢å¸¶ç´‹è·¯çš„æ˜ å°„ã€‚\nDataset / è³‡æ–™é›†ï¼š Western Blot Dataset â€” 402 template pairs + 402 target images, 64x64 grayscale\nMethod / æ–¹æ³•ï¼š Conditional GAN â€” Encoder-Decoder Generator + PatchGAN-style Discriminator\n\n\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35}}}%%\nflowchart TD\n    T1[\"Template 1: 64x64     \"] --&gt; CAT[\"Concat 2ch     \"]\n    T2[\"Template 2: 64x64     \"] --&gt; CAT\n    CAT --&gt; G[\"Generator     \"]\n    G --&gt; FAKE[\"Generated Image     \"]\n    REAL[\"Real Image     \"] --&gt; D[\"Discriminator     \"]\n    FAKE --&gt; D\n    D --&gt;|G loss| UG[\"Update G     \"]\n    D --&gt;|D loss| UD[\"Update D     \"]\n\n    style T1   fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style T2   fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style CAT  fill:#F5F5F5,color:#424242,stroke:#BDBDBD,stroke-width:2px\n    style G    fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style FAKE fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style REAL fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n    style D    fill:#FCE4EC,color:#C62828,stroke:#F48FB1,stroke-width:2px\n    style UG   fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style UD   fill:#FCE4EC,color:#C62828,stroke:#F48FB1,stroke-width:2px\n\n\n\n\n\n\n\nGAN è¨“ç·´æµç¨‹ï¼š å…©å¼µ template åœ– concat æˆ 2-channel è¼¸å…¥ï¼Œç¶“é è—è‰² Generator ç”Ÿæˆå‡ blot å½±åƒï¼ˆæ©˜è‰²ï¼‰ã€‚ç²‰ç´…è‰² Discriminator åˆ¤æ–·è¼¸å…¥æ˜¯ çœŸï¼ˆç¶ è‰²ï¼‰ é‚„æ˜¯ å‡ï¼ˆæ©˜è‰²ï¼‰ï¼Œä¸¦åˆ†åˆ¥å›å‚³ G loss / D loss æ›´æ–°å„è‡ªçš„åƒæ•¸ã€‚\n\n\n\n\nclass TemplateToImageGenerator(nn.Module):\n    def __init__(self):\n        super(TemplateToImageGenerator, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Conv2d(1, 64, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(64), nn.ReLU(),\n            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(128), nn.ReLU(),\n            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(256), nn.ReLU(),\n        )\n        self.decoder = nn.Sequential(\n            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(128), nn.ReLU(),\n            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(64), nn.ReLU(),\n            nn.ConvTranspose2d(64, 1, kernel_size=4, stride=2, padding=1),\n            nn.Tanh(),\n        )\n\n\n\nclass TemplateToImageDiscriminator(nn.Module):\n    def __init__(self):\n        super(TemplateToImageDiscriminator, self).__init__()\n        self.model = nn.Sequential(\n            nn.Conv2d(2, 64, kernel_size=4, stride=2, padding=1),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(128), nn.LeakyReLU(0.2),\n            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(256), nn.LeakyReLU(0.2),\n            nn.AdaptiveAvgPool2d((1, 1)),\n            nn.Flatten(),\n            nn.Linear(256, 1),\n            nn.Sigmoid()\n        )\n\n\n\ng_optimizer = optim.Adam(generator.parameters(),     lr=0.0002)\nd_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002)\ncriterion   = nn.BCELoss()\nnum_epochs  = 200    # trained on CPU, stopped at epoch 118\nbatch_size  = 1\n\n\n\nTraining ran on CPU and was recorded up to epoch 118/200. By that point the Discriminator had begun to dominate (D Loss &lt; 0.1 in some steps), causing G Loss to climb â€” a classic sign the generator needs more capacity or learning rate balancing. è¨“ç·´åœ¨ CPU ä¸Šé€²è¡Œï¼Œè¨˜éŒ„è‡³ç¬¬ 118 å€‹ epochã€‚æ­¤æ™‚åˆ¤åˆ¥å™¨é–‹å§‹ä¸»å°è¨“ç·´ï¼ˆD Loss ä½è‡³ 0.03ï¼‰ï¼Œå°è‡´ G Loss æ”€å‡ï¼Œç‚ºå…¸å‹çš„åˆ¤åˆ¥å™¨éå¼·å•é¡Œã€‚\n\n\n\nEpoch\nD Loss (sample)\nG Loss (sample)\n\n\n\n\n1 / step 10\n1.3715\n0.7412\n\n\n1 / step 40\n1.3699\n0.6840\n\n\n118 / step 200\n0.4921\n2.2424\n\n\n118 / step 230\n0.0263\n4.2039\n\n\n118 / step 270\n0.0683\n3.5220\n\n\n\n\n\n\n\n\n\n\n\n\n\nGAN è¨“ç·´å‹•æ…‹åˆ†æï¼š ç°è‰²è™›ç·š ln(2)=0.693 ç‚º GAN ç†æƒ³å‡è¡¡é»ï¼ˆD åˆ†ä¸å‡ºçœŸå‡æ™‚çš„ BCE lossï¼‰ã€‚å·¦åœ–åŸå§‹ log å¯è¦‹æ—©æœŸ D Loss æ¥è¿‘ ln(2)ï¼ˆD/G æ¥è¿‘å‡è¡¡ï¼‰ï¼Œå¾ŒæœŸ ç²‰è‰² D Loss å¿«é€Ÿä¸‹é™è‡³æ¥è¿‘ 0ï¼Œé’è‰² G Loss æ”€å‡è‡³ 3-4ï¼Œè¡¨ç¤º Discriminator éå¼·ï¼ˆD èƒ½è¼•é¬†åˆ†è¾¨çœŸå‡ï¼‰ã€‚å³åœ– per-epoch å¹³å‡è¶¨å‹¢æ›´æ¸…æ¥šå‘ˆç¾æ­¤åˆ†æ­§ã€‚å¯è€ƒæ…®é™ä½ D çš„å­¸ç¿’ç‡ã€å¢åŠ  G çš„å®¹é‡ã€æˆ–åŠ å…¥ label smoothing ä¾†ç·©è§£ã€‚"
  },
  {
    "objectID": "DL.html#assignments-ä½œæ¥­",
    "href": "DL.html#assignments-ä½œæ¥­",
    "title": "Deep Learning",
    "section": "",
    "text": "Task / ä»»å‹™ï¼š Classify industrial component images into 6 defect categories using a fine-tuned ResNet-18, trained on the AOI (Automated Optical Inspection) dataset. ä½¿ç”¨ ResNet-18 å°å·¥æ¥­å…ƒä»¶å½±åƒé€²è¡Œ 6 é¡ç¼ºé™·åˆ†é¡ï¼ˆAOI è‡ªå‹•å…‰å­¸æª¢æ¸¬è³‡æ–™é›†ï¼‰ã€‚\nDataset / è³‡æ–™é›†ï¼š AOI Dataset â€” 2,530 training images, 10,144 test images, 6 classes (normal, void, horizontal defect, vertical defect, edge defect, particle)\nMethod / æ–¹æ³•ï¼š ResNet-18 (ImageNet pretrained) â€” frozen backbone, fine-tuned classifier head\n\n\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35}}}%%\nflowchart LR\n    A[\"Pretrained ResNet-18     \"] --&gt; B[\"Freeze Backbone     \"] --&gt; C[\"Replace fc â†’ 6 cls     \"] --&gt; D[\"Train Classifier     \"] --&gt; E[\"Defect Prediction     \"]\n\n    style A fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style B fill:#F5F5F5,color:#424242,stroke:#BDBDBD,stroke-width:2px\n    style C fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style D fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n    style E fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n\n\n\n\n\n\n\né·ç§»å­¸ç¿’ç­–ç•¥ï¼š å‡çµ ResNet-18 å…¨éƒ¨é è¨“ç·´åƒæ•¸ï¼Œåƒ…æ›¿æ›æœ€å¾Œä¸€å±¤ fc â†’ 6 è¼¸å‡ºï¼Œä»¥ Adam optimizer (lr=0.001) è¨“ç·´åˆ†é¡é ­ã€‚è³‡æ–™å‰è™•ç†ï¼š224x224 RGBï¼ŒImageNet mean/std æ­£è¦åŒ–ã€‚\n\n\n\n\nmodel = models.resnet18(pretrained=True)\n\n# Freeze all parameters\nfor param in model.parameters():\n    param.requires_grad = False\n\n# Replace final layer for 6-class output\nnum_ftrs = model.fc.in_features\nmodel.fc = nn.Linear(num_ftrs, 6)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)\n\n\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.fc.parameters(), lr=0.001)\nnum_epochs = 10\nbatch_size = 32\n# Input: 224x224 RGB, normalized to ImageNet mean/std\n\n\n\n\n\n\nEpoch\nTrain Loss\nVal Accuracy\n\n\n\n\n1\n0.8943\n95.26%\n\n\n2\n0.4654\n96.25%\n\n\n6\n0.2635\n96.44%\n\n\n10\n0.2381\n95.85%\n\n\n\nBest Validation Accuracy: 96.44%\n\n\n\n\n\n\n\n\n\n\nTraining Curve åˆ†æï¼š ç²‰è‰² Train Loss åœ¨å‰ 3 å€‹ epoch æ€¥é½ä¸‹é™ï¼Œä¹‹å¾Œè¶¨æ–¼å¹³ç©©ã€‚é’è‰² Val Accuracy åœ¨ Epoch 6 é”åˆ°æœ€é«˜ 96.44%ï¼ˆæ©˜è‰²è±å½¢ï¼‰ï¼Œä¹‹å¾Œå‡ºç¾è¼•å¾® overfittingï¼ˆaccuracy å¾®å¹…ä¸‹é™ï¼‰ã€‚æ©˜è‰²è™›ç·š æ¨™ç¤º best epoch ä½ç½®ã€‚\n\n\n\n\n\n\nTask / ä»»å‹™ï¼š Perform binary semantic segmentation of blood vessels in retinal fundus images using a custom U-Net architecture trained on the DRIVE dataset. ä½¿ç”¨è‡ªè£½ U-Net å° DRIVE è³‡æ–™é›†çš„çœ¼åº•å½±åƒé€²è¡Œè¦–ç¶²è†œè¡€ç®¡äºŒå…ƒèªæ„åˆ†å‰²ã€‚\nDataset / è³‡æ–™é›†ï¼š DRIVE (Digital Retinal Images for Vessel Extraction) â€” 22 training, 20 test images, 512x512\nMethod / æ–¹æ³•ï¼š U-Net (5-level encoder-decoder with skip connections) + Focal Tversky Loss\n\n\n\n\n\n\n\n\nSample 1 â€” fundus image\n\n\n\n\n\n\n\nSample 2 â€” different vessel pattern\n\n\n\n\n\n\n\nSample 3 â€” optic disc visible\n\n\n\n\n\n\n\n\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35}}}%%\nflowchart TD\n    IN[\"Input 1ch 512x512     \"] --&gt; E1[\"Conv1: 1â†’64     \"]\n    E1 --&gt;|Pool 2x| E2[\"Conv2: 64â†’128     \"]\n    E2 --&gt;|Pool 2x| E3[\"Conv3: 128â†’256     \"]\n    E3 --&gt;|Pool 2x| E4[\"Conv4: 256â†’512     \"]\n    E4 --&gt;|Pool 2x| BN[\"Bottleneck: 512â†’1024     \"]\n    BN --&gt; U1[\"Up: 1024â†’512     \"]\n    U1 --&gt;|+skip E4| U2[\"Up: 512â†’256     \"]\n    U2 --&gt;|+skip E3| U3[\"Up: 256â†’128     \"]\n    U3 --&gt;|+skip E2| U4[\"Up: 128â†’64     \"]\n    U4 --&gt;|+skip E1| OUT[\"Mask Output     \"]\n\n    E4 -.-&gt;|\"skip\"| U1\n    E3 -.-&gt;|\"skip\"| U2\n    E2 -.-&gt;|\"skip\"| U3\n    E1 -.-&gt;|\"skip\"| U4\n\n    style IN  fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style E1  fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style E2  fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style E3  fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style E4  fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style BN  fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style U1  fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n    style U2  fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n    style U3  fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n    style U4  fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n    style OUT fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n\n\n\n\n\n\n\nU-Net æ¶æ§‹è‰²ç¢¼ï¼š æ·ºè—è‰² = Encoderï¼ˆæ”¶ç¸®è·¯å¾‘ï¼‰ï¼Œæ·ºç°è‰² = MaxPool ä¸‹æ¡æ¨£ï¼Œæ·ºæ©˜è‰² = Bottleneckï¼ˆæœ€åº•å±¤ 512â†’1024ï¼‰ï¼Œæ·ºç¶ è‰² = Decoderï¼ˆæ“´å±•è·¯å¾‘ï¼‰+ skip connectionã€‚æ¯å±¤ skip connection æŠŠ encoder çš„ç©ºé–“ç´°ç¯€å‚³éçµ¦ decoderï¼Œä¿ç•™é«˜è§£æåº¦çš„è¡€ç®¡é‚Šç·£è³‡è¨Šã€‚\n\nclass UNet(torch.nn.Module):\n    def __init__(self, inchannel, outchannel):\n        super(UNet, self).__init__()\n        # Encoder\n        self.conv1 = Conv(inchannel, 64)\n        self.conv2 = Conv(64, 128)\n        self.conv3 = Conv(128, 256)\n        self.conv4 = Conv(256, 512)\n        self.conv5 = Conv(512, 1024)\n        self.pool  = torch.nn.MaxPool2d(2)\n        # Decoder\n        self.up1   = torch.nn.ConvTranspose2d(1024, 512, 2, 2)\n        self.conv6 = Conv(1024, 512)\n        self.up2   = torch.nn.ConvTranspose2d(512, 256, 2, 2)\n        self.conv7 = Conv(512, 256)\n        self.up3   = torch.nn.ConvTranspose2d(256, 128, 2, 2)\n        self.conv8 = Conv(256, 128)\n        self.up4   = torch.nn.ConvTranspose2d(128, 64, 2, 2)\n        self.conv9 = Conv(128, 64)\n        self.conv10 = torch.nn.Conv2d(64, outchannel, 3, 1, 1)\n\n\n\n# Focal Tversky Loss â€” handles class imbalance in vessel vs background\ncriterion = lambda y_pred, y_true: focal_tversky_loss(\n    y_pred, y_true, alpha=0.5, beta=0.5, gamma=0.75\n)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, mode='min', factor=0.5, patience=5\n)\ndevice = torch.device(\"mps\")  # Apple Silicon\nnum_epochs = 100\n\n\n\nEach row shows: Original fundus image â†’ Predicted segmentation mask â†’ Ground truth mask æ¯åˆ—ä¾åºç‚ºï¼šåŸå§‹çœ¼åº•å½±åƒ â†’ é æ¸¬åˆ†å‰²é®ç½© â†’ çœŸå¯¦æ¨™è¨˜é®ç½©\n\n\n\nSegmentation output â€” Original / Segmentation / Ground Truth (all 20 test images)\n\n\n\nåˆ†å‰²çµæœè§€å¯Ÿï¼š æ¨¡å‹æˆåŠŸè­˜åˆ¥ä¸»è¦è¡€ç®¡èµ°å‘èˆ‡åˆ†ä½ˆï¼Œä½†åœ¨å¾®è¡€ç®¡ï¼ˆfine capillariesï¼‰çš„è¾¨è­˜ä¸Šä»æœ‰æå‡ç©ºé–“ã€‚Ground truth ä¸­å¯è¦‹è¨±å¤šæ¥µç´°çš„æ¯›ç´°è¡€ç®¡ï¼Œæ¨¡å‹å‚¾å‘æ–¼åªé æ¸¬è¼ƒç²—çš„è¡€ç®¡çµæ§‹ã€‚\n\n\n\n\n\n\n\nMetric\nValue\n\n\n\n\nMean IoU (mIoU)\n0.3510\n\n\nTraining epochs\n100\n\n\nInput resolution\n512 x 512\n\n\n\n\n\n\n\n\n\n\n\n\n\nmIoU åˆ†æï¼š å·¦åœ–æ¯”è¼ƒä¸‰è€…ï¼šéš¨æ©Ÿ baselineï¼ˆ0.05ï¼‰ã€æœ¬æ¨¡å‹ U-Netï¼ˆ0.351ï¼‰ã€DRIVE è³‡æ–™é›† SOTAï¼ˆ~0.82ï¼‰ã€‚å³åœ–ä»¥ Venn æ•£é»ç¤ºæ„ IoU æ¦‚å¿µ â€” é’è‰² ç‚º Ground Truthã€ç²‰è‰² ç‚º Predictionï¼Œé‡ç–Šå€åŸŸå³ Intersectionã€‚mIoU 0.351 è¡¨ç¤ºé æ¸¬èˆ‡æ¨™è¨˜çš„é‡ç–Šç¨‹åº¦ç´„ 35%ï¼Œä»æœ‰æå‡ç©ºé–“ï¼ˆå¯å˜—è©¦æ›´æ·±ç¶²è·¯ã€æ›´å¤š data augmentationã€class-weighted lossï¼‰ã€‚\n\n\n\n\n\n\nTask / ä»»å‹™ï¼š Train a convolutional autoencoder to reconstruct retinal fundus images in an unsupervised manner, evaluated by Peak Signal-to-Noise Ratio (PSNR). ä»¥ç„¡ç›£ç£æ–¹å¼è¨“ç·´å·ç©è‡ªå‹•ç·¨ç¢¼å™¨é‡å»ºçœ¼åº•å½±åƒï¼Œä»¥ PSNR ä½œç‚ºè©•ä¼°æŒ‡æ¨™ã€‚\nDataset / è³‡æ–™é›†ï¼š DRIVE â€” 21 training, 20 test images, 512x512 RGB\nMethod / æ–¹æ³•ï¼š Convolutional Autoencoder (Encoder-Decoder with skip connections) + MSE Loss\n\n\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35}}}%%\nflowchart TD\n    IN[\"Input 3ch 512x512     \"] --&gt; C1[\"Conv1: 3â†’64     \"]\n    C1 --&gt;|Pool 2x| C2[\"Conv2: 64â†’128     \"]\n    C2 --&gt;|Pool 2x| C3[\"Conv3: 128â†’256     \"]\n    C3 --&gt;|Pool 2x| C4[\"Bottleneck: 256â†’512     \"]\n    C4 --&gt; U1[\"Up: 512â†’256     \"]\n    U1 --&gt;|+skip C3| U2[\"Up: 256â†’128     \"]\n    U2 --&gt;|+skip C2| U3[\"Up: 128â†’64     \"]\n    U3 --&gt;|+skip C1| OUT[\"Output: 64â†’3ch     \"]\n\n    C3 -.-&gt;|\"skip\"| U1\n    C2 -.-&gt;|\"skip\"| U2\n    C1 -.-&gt;|\"skip\"| U3\n\n    style IN  fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style C1  fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style C2  fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style C3  fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style C4  fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style U1  fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n    style U2  fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n    style U3  fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n    style OUT fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n\n\n\n\n\n\n\nAutoencoder æ¶æ§‹è‰²ç¢¼ï¼š æ·ºè—è‰² = Encoderï¼Œæ·ºæ©˜è‰² = Bottleneckï¼ˆ256â†’512 å£“ç¸®è¡¨ç¤ºï¼‰ï¼Œæ·ºç¶ è‰² = Decoder + skip connectionsã€‚èˆ‡ U-Net ç›¸åŒçš„ encoder-decoder çµæ§‹ï¼Œä½†ç›®æ¨™æ˜¯é‡å»ºè¼¸å…¥å½±åƒï¼ˆè‡ªç›£ç£å­¸ç¿’ï¼‰ï¼Œè€Œéåˆ†å‰²ã€‚\n\nclass AutoEncoder(nn.Module):\n    def __init__(self, inchannel=3, outchannel=3):\n        super(AutoEncoder, self).__init__()\n        # Encoder\n        self.conv1 = Conv(inchannel, 64)\n        self.conv2 = Conv(64, 128)\n        self.conv3 = Conv(128, 256)\n        self.conv4 = Conv(256, 512)\n        self.pool  = nn.MaxPool2d(2)\n        # Decoder (with skip connections)\n        self.up1   = nn.ConvTranspose2d(512, 256, 2, 2)\n        self.conv5 = Conv(512, 256)\n        self.up2   = nn.ConvTranspose2d(256, 128, 2, 2)\n        self.conv6 = Conv(256, 128)\n        self.up3   = nn.ConvTranspose2d(128, 64, 2, 2)\n        self.conv7 = Conv(128, 64)\n        self.conv8 = nn.Conv2d(64, outchannel, 3, 1, 1)\n\n\n\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\nnum_epochs = 20\nbatch_size = 1\n# Normalization: mean=0.5, std=0.5\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n\n\n\n\n\n\nEpoch\nTrain Loss\nTest Loss\nPSNR (dB)\n\n\n\n\n1\n0.0979\n0.1152\n16.29\n\n\n3\n0.0323\n0.0067\n27.98\n\n\n10\n0.0309\n0.0055\n29.06\n\n\n13\n0.0263\n0.0043\n30.16\n\n\n18\n0.0280\n0.0037\n30.84\n\n\n20\n0.0268\n0.0048\n29.50\n\n\n\nBest PSNR: 30.84 dB at Epoch 18\n\n\nWarning in annotate(\"label\", x = best_ep - 1, y = 32, label = paste0(\"Best: \",\n: Ignoring unknown parameters: `label.size`\n\n\n\n\n\n\n\n\n\n\nTraining Curve åˆ†æï¼š å·¦åœ– â€” ç²‰è‰² Train Loss èˆ‡ é’è‰²è™›ç·š Test Loss éƒ½åœ¨å‰ 3 epoch æ€¥é€Ÿä¸‹é™ï¼Œä¹‹å¾Œè¶¨æ–¼å¹³ç©©ã€‚å³åœ– â€” PSNR åœ¨ Epoch 18 é”åˆ°å³°å€¼ 30.84 dBï¼ˆè¶…é 30 dB é–€æª»ï¼Œç°è‰²è™›ç·šï¼‰ï¼Œä¹‹å¾Œå¾®å¹…ä¸‹é™ï¼ˆEpoch 20 ç‚º 29.50 dBï¼‰ï¼Œé¡¯ç¤º Epoch 18 ç‚ºæœ€ä½³åœæ­¢é»ã€‚\n\n\n\n\n\n\nTask / ä»»å‹™ï¼š Train a conditional GAN to generate Western blot images from two template images, learning the mapping from template patterns to realistic blot patterns. è¨“ç·´æ¢ä»¶å¼ GANï¼Œå¾å…©å¼µæ¨¡æ¿å½±åƒç”Ÿæˆ Western blot å½±åƒï¼Œå­¸ç¿’æ¨¡æ¿åœ–æ¡ˆåˆ°çœŸå¯¦æ¢å¸¶ç´‹è·¯çš„æ˜ å°„ã€‚\nDataset / è³‡æ–™é›†ï¼š Western Blot Dataset â€” 402 template pairs + 402 target images, 64x64 grayscale\nMethod / æ–¹æ³•ï¼š Conditional GAN â€” Encoder-Decoder Generator + PatchGAN-style Discriminator\n\n\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35}}}%%\nflowchart TD\n    T1[\"Template 1: 64x64     \"] --&gt; CAT[\"Concat 2ch     \"]\n    T2[\"Template 2: 64x64     \"] --&gt; CAT\n    CAT --&gt; G[\"Generator     \"]\n    G --&gt; FAKE[\"Generated Image     \"]\n    REAL[\"Real Image     \"] --&gt; D[\"Discriminator     \"]\n    FAKE --&gt; D\n    D --&gt;|G loss| UG[\"Update G     \"]\n    D --&gt;|D loss| UD[\"Update D     \"]\n\n    style T1   fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style T2   fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style CAT  fill:#F5F5F5,color:#424242,stroke:#BDBDBD,stroke-width:2px\n    style G    fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style FAKE fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style REAL fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n    style D    fill:#FCE4EC,color:#C62828,stroke:#F48FB1,stroke-width:2px\n    style UG   fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style UD   fill:#FCE4EC,color:#C62828,stroke:#F48FB1,stroke-width:2px\n\n\n\n\n\n\n\nGAN è¨“ç·´æµç¨‹ï¼š å…©å¼µ template åœ– concat æˆ 2-channel è¼¸å…¥ï¼Œç¶“é è—è‰² Generator ç”Ÿæˆå‡ blot å½±åƒï¼ˆæ©˜è‰²ï¼‰ã€‚ç²‰ç´…è‰² Discriminator åˆ¤æ–·è¼¸å…¥æ˜¯ çœŸï¼ˆç¶ è‰²ï¼‰ é‚„æ˜¯ å‡ï¼ˆæ©˜è‰²ï¼‰ï¼Œä¸¦åˆ†åˆ¥å›å‚³ G loss / D loss æ›´æ–°å„è‡ªçš„åƒæ•¸ã€‚\n\n\n\n\nclass TemplateToImageGenerator(nn.Module):\n    def __init__(self):\n        super(TemplateToImageGenerator, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Conv2d(1, 64, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(64), nn.ReLU(),\n            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(128), nn.ReLU(),\n            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(256), nn.ReLU(),\n        )\n        self.decoder = nn.Sequential(\n            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(128), nn.ReLU(),\n            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(64), nn.ReLU(),\n            nn.ConvTranspose2d(64, 1, kernel_size=4, stride=2, padding=1),\n            nn.Tanh(),\n        )\n\n\n\nclass TemplateToImageDiscriminator(nn.Module):\n    def __init__(self):\n        super(TemplateToImageDiscriminator, self).__init__()\n        self.model = nn.Sequential(\n            nn.Conv2d(2, 64, kernel_size=4, stride=2, padding=1),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(128), nn.LeakyReLU(0.2),\n            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(256), nn.LeakyReLU(0.2),\n            nn.AdaptiveAvgPool2d((1, 1)),\n            nn.Flatten(),\n            nn.Linear(256, 1),\n            nn.Sigmoid()\n        )\n\n\n\ng_optimizer = optim.Adam(generator.parameters(),     lr=0.0002)\nd_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002)\ncriterion   = nn.BCELoss()\nnum_epochs  = 200    # trained on CPU, stopped at epoch 118\nbatch_size  = 1\n\n\n\nTraining ran on CPU and was recorded up to epoch 118/200. By that point the Discriminator had begun to dominate (D Loss &lt; 0.1 in some steps), causing G Loss to climb â€” a classic sign the generator needs more capacity or learning rate balancing. è¨“ç·´åœ¨ CPU ä¸Šé€²è¡Œï¼Œè¨˜éŒ„è‡³ç¬¬ 118 å€‹ epochã€‚æ­¤æ™‚åˆ¤åˆ¥å™¨é–‹å§‹ä¸»å°è¨“ç·´ï¼ˆD Loss ä½è‡³ 0.03ï¼‰ï¼Œå°è‡´ G Loss æ”€å‡ï¼Œç‚ºå…¸å‹çš„åˆ¤åˆ¥å™¨éå¼·å•é¡Œã€‚\n\n\n\nEpoch\nD Loss (sample)\nG Loss (sample)\n\n\n\n\n1 / step 10\n1.3715\n0.7412\n\n\n1 / step 40\n1.3699\n0.6840\n\n\n118 / step 200\n0.4921\n2.2424\n\n\n118 / step 230\n0.0263\n4.2039\n\n\n118 / step 270\n0.0683\n3.5220\n\n\n\n\n\n\n\n\n\n\n\n\n\nGAN è¨“ç·´å‹•æ…‹åˆ†æï¼š ç°è‰²è™›ç·š ln(2)=0.693 ç‚º GAN ç†æƒ³å‡è¡¡é»ï¼ˆD åˆ†ä¸å‡ºçœŸå‡æ™‚çš„ BCE lossï¼‰ã€‚å·¦åœ–åŸå§‹ log å¯è¦‹æ—©æœŸ D Loss æ¥è¿‘ ln(2)ï¼ˆD/G æ¥è¿‘å‡è¡¡ï¼‰ï¼Œå¾ŒæœŸ ç²‰è‰² D Loss å¿«é€Ÿä¸‹é™è‡³æ¥è¿‘ 0ï¼Œé’è‰² G Loss æ”€å‡è‡³ 3-4ï¼Œè¡¨ç¤º Discriminator éå¼·ï¼ˆD èƒ½è¼•é¬†åˆ†è¾¨çœŸå‡ï¼‰ã€‚å³åœ– per-epoch å¹³å‡è¶¨å‹¢æ›´æ¸…æ¥šå‘ˆç¾æ­¤åˆ†æ­§ã€‚å¯è€ƒæ…®é™ä½ D çš„å­¸ç¿’ç‡ã€å¢åŠ  G çš„å®¹é‡ã€æˆ–åŠ å…¥ label smoothing ä¾†ç·©è§£ã€‚"
  },
  {
    "objectID": "EPPS6356.html",
    "href": "EPPS6356.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Assignment 1\n\n\n\nAssignment 2\n\n\n\nAssignment 3\n\n\n\n\ndata(iris)\n\nlibrary(tidyverse)\n\nâ”€â”€ Attaching core tidyverse packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 2.0.0 â”€â”€\nâœ” dplyr     1.1.4     âœ” readr     2.1.5\nâœ” forcats   1.0.1     âœ” stringr   1.5.2\nâœ” ggplot2   4.0.0     âœ” tibble    3.3.0\nâœ” lubridate 1.9.4     âœ” tidyr     1.3.1\nâœ” purrr     1.1.0     \nâ”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€\nâœ– dplyr::filter() masks stats::filter()\nâœ– dplyr::lag()    masks stats::lag()\nâ„¹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# 1. Divide the dataset into three rectangles based on species.\n# The average of Petal.Length and Petal.Width is the length and width.\n# Draw three rectangles arranged horizontally.\n\n#1\n\nplot_data &lt;- iris %&gt;%\n  mutate(\n    sepal_length_group = cut(\n      Sepal.Length,\n      breaks = c(4, 5.5, 7.0, 8.0),\n      labels = c(\"Small (4.0-5.5)\", \"Medium (5.6-7.0)\", \"Large (7.1-8.0)\"),\n      include.lowest = TRUE\n    )\n  ) %&gt;%\n  group_by(sepal_length_group) %&gt;%\n  summarise(\n    count = n(),\n    avg_petal_length = mean(Petal.Length)\n  ) %&gt;%\n  mutate(\n    xmax = cumsum(count),\n    xmin = xmax - count,\n    x_label_pos = (xmin + xmax) / 2\n  )\n\nggplot(plot_data, aes(ymin = 0)) +\n  geom_rect(\n    aes(\n      xmin = xmin,\n      xmax = xmax,\n      ymax = avg_petal_length,\n      fill = sepal_length_group\n    ),\n    color = \"white\"\n  ) +\n  scale_x_continuous(\n    breaks = plot_data$x_label_pos,\n    labels = plot_data$sepal_length_group,\n    expand = c(0, 0)\n  ) +\n  scale_fill_viridis_d(option = \"D\", direction = -1) +\n  labs(\n    title = \"Average Petal Length by Sepal Length Group\",\n    subtitle = \"Column width is proportional to the number of flowers in each group\",\n    x = \"Count of Flowers in Group\",\n    y = \"Average Petal Length (cm)\",\n    fill = \"Sepal Length Group\"\n  ) +\n  # Apply a clean theme\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 18),\n    legend.position = \"bottom\",\n    panel.grid.major.x = element_blank(), # Remove vertical grid lines\n    panel.grid.minor.x = element_blank()\n  )\n\n\n\n\n\n\n\n\n\n# 2. table with embedded charts\niris_long &lt;- iris %&gt;%\n  pivot_longer(cols = -Species, names_to = \"Measurement\", values_to = \"Value\")\n\nggplot(iris_long, aes(x = Value, fill = Species)) +\n  geom_histogram(color = \"white\", bins = 15) +\n  facet_grid(Species ~ Measurement, scales = \"free\") +\n  scale_fill_manual( #coloring each species\n    values = c(\n      \"setosa\" = \"steelblue\", \n      \"versicolor\" = \"orange\",   \n      \"virginica\" = \"seagreen\"     \n    ) \n    ) + #labels\n      labs(\n        title = \"Distribution of Iris Measurements by Species\",\n        x = \"Measurement Value (cm)\",\n        y = \"Count\"\n      ) +\n  theme_bw() +\n    theme(\n      plot.title = element_text(hjust = 0.5, face = \"bold\"),\n      strip.text.x = element_text(face = \"bold\"),\n      strip.text.y = element_text(face = \"bold\"),\n      panel.border = element_rect(color = \"grey80\", fill = NA),\n      legend.position = \"bottom\"\n    )\n\n\n\n\n\n\n\n\n\n# 3. Extract setona and versicolor from species.\n# Then create df_2 and df_3. Draw a bar plot using petal.width: p1 p2.\n# Finally, use gridExtra to combine the plots.'\nlibrary(\"gridExtra\")\n\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\ndf_2 &lt;- subset(iris, Species %in% \"setosa\")\ndf_3 &lt;- subset(iris, Species %in% \"versicolor\")\ndf_2$id &lt;- 1:nrow(df_2)\ndf_3$id &lt;- 1:nrow(df_3)\n\n\n\np1 = ggplot(df_2, aes(x = factor(id), y = Petal.Width)) +\n  geom_bar(stat = \"identity\", fill = 'red', color = \"black\") +\n  coord_flip() +\n  labs(title = \"setosa\") +\n  theme(\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank() #this was by GPT\n  )\n\n\np2 = ggplot(df_3, aes(x = factor(id), y = Petal.Width)) +\n  geom_bar(stat = \"identity\", fill = \"blue\", color = \"black\") +\n  coord_flip() +\n  labs(title = \"versicolor\")+\n  theme(\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank() #this was by GPT\n  )\n\n\n\ngridExtra::grid.arrange(p1, p2, ncol = 2)\n\n\n\n\n\n\n\n\n\n# 4 Column Chart\n# getting means of Petal length and width for each species\n# and mean sepal length and sepal width\niris_means &lt;- iris %&gt;%\n  group_by(Species) %&gt;%\n  summarise(\n    mean_sepal_length = mean(Sepal.Length),\n    mean_sepal_width = mean(Sepal.Width),\n    mean_petal_length = mean(Petal.Length),\n    mean_petal_width = mean(Petal.Width)\n  ) %&gt;%\n  pivot_longer(\n    cols = -Species,\n    names_to = \"Measurement\",\n    values_to = \"MeanValue\"\n    )\n\nggplot(iris_means, aes(x = Measurement, y = MeanValue, fill = Species)) +\n  geom_col(position = position_dodge(width = 0.8)) + \n  labs(title = \"Mean Iris Measurements by Species\",\n       x = \"Measurement\", y = \"Mean Value\") + \n  theme_minimal(base_size = 12) +\n  scale_fill_manual(values = c(\"steelblue\", \"orange\", \"seagreen\"))\n\n\n\n\n\n\n\n\n\nClass coding competition\n\n\nlibrary(ggplot2)\nmpg &lt;- as.data.frame(mpg)\n#2seater, compact, midsize, minivan, pickup, subcompact, suv scatterplots in one view\nggplot(mpg, aes(x=displ, y=hwy)) +\n  geom_point(color = \"black\") +\n  facet_wrap(~ class) +\n  labs(x=\"displ\",\n       y=\"hwy\") +\n  theme_gray()\n\n\n\n\n\n\n\n\n\n#improving the chart\nggplot(mpg, aes(x=displ, y=hwy)) +\n  geom_point(color = \"blue\", size=2, alpha=0.3) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"#E65100\", linewidth = 0.8) +\n  facet_wrap(~ class) +\n  labs(title=\"Engine Displacement vs Highway MPG by Vehicle Class\",\n       x=\"Engine Displacement (liters)\",\n       y=\"Highway Miles per Gallon (MPG)\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size=16, face=\"bold\"),\n    axis.title.x = element_text(size=12),\n    axis.title.y = element_text(size=12)\n  )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n# GPT was used for picking colors and family.\n# GPT was used for adjusting the format of the code.\nlibrary(ggplot2)\nlibrary(scales)   # for alpha()\n\n\nAttaching package: 'scales'\n\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\ndata(iris)\ncolor1 &lt;- \"#18A3A380\"\ncolor2 &lt;- \"#FF4D8DCC\"\ncolor3 &lt;- \"#7A7A7A\"\ncolor4 &lt;- \"#000000\"\nbase_family &lt;- \"sans\"\n\n# custom theme used across plots\ntheme1 &lt;- function() {\n  theme_minimal(base_family = base_family) +\n    theme(\n      text        = element_text(family = base_family, colour = color4),\n      plot.title  = element_text(face = \"bold\", colour = color4, size = 13),\n      axis.title  = element_text(colour = color4),\n      axis.text   = element_text(colour = color3),\n      panel.grid.major = element_line(color = scales::alpha(color3, 0.3), linetype = \"dotted\"),\n      panel.grid.minor = element_blank()\n    )\n}\n\n\nHisto &lt;- function(){\n  hist(iris$Sepal.Length,\n       main=\"Distribution of Sepal Length (iris)\",\n       col=color1, border=color3)\n}\n\nBar1 &lt;- function(){\n  barplot(table(iris$Species),\n          col=c(color1,color2,color3),\n          border=color4,\n          main=\"Count by Species\",\n          xlab=\"Species\", ylab=\"Count\")\n}\n\nBar2 &lt;- function(){\n  barplot(table(iris$Species),\n          horiz=TRUE,\n          col=c(color1,color2,color3),\n          border=color4,\n          main=\"Count by Species (Horizontal)\",\n          xlab=\"Count\", ylab=\"Species\")\n}\n\nPie &lt;- function(){\n  pie(table(iris$Species),\n      col=c(color1,color2,color3),\n      main=\"Species Composition\",\n      clockwise=TRUE)\n}\n\nBox &lt;- function(){\n  boxplot(Sepal.Length~Species, data=iris,\n          col=c(color1,color2,color3),\n          main=\"Sepal Length by Species\",\n          xlab=\"Species\", ylab=\"Sepal Length (cm)\")\n}\n\nScat &lt;- function(){\n  plot(iris$Petal.Length, iris$Sepal.Length,\n       main=\"Sepal vs Petal Length\",\n       xlab=\"Petal Length (cm)\", ylab=\"Sepal Length (cm)\",\n       pch=19, col=color1)\n}\n\n\nlibrary(gridExtra)\n\npar(mfrow=c(2,3), mar=c(4,4,2.5,1), family=\"sans\")\nHisto(); Bar1(); Bar2(); Pie(); Box(); Scat()\n\n\n\n\n\n\n\n\n\ndraw6 &lt;- function(){\n  par(mfrow=c(2,3), mar=c(4,4,2.5,1), family=base_family)\n  Histo(); Bar1(); Bar2(); Pie(); Box(); Scat()\n}\n\nsave_plot &lt;- function(fmt, file){\n  switch(fmt,\n    pdf  = pdf(file, width=10, height=7, family=base_family),\n    jpg  = jpeg(file, width=2400, height=1600, res=300, quality=95),\n    svg  = svg(file, width=2400, height=1600),\n    tiff = tiff(file, width=2400, height=1600, res=300),\n    bmp  = bmp(file, width=2400, height=1600, res=300), # cannot find bmg, and was told it might be .bmp by GPT\n  )\n  draw6(); invisible(dev.off())\n}\n\n\nsave_plot(\"pdf\",  \"/Users/buttegg/Documents/IAmTryingToUseQuartoDoSthCool/buttegggggggg.github.io/baseR_6plots.pdf\")\nsave_plot(\"jpg\",  \"/Users/buttegg/Documents/IAmTryingToUseQuartoDoSthCool/buttegggggggg.github.io/baseR_6plots.jpg\")\nsave_plot(\"svg\",  \"/Users/buttegg/Documents/IAmTryingToUseQuartoDoSthCool/buttegggggggg.github.io/baseR_6plots.svg\")\nsave_plot(\"tiff\", \"/Users/buttegg/Documents/IAmTryingToUseQuartoDoSthCool/buttegggggggg.github.io/baseR_6plots.tiff\")\nsave_plot(\"bmp\",  \"/Users/buttegg/Documents/IAmTryingToUseQuartoDoSthCool/buttegggggggg.github.io/baseR_6plots.bmp\")\n\nPDF of base R plots\nJPG of base R plots\nSVG of base R plots\nTIFF of base R plots\nBMP of base R plots\n\nggHisto &lt;- ggplot(iris, aes(x=Sepal.Length)) +\n  geom_histogram(fill=color1, color=color3, bins=20) +\n  labs(title=\"Distribution of Sepal Length (iris)\") +\n  theme1()\n\nggBar1 &lt;- ggplot(iris, aes(x=Species, fill=Species)) +\n  geom_bar(color=color4) +\n  scale_fill_manual(values=c(color1,color2,color3)) +\n  labs(title=\"Count by Species\") +\n  theme1() + ggplot2::theme(legend.position=\"none\")\n\nggBar2 &lt;- ggplot(iris, aes(x=Species, fill=Species)) +\n  geom_bar(color=color4) + coord_flip() +\n  scale_fill_manual(values=c(color1,color2,color3)) +\n  labs(title=\"Count by Species (Horizontal)\") +\n  theme1() + ggplot2::theme(legend.position=\"none\")\n\ndf &lt;- as.data.frame(prop.table(table(iris$Species)))\ncolnames(df) &lt;- c(\"Species\",\"prop\")\nggPie &lt;- ggplot(df, aes(x=\"\", y=prop, fill=Species)) +\n  geom_col(width=1, color=NA) + coord_polar(theta=\"y\") +\n  scale_fill_manual(values=c(color1,color2,color3)) +\n  labs(title=\"Species Composition\") +\n  theme1() + ggplot2::theme(axis.text=ggplot2::element_blank(),\n                            axis.title=ggplot2::element_blank(),\n                            panel.grid=ggplot2::element_blank(),\n                            legend.position=\"right\")\n\nggBox &lt;- ggplot(iris, aes(x=Species, y=Sepal.Length, fill=Species)) +\n  geom_boxplot(color=color4) +\n  scale_fill_manual(values=c(color1,color2,color3)) +\n  labs(title=\"Sepal Length by Species\") +\n  theme1() + ggplot2::theme(legend.position=\"none\")\n\nggScat &lt;- ggplot(iris, aes(x=Petal.Length, y=Sepal.Length)) +\n  geom_point(color=color1, size=2) +\n  labs(title=\"Sepal vs Petal Length\") +\n  theme1()\n\n\ngridExtra::grid.arrange(ggHisto, ggBar1, ggBar2, ggPie, ggBox, ggScat, ncol = 3)\n\n\n\n\n\n\n\n\n\ncombo &lt;- gridExtra::arrangeGrob(ggHisto, ggBar1, ggBar2, ggPie, ggBox, ggScat, ncol = 3)\noutdir &lt;- \"/Users/buttegg/Documents/IAmTryingToUseQuartoDoSthCool/buttegggggggg.github.io\"\nformats &lt;- c(\"pdf\", \"jpg\", \"svg\", \"tiff\", \"bmp\")\n\nfor (fmt in formats) {\n  outpath &lt;- file.path(outdir, paste0(\"ggplot_6plots.\", fmt))\n  ggplot2::ggsave(\n    filename = outpath,\n    plot = combo,\n    width = 10, height = 7, dpi = 300\n  )\n}\n\nPDF of ggplot2 plots\nJPG of ggplot2 plots\nSVG of ggplot2 plots\nTIFF of ggplot2 plots\nBMP of ggplot2 plots"
  },
  {
    "objectID": "EPPS6356.html#assignments",
    "href": "EPPS6356.html#assignments",
    "title": "Data Visualization",
    "section": "",
    "text": "Assignment 1\n\n\n\nAssignment 2\n\n\n\nAssignment 3\n\n\n\n\ndata(iris)\n\nlibrary(tidyverse)\n\nâ”€â”€ Attaching core tidyverse packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 2.0.0 â”€â”€\nâœ” dplyr     1.1.4     âœ” readr     2.1.5\nâœ” forcats   1.0.1     âœ” stringr   1.5.2\nâœ” ggplot2   4.0.0     âœ” tibble    3.3.0\nâœ” lubridate 1.9.4     âœ” tidyr     1.3.1\nâœ” purrr     1.1.0     \nâ”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€\nâœ– dplyr::filter() masks stats::filter()\nâœ– dplyr::lag()    masks stats::lag()\nâ„¹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# 1. Divide the dataset into three rectangles based on species.\n# The average of Petal.Length and Petal.Width is the length and width.\n# Draw three rectangles arranged horizontally.\n\n#1\n\nplot_data &lt;- iris %&gt;%\n  mutate(\n    sepal_length_group = cut(\n      Sepal.Length,\n      breaks = c(4, 5.5, 7.0, 8.0),\n      labels = c(\"Small (4.0-5.5)\", \"Medium (5.6-7.0)\", \"Large (7.1-8.0)\"),\n      include.lowest = TRUE\n    )\n  ) %&gt;%\n  group_by(sepal_length_group) %&gt;%\n  summarise(\n    count = n(),\n    avg_petal_length = mean(Petal.Length)\n  ) %&gt;%\n  mutate(\n    xmax = cumsum(count),\n    xmin = xmax - count,\n    x_label_pos = (xmin + xmax) / 2\n  )\n\nggplot(plot_data, aes(ymin = 0)) +\n  geom_rect(\n    aes(\n      xmin = xmin,\n      xmax = xmax,\n      ymax = avg_petal_length,\n      fill = sepal_length_group\n    ),\n    color = \"white\"\n  ) +\n  scale_x_continuous(\n    breaks = plot_data$x_label_pos,\n    labels = plot_data$sepal_length_group,\n    expand = c(0, 0)\n  ) +\n  scale_fill_viridis_d(option = \"D\", direction = -1) +\n  labs(\n    title = \"Average Petal Length by Sepal Length Group\",\n    subtitle = \"Column width is proportional to the number of flowers in each group\",\n    x = \"Count of Flowers in Group\",\n    y = \"Average Petal Length (cm)\",\n    fill = \"Sepal Length Group\"\n  ) +\n  # Apply a clean theme\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 18),\n    legend.position = \"bottom\",\n    panel.grid.major.x = element_blank(), # Remove vertical grid lines\n    panel.grid.minor.x = element_blank()\n  )\n\n\n\n\n\n\n\n\n\n# 2. table with embedded charts\niris_long &lt;- iris %&gt;%\n  pivot_longer(cols = -Species, names_to = \"Measurement\", values_to = \"Value\")\n\nggplot(iris_long, aes(x = Value, fill = Species)) +\n  geom_histogram(color = \"white\", bins = 15) +\n  facet_grid(Species ~ Measurement, scales = \"free\") +\n  scale_fill_manual( #coloring each species\n    values = c(\n      \"setosa\" = \"steelblue\", \n      \"versicolor\" = \"orange\",   \n      \"virginica\" = \"seagreen\"     \n    ) \n    ) + #labels\n      labs(\n        title = \"Distribution of Iris Measurements by Species\",\n        x = \"Measurement Value (cm)\",\n        y = \"Count\"\n      ) +\n  theme_bw() +\n    theme(\n      plot.title = element_text(hjust = 0.5, face = \"bold\"),\n      strip.text.x = element_text(face = \"bold\"),\n      strip.text.y = element_text(face = \"bold\"),\n      panel.border = element_rect(color = \"grey80\", fill = NA),\n      legend.position = \"bottom\"\n    )\n\n\n\n\n\n\n\n\n\n# 3. Extract setona and versicolor from species.\n# Then create df_2 and df_3. Draw a bar plot using petal.width: p1 p2.\n# Finally, use gridExtra to combine the plots.'\nlibrary(\"gridExtra\")\n\n\nAttaching package: 'gridExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\ndf_2 &lt;- subset(iris, Species %in% \"setosa\")\ndf_3 &lt;- subset(iris, Species %in% \"versicolor\")\ndf_2$id &lt;- 1:nrow(df_2)\ndf_3$id &lt;- 1:nrow(df_3)\n\n\n\np1 = ggplot(df_2, aes(x = factor(id), y = Petal.Width)) +\n  geom_bar(stat = \"identity\", fill = 'red', color = \"black\") +\n  coord_flip() +\n  labs(title = \"setosa\") +\n  theme(\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank() #this was by GPT\n  )\n\n\np2 = ggplot(df_3, aes(x = factor(id), y = Petal.Width)) +\n  geom_bar(stat = \"identity\", fill = \"blue\", color = \"black\") +\n  coord_flip() +\n  labs(title = \"versicolor\")+\n  theme(\n    axis.text.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    axis.text.x = element_blank(),\n    axis.ticks.x = element_blank() #this was by GPT\n  )\n\n\n\ngridExtra::grid.arrange(p1, p2, ncol = 2)\n\n\n\n\n\n\n\n\n\n# 4 Column Chart\n# getting means of Petal length and width for each species\n# and mean sepal length and sepal width\niris_means &lt;- iris %&gt;%\n  group_by(Species) %&gt;%\n  summarise(\n    mean_sepal_length = mean(Sepal.Length),\n    mean_sepal_width = mean(Sepal.Width),\n    mean_petal_length = mean(Petal.Length),\n    mean_petal_width = mean(Petal.Width)\n  ) %&gt;%\n  pivot_longer(\n    cols = -Species,\n    names_to = \"Measurement\",\n    values_to = \"MeanValue\"\n    )\n\nggplot(iris_means, aes(x = Measurement, y = MeanValue, fill = Species)) +\n  geom_col(position = position_dodge(width = 0.8)) + \n  labs(title = \"Mean Iris Measurements by Species\",\n       x = \"Measurement\", y = \"Mean Value\") + \n  theme_minimal(base_size = 12) +\n  scale_fill_manual(values = c(\"steelblue\", \"orange\", \"seagreen\"))\n\n\n\n\n\n\n\n\n\nClass coding competition\n\n\nlibrary(ggplot2)\nmpg &lt;- as.data.frame(mpg)\n#2seater, compact, midsize, minivan, pickup, subcompact, suv scatterplots in one view\nggplot(mpg, aes(x=displ, y=hwy)) +\n  geom_point(color = \"black\") +\n  facet_wrap(~ class) +\n  labs(x=\"displ\",\n       y=\"hwy\") +\n  theme_gray()\n\n\n\n\n\n\n\n\n\n#improving the chart\nggplot(mpg, aes(x=displ, y=hwy)) +\n  geom_point(color = \"blue\", size=2, alpha=0.3) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"#E65100\", linewidth = 0.8) +\n  facet_wrap(~ class) +\n  labs(title=\"Engine Displacement vs Highway MPG by Vehicle Class\",\n       x=\"Engine Displacement (liters)\",\n       y=\"Highway Miles per Gallon (MPG)\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, size=16, face=\"bold\"),\n    axis.title.x = element_text(size=12),\n    axis.title.y = element_text(size=12)\n  )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n\n# GPT was used for picking colors and family.\n# GPT was used for adjusting the format of the code.\nlibrary(ggplot2)\nlibrary(scales)   # for alpha()\n\n\nAttaching package: 'scales'\n\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\ndata(iris)\ncolor1 &lt;- \"#18A3A380\"\ncolor2 &lt;- \"#FF4D8DCC\"\ncolor3 &lt;- \"#7A7A7A\"\ncolor4 &lt;- \"#000000\"\nbase_family &lt;- \"sans\"\n\n# custom theme used across plots\ntheme1 &lt;- function() {\n  theme_minimal(base_family = base_family) +\n    theme(\n      text        = element_text(family = base_family, colour = color4),\n      plot.title  = element_text(face = \"bold\", colour = color4, size = 13),\n      axis.title  = element_text(colour = color4),\n      axis.text   = element_text(colour = color3),\n      panel.grid.major = element_line(color = scales::alpha(color3, 0.3), linetype = \"dotted\"),\n      panel.grid.minor = element_blank()\n    )\n}\n\n\nHisto &lt;- function(){\n  hist(iris$Sepal.Length,\n       main=\"Distribution of Sepal Length (iris)\",\n       col=color1, border=color3)\n}\n\nBar1 &lt;- function(){\n  barplot(table(iris$Species),\n          col=c(color1,color2,color3),\n          border=color4,\n          main=\"Count by Species\",\n          xlab=\"Species\", ylab=\"Count\")\n}\n\nBar2 &lt;- function(){\n  barplot(table(iris$Species),\n          horiz=TRUE,\n          col=c(color1,color2,color3),\n          border=color4,\n          main=\"Count by Species (Horizontal)\",\n          xlab=\"Count\", ylab=\"Species\")\n}\n\nPie &lt;- function(){\n  pie(table(iris$Species),\n      col=c(color1,color2,color3),\n      main=\"Species Composition\",\n      clockwise=TRUE)\n}\n\nBox &lt;- function(){\n  boxplot(Sepal.Length~Species, data=iris,\n          col=c(color1,color2,color3),\n          main=\"Sepal Length by Species\",\n          xlab=\"Species\", ylab=\"Sepal Length (cm)\")\n}\n\nScat &lt;- function(){\n  plot(iris$Petal.Length, iris$Sepal.Length,\n       main=\"Sepal vs Petal Length\",\n       xlab=\"Petal Length (cm)\", ylab=\"Sepal Length (cm)\",\n       pch=19, col=color1)\n}\n\n\nlibrary(gridExtra)\n\npar(mfrow=c(2,3), mar=c(4,4,2.5,1), family=\"sans\")\nHisto(); Bar1(); Bar2(); Pie(); Box(); Scat()\n\n\n\n\n\n\n\n\n\ndraw6 &lt;- function(){\n  par(mfrow=c(2,3), mar=c(4,4,2.5,1), family=base_family)\n  Histo(); Bar1(); Bar2(); Pie(); Box(); Scat()\n}\n\nsave_plot &lt;- function(fmt, file){\n  switch(fmt,\n    pdf  = pdf(file, width=10, height=7, family=base_family),\n    jpg  = jpeg(file, width=2400, height=1600, res=300, quality=95),\n    svg  = svg(file, width=2400, height=1600),\n    tiff = tiff(file, width=2400, height=1600, res=300),\n    bmp  = bmp(file, width=2400, height=1600, res=300), # cannot find bmg, and was told it might be .bmp by GPT\n  )\n  draw6(); invisible(dev.off())\n}\n\n\nsave_plot(\"pdf\",  \"/Users/buttegg/Documents/IAmTryingToUseQuartoDoSthCool/buttegggggggg.github.io/baseR_6plots.pdf\")\nsave_plot(\"jpg\",  \"/Users/buttegg/Documents/IAmTryingToUseQuartoDoSthCool/buttegggggggg.github.io/baseR_6plots.jpg\")\nsave_plot(\"svg\",  \"/Users/buttegg/Documents/IAmTryingToUseQuartoDoSthCool/buttegggggggg.github.io/baseR_6plots.svg\")\nsave_plot(\"tiff\", \"/Users/buttegg/Documents/IAmTryingToUseQuartoDoSthCool/buttegggggggg.github.io/baseR_6plots.tiff\")\nsave_plot(\"bmp\",  \"/Users/buttegg/Documents/IAmTryingToUseQuartoDoSthCool/buttegggggggg.github.io/baseR_6plots.bmp\")\n\nPDF of base R plots\nJPG of base R plots\nSVG of base R plots\nTIFF of base R plots\nBMP of base R plots\n\nggHisto &lt;- ggplot(iris, aes(x=Sepal.Length)) +\n  geom_histogram(fill=color1, color=color3, bins=20) +\n  labs(title=\"Distribution of Sepal Length (iris)\") +\n  theme1()\n\nggBar1 &lt;- ggplot(iris, aes(x=Species, fill=Species)) +\n  geom_bar(color=color4) +\n  scale_fill_manual(values=c(color1,color2,color3)) +\n  labs(title=\"Count by Species\") +\n  theme1() + ggplot2::theme(legend.position=\"none\")\n\nggBar2 &lt;- ggplot(iris, aes(x=Species, fill=Species)) +\n  geom_bar(color=color4) + coord_flip() +\n  scale_fill_manual(values=c(color1,color2,color3)) +\n  labs(title=\"Count by Species (Horizontal)\") +\n  theme1() + ggplot2::theme(legend.position=\"none\")\n\ndf &lt;- as.data.frame(prop.table(table(iris$Species)))\ncolnames(df) &lt;- c(\"Species\",\"prop\")\nggPie &lt;- ggplot(df, aes(x=\"\", y=prop, fill=Species)) +\n  geom_col(width=1, color=NA) + coord_polar(theta=\"y\") +\n  scale_fill_manual(values=c(color1,color2,color3)) +\n  labs(title=\"Species Composition\") +\n  theme1() + ggplot2::theme(axis.text=ggplot2::element_blank(),\n                            axis.title=ggplot2::element_blank(),\n                            panel.grid=ggplot2::element_blank(),\n                            legend.position=\"right\")\n\nggBox &lt;- ggplot(iris, aes(x=Species, y=Sepal.Length, fill=Species)) +\n  geom_boxplot(color=color4) +\n  scale_fill_manual(values=c(color1,color2,color3)) +\n  labs(title=\"Sepal Length by Species\") +\n  theme1() + ggplot2::theme(legend.position=\"none\")\n\nggScat &lt;- ggplot(iris, aes(x=Petal.Length, y=Sepal.Length)) +\n  geom_point(color=color1, size=2) +\n  labs(title=\"Sepal vs Petal Length\") +\n  theme1()\n\n\ngridExtra::grid.arrange(ggHisto, ggBar1, ggBar2, ggPie, ggBox, ggScat, ncol = 3)\n\n\n\n\n\n\n\n\n\ncombo &lt;- gridExtra::arrangeGrob(ggHisto, ggBar1, ggBar2, ggPie, ggBox, ggScat, ncol = 3)\noutdir &lt;- \"/Users/buttegg/Documents/IAmTryingToUseQuartoDoSthCool/buttegggggggg.github.io\"\nformats &lt;- c(\"pdf\", \"jpg\", \"svg\", \"tiff\", \"bmp\")\n\nfor (fmt in formats) {\n  outpath &lt;- file.path(outdir, paste0(\"ggplot_6plots.\", fmt))\n  ggplot2::ggsave(\n    filename = outpath,\n    plot = combo,\n    width = 10, height = 7, dpi = 300\n  )\n}\n\nPDF of ggplot2 plots\nJPG of ggplot2 plots\nSVG of ggplot2 plots\nTIFF of ggplot2 plots\nBMP of ggplot2 plots"
  },
  {
    "objectID": "EPPS6356.html#reviews",
    "href": "EPPS6356.html#reviews",
    "title": "Data Visualization",
    "section": "Reviews",
    "text": "Reviews\n\nReview: Inge Druckrey â€“ Teaching to See\nReview: Journalism in the Age of Data\nReview: The Future of Data Analysis\nReview: Data Visualization and Data Science\nReview: The Week in Charts & 2024 The Year in Charts"
  },
  {
    "objectID": "EPPS6356.html#notes",
    "href": "EPPS6356.html#notes",
    "title": "Data Visualization",
    "section": "Notes",
    "text": "Notes\n\nNotes: Big Data Pitfalls"
  },
  {
    "objectID": "EPPS6356.html#checklist",
    "href": "EPPS6356.html#checklist",
    "title": "Data Visualization",
    "section": "Checklist",
    "text": "Checklist\n\nTzu-Yuanâ€™s Data Guide Checklist"
  },
  {
    "objectID": "SDAR.html",
    "href": "SDAR.html",
    "title": "MS of Social Data Analytics & Research",
    "section": "",
    "text": "å­¸æ­·ï¼š å¾·å·å¤§å­¸é”æ‹‰æ–¯åˆ†æ ¡ ç¤¾æœƒæ•¸æ“šåˆ†æèˆ‡ç ”ç©¶ç¢©å£« (Master of Science in Social Data Analytics and Research, UTD)ã€‚"
  },
  {
    "objectID": "SDAR.html#about-the-program",
    "href": "SDAR.html#about-the-program",
    "title": "MS of Social Data Analytics & Research",
    "section": "About the Program",
    "text": "About the Program\nThe Master of Science in Social Data Analytics and Research (SDAR) program at The University of Texas at Dallas trains students to collect, manage, analyze, and visualize social and behavioral data. The curriculum spans GIS, data visualization, information management, and quantitative methods â€” equipping graduates with skills for data-driven decision-making in public policy, social science, and industry."
  },
  {
    "objectID": "SDAR.html#coursework",
    "href": "SDAR.html#coursework",
    "title": "MS of Social Data Analytics & Research",
    "section": "Coursework",
    "text": "Coursework\n\n\nGIS & Python Programming\nGeographic information systems, spatial data analysis, and Python programming â€” from basic scripting and GUI applications to web scraping and SVD image compression.\n\n\nData Visualization\nPrinciples of effective data visualization, grammar of graphics, and practical chart design â€” with assignments in R (ggplot2) and critical reviews of visualization literature.\n\n\nInformation Management\nDatabase design, SQL, information systems architecture, and data management strategies for social science research."
  },
  {
    "objectID": "EPPS6354.html",
    "href": "EPPS6354.html",
    "title": "Information Management",
    "section": "",
    "text": "Name and describe three applications you have used that employed a database system to store and access persistent data. (e.g.Â airlines, online trade, banking, university system)\nFor the first question, one example that comes to mind is video games. In video games, a playerâ€™s level and experience points, as well as the items and equipment they have obtained, are recorded, so the player can still access them the next time they log in. Another example is online shopping. For instance, Amazon records information such as the price of each product, the catalog it belongs to, whether it is eligible for free shipping, and whether it is in stock. A third example is a streaming platform, such as Netflix, which records a userâ€™s region and subscription level. All of this data is stored persistently and can be accessed at a later time.\n\n\n\nPropose three applications in domain projects (e.g.Â criminology, economics, brain science, etc.) Be sure you include: i. Purpose ii. Functions iii. Simple interface design\n\n\n\n\nThe main purpose of this wardrobe management database is to minimize the time spent choosing outfits before going out.\nFor many people, the difficulty in daily outfit selection is not a lack of clothing, but the need to simultaneously consider colors, styles, occasions, and overall coordination, which leads to a high decision-making cost.\nTherefore, I model the wardrobe as a relational database, which not only records individual clothing items but also describes the relationships between items, allowing outfit selection to be handled in a systematic way.\nBy structuring clothing data, this system aims to transform â€œrethinking what to wear every dayâ€ into â€œquickly selecting optimal combinations from a database.â€\n\n\n\nIn this system, each clothing item is treated as a data entity and described using a set of attributes, such as:\n\ncategory (T-shirts, jeans, outerwear, shoes),\ncolor (including the proportion of each color),\nstyle (clean-fit, formal, vintage, sports, etc.),\nmaterial (denim, linen, cotton).\n\nThese attributes are normalized into multiple tables, and many-to-many relationships are used to represent that a single item can belong to multiple styles or be suitable for different occasions.\nThe core function of this database is not only to store items, but to describe the compatibility between items.\nThe system uses compatibility rules to define:\n\nVisual aesthetic constraints, such as avoiding more than three colors in a single outfit and limiting the number of style tags to maintain overall consistency\nClimate adaptability, where combinations are evaluated based on insulation-related variables to ensure balanced warmth between upper and lower body layers, and higher overall insulation is preferred as the temperature decreases\n\nWhen a user selects a specific item (for example, a pink T-shirt), the system can immediately recommend other highly compatible items (such as light blue jeans and white sneakers) based on database relationships and rules, and rank these combinations by compatibility score to help the user make decisions more efficiently.\nIn addition, as data accumulates, the system can analyze the overall structure of the wardrobe, such as:\n\nWhether certain styles or clothing categories are lacking\nWhether colors or item types are overly concentrated\nWhether newly purchased items overlap in function with existing ones\nWhich older items have not been used for a long time and could be considered for removal\n\nThis allows the wardrobe to function not just as an item list, but as a system that can be queried, analyzed, and optimized, and that can be extended to daily life applications such as outfit recommendations and purchase decision support.\nThis problem is particularly well suited for a relational database, because outfit selection inherently involves structured data and many-to-many relationships (such as items, styles, and compatibility rules), which can be efficiently combined and analyzed through relational queries.\n\n\n\nWhen users enter the system, the home page displays a table view of all items in the wardrobe, including basic information such as category, color, style, material, and seasonality. The interface supports multi-select functionality.\nUsers can select one or more items they plan to wear and submit their selection to generate outfit results.\nBased on the selected items and the compatibility rules stored in the database, the system generates multiple outfit candidates.\nThe outfit results page provides different sorting options, such as sorting by comfort score, aesthetic score, or climate fit score.\nEach outfit displays its corresponding numerical scores, allowing users to quickly compare options and select the most suitable combination without repeatedly trying on clothes or overthinking the decision.\nThe interface supports fast decision-making: select items â†’ generate outfits â†’ sort by scores â†’ pick the best match.\n\n\n\n\n\n\nThe purpose of this 3D printing farm database is to systematize the entire workflowâ€”from customer order intake to automated estimation, machine scheduling, and progress trackingâ€”so the farm can operate efficiently as order volume grows. The goals are to shorten turnaround time, reduce human scheduling errors, improve machine utilization, and maximize profitability.\nIn practice, 3D printing orders vary widely (model size, material, resolution, multi-color requirements, and post-processing such as painting). If pricing and scheduling rely on manual judgment, it is easy to underestimate time/cost, assign the wrong machine, or create bottlenecks in the order queue. Therefore, this system uses a relational database to store orders, machine capabilities, material usage, and scheduling states in a structured way, enabling fast and consistent decisions through rules and queries.\n\n\n\nOrder intake & requirement tagging (Order Intake & Requirement Tagging)\nWhen a customer submits an order, the system stores it as an order record with structured attributes, such as:\n\nModel size and volume (bounding box / volume)\nPrinting type (FDM / SLA)\nResolution settings (layer height / resolution)\nMulti-color requirement (multi-color)\nMaterial type (material type)\nPost-processing needs (post-processing, e.g., painting/sanding)\nOther customization requests (stored as tags)\n\nThese fields can be normalized into multiple tables, with many-to-many relationships used to represent that a single order can have multiple requirement tags.\nPer-machine estimation (Per-Machine Estimation)\nThe key is not only to calculate an overall price for the order, but to estimate how the same order would perform on different machines, since time, cost, and completion time may vary by machine. This supports better machine assignment and scheduling decisions.\nFor each candidate machine, the system applies pricing rules or an estimation model to perform per-machine estimation, including:\n\nEstimated print time (estimated print time)\nEstimated material usage (estimated material usage)\nMachine-specific estimated cost & quote (machine-specific estimated cost & quote)\nEstimated completion time (estimated completion time, considering current workload)\n\nThe system stores these â€œorder Ã— machineâ€ estimates for querying and ranking using different objective functions, such as lowest cost, earliest completion, or the most stable option within a deadline.\nOrder queue & status tracking (Order Queue & Status Tracking)\nAll orders are automatically added to an order queue (order list), and each order maintains a clear status, such as:\n\npending\nqueued\nprinting\npost-processing\ncompleted\nfailed\n\nManagers can query:\n\nWhat is currently in the queue and its priority\nWhich orders are printing vs.Â waiting for machines\nWhich failed orders require reprinting or manual intervention\n\nMachine capability modeling & assignment recommendations (Machine Capability & Assignment)\nThe database stores each machineâ€™s capabilities and constraints, such as: - Machine type: multi-color / single-color / SLA / FDM - Maximum build volume (max build volume) - Supported materials (supported materials) - Speed/quality profile (speed/quality profile) - Current workload and availability (workload & availability)\nWhen a new order arrives, the system first performs constraint filtering (e.g., size, material, multi-color requirements) to identify feasible machines, then uses per-machine estimation to generate recommended assignments, for example:\n\nEarliest completion time (earliest completion time)\nLowest estimated cost (lowest estimated cost)\nBalanced option (deadline + stability)\n\nThis turns scheduling into a decision-support process rather than manual guesswork.\n\n\n\nOn the customer side, the system provides a customer order page where users can upload a 3D model or specify printing requirements such as size, material, resolution, multi-color options, and post-processing needs. Based on this information, the system automatically returns an estimated price and an estimated delivery time.\nOn the admin side, the system offers an order dashboard that displays the current order queue and order statuses. Administrators can sort or filter orders by deadline, priority, or processing status to manage workflow more efficiently.\nThe system also includes a machine dashboard that lists all available machines along with their machine type, maximum build volume, supported materials, current workload, and estimated availability. This allows operators to quickly understand machine capacity and constraints.\nWhen an order is selected, the scheduling view presents a list of candidate machines that can fulfill the order. For each candidate machine, the system displays the estimated print time, estimated material usage, machine-specific cost and quote, and estimated completion time. The interface supports one-click sorting options, such as fastest, cheapest, or most stable, to assist administrators in making assignment decisions.\nThe interface supports efficient operations: submit order â†’ per-machine estimation â†’ queue order â†’ recommend machines â†’ schedule & track progress.\n\n\n\n\n\n\nThe purpose of this system is to manage the core information of a farmâ€”such as fields, crop types, growth stages, and irrigation equipmentâ€”using a relational database.\nAt the same time, the system retrieves and stores weather data through APIs provided by weather forecast services, and combines this information with a set of irrigation rules to automatically generate a daily irrigation schedule.\nThe goal is to reduce manual decision-making costs while improving water-use efficiency and consistency in crop management.\n\n\n\nIn this system, the database is not used only for data storage. Its core function is to integrate internal farm information with external weather data and automatically generate irrigation decisions based on predefined rules.\nCore Data Management\nThe system uses a relational schema to manage the main entities of the farm, including:\n\nField: field ID, location, area, and the crop currently planted\nCrop: crop type and its basic water requirements\nGrowth Stage: stages such as germination, growth, flowering, and fruiting, each with different water needs\nIrrigation Equipment: equipment type (e.g., drip irrigation, sprinkler), flow rate or efficiency factor, and availability status\n\nThese entities are connected through relationships. For example, each field is associated with a specific crop and a current growth stage, and can be assigned available irrigation equipment.\nWeather Data Integration\nThe system retrieves weather information through external weather forecast APIs, such as:\n\nPredicted rainfall amount\nProbability of precipitation\nTemperature range\n\nThis weather data is stored in the database and used as an important input for daily irrigation decisions, without requiring manual input from users.\nIrrigation Rules and Schedule Generation\nThe system maintains a set of irrigation rules that describe irrigation requirements under different conditions, such as:\n\nCrop type Ã— growth stage â†’ recommended baseline irrigation amount\nIf predicted rainfall exceeds a certain threshold â†’ automatically reduce or cancel irrigation for the day\nDifferences in irrigation equipment efficiency â†’ adjust actual irrigation duration\n\nWhen the daily scheduling process runs, the system combines:\n\nThe crop type and growth stage of each field\nThe weather forecast for the day\nThe availability and efficiency of irrigation equipment\n\nBased on this information, the system automatically generates a daily irrigation schedule, indicating whether each field requires irrigation and the recommended water amount or irrigation time.\n\n\n\nWhen users enter the system, the home page displays a table view of all fields on the farm, including the current crop type, growth stage, and the systemâ€™s irrigation recommendation for the day.\nUsers can generate the daily irrigation schedule with a single action. Based on field information, weather forecasts, and irrigation rules, the system lists which fields require irrigation and provides recommended water amounts or irrigation durations.\nThe schedule is presented in a simple list format, allowing users to quickly review and execute irrigation tasks. After completion, users can mark irrigation status for record-keeping and future reference.\n\n\n\n\n\nWhat are the things current database system cannot do?\nCurrent database systems are not capable of understanding the semantics behind data. As a result, in more complex applications, they often rely on manually defined rules or continuously adjusted weights to produce reasonable outputs. In addition, databases are limited in handling cross-context decision-making, where multiple competing objectives must be balanced simultaneously.\nFor example, in a wardrobe management database, the system can evaluate outfits based on structured criteria such as color combinations, style tags, material properties, and weather conditions. It can assign scores for factors like aesthetic quality, comfort, and climate suitability, and generate multiple candidate outfits that satisfy predefined rules. However, the database cannot determine which outfit represents the optimal balance among being visually appealing, comfortable, and suitable for the weather.\nThis limitation arises because preferences such as â€œlooking goodâ€ or â€œfeeling comfortableâ€ are inherently subjective and context-dependent, and there is no single optimal solution that applies to all users or situations. Therefore, the role of the database is not to make the final decision, but to support decision-making by filtering infeasible options, structuring relevant information, and presenting comparable alternatives with transparent evaluation metrics.\nUltimately, the final choice must be made by the user, who can decide whether to prioritize comfort, aesthetics, or climate suitability in a given context. This highlights a fundamental limitation of current database systems: they are effective at decision support, but they cannot replace human judgment in complex, value-driven decisions.\n\n\n\nDescribe at least three tables that might be used to store information in a social-network/social media system such as Twitter or Reddit.\nA social-network or social media system such as Twitter or Reddit may be supported by at least the following three core tables:\n1. User Table\nThe user table stores basic information about users, such as: - user_id - username - account creation time - profile metadata (e.g., bio or status)\nThis table represents the identities of users and serves as a reference for other tables in the system.\n2. Post Table\nThe post table stores content created by users, such as:\n\npost_id\nauthor_id (foreign key referencing the User table)\ncontent\ntimestamp\n\nEach post is associated with a specific user, forming a one-to-many relationship between users and posts.\n3. Comment Table\nThe comment table stores replies to posts (or other comments), such as:\n\ncomment_id\npost_id (foreign key referencing the Post table)\nauthor_id\ncontent\ntimestamp\n\nThis table supports threaded discussions and allows multiple users to participate in conversations under the same post.\nThese tables are separated to support relational queries, maintain data consistency, and enable efficient retrieval of users, posts, and discussion threads.\n\n\n\n\n\n\nWhat are the differences between relation schema, relation, and instance? Give an example using the university database to illustrate.\n\nRelation Schema = The logical structure of a relation: a list of attribute names and their domains. It does not change over time.\nExample: instructor(ID, name, dept_name, salary)\nRelation = Informally used to refer to both the schema and instance together.\nExample: â€œThe department relationâ€ can refer to either the schema department(dept_name, building, budget) or the actual data it currently holds.\nInstance = A snapshot of the actual data in a relation at a given point in time. It changes as tuples are inserted, updated, or deleted.\nExample: The department relation instance in Figure 2.5 contains 7 tuples. If the university adds a â€œData Scienceâ€ department, the instance grows to 8 tuples, but the schema remains department(dept_name, building, budget).\n\n\n\n\nDraw a schema diagram for the following bank database. Identify primary keys (underlined) and foreign keys.\nThe bank database consists of the following relations:\n\nbranch(branch_name, branch_city, assets)\ncustomer(ID, customer_name, customer_street, customer_city)\nloan(loan_number, branch_name, amount)\nborrower(ID, loan_number)\naccount(account_number, branch_name, balance)\ndepositor(ID, account_number)\n\n\n\n\nBank Database Schema Diagram\n\n\n\n\n\nDescribe two ways artificial intelligence or LLM can assist in managing or querying a database. In your answer, briefly explain how each method improves efficiency or accuracy compared to traditional (non-AI) approaches. (3â€“5 sentences)\n\nNatural Language to SQL (Querying): LLMs can translate plain language questions directly into executable SQL queries, lowering the barrier for non-technical users and reducing syntax errors compared to writing SQL manually.\nAI-Driven Database Tuning (Managing): LLMs can automatically analyze slow queries and recommend index optimizations, replacing the traditionally time-consuming process of a DBA manually examining query logs and execution plans.\n\nOverall, both approaches reduce the need for specialized expertise and allow faster, more accurate database operations compared to traditional manual methods.\n\n\n\n\n\n\nOpen the Online SQL interpreter and load the university database.\n\n\n\nWrite SQL codes to get a list of: i. Student IDs, ii. Instructors, iii. Departments\n\n\n\nQ2 â€” Student IDs (from takes), Instructors, and Departments\n\n\n\n\n\nWrite SQL codes to do the following queries:\ni. Find the ID and name of each student who has taken at least one Comp. Sci. course; make sure there are no duplicate names in the result.\n\n\n\nQ3i â€” Students who took at least one Comp. Sci. course\n\n\nii. Add grades to the list\n\n\n\nQ3ii â€” Add grades to the result\n\n\niii. Find the ID and name of each student who has not taken any course offered before 2017.\n\n\n\nQ3iii â€” Students who have not taken any course before 2017\n\n\niv. For each department, find the maximum salary of instructors in that department.\n\n\n\nQ3iv â€” Maximum instructor salary per department\n\n\nv. Find the lowest, across all departments, of the per-department maximum salary computed by the preceding query.\n\n\n\nQ3v â€” Lowest of the per-department maximum salaries\n\n\nvi. Add names to the list\n\n\n\nQ3vi â€” Add instructor names to the result\n\n\n\n\n\nFind instructor (with name and ID) who has never given an A grade in any course she or he has taught. (Instructors who have never taught a course trivially satisfy this condition.)\n\n\n\nQ4 â€” Instructors who have never given an A grade"
  },
  {
    "objectID": "EPPS6354.html#assignments",
    "href": "EPPS6354.html#assignments",
    "title": "Information Management",
    "section": "",
    "text": "Name and describe three applications you have used that employed a database system to store and access persistent data. (e.g.Â airlines, online trade, banking, university system)\nFor the first question, one example that comes to mind is video games. In video games, a playerâ€™s level and experience points, as well as the items and equipment they have obtained, are recorded, so the player can still access them the next time they log in. Another example is online shopping. For instance, Amazon records information such as the price of each product, the catalog it belongs to, whether it is eligible for free shipping, and whether it is in stock. A third example is a streaming platform, such as Netflix, which records a userâ€™s region and subscription level. All of this data is stored persistently and can be accessed at a later time.\n\n\n\nPropose three applications in domain projects (e.g.Â criminology, economics, brain science, etc.) Be sure you include: i. Purpose ii. Functions iii. Simple interface design\n\n\n\n\nThe main purpose of this wardrobe management database is to minimize the time spent choosing outfits before going out.\nFor many people, the difficulty in daily outfit selection is not a lack of clothing, but the need to simultaneously consider colors, styles, occasions, and overall coordination, which leads to a high decision-making cost.\nTherefore, I model the wardrobe as a relational database, which not only records individual clothing items but also describes the relationships between items, allowing outfit selection to be handled in a systematic way.\nBy structuring clothing data, this system aims to transform â€œrethinking what to wear every dayâ€ into â€œquickly selecting optimal combinations from a database.â€\n\n\n\nIn this system, each clothing item is treated as a data entity and described using a set of attributes, such as:\n\ncategory (T-shirts, jeans, outerwear, shoes),\ncolor (including the proportion of each color),\nstyle (clean-fit, formal, vintage, sports, etc.),\nmaterial (denim, linen, cotton).\n\nThese attributes are normalized into multiple tables, and many-to-many relationships are used to represent that a single item can belong to multiple styles or be suitable for different occasions.\nThe core function of this database is not only to store items, but to describe the compatibility between items.\nThe system uses compatibility rules to define:\n\nVisual aesthetic constraints, such as avoiding more than three colors in a single outfit and limiting the number of style tags to maintain overall consistency\nClimate adaptability, where combinations are evaluated based on insulation-related variables to ensure balanced warmth between upper and lower body layers, and higher overall insulation is preferred as the temperature decreases\n\nWhen a user selects a specific item (for example, a pink T-shirt), the system can immediately recommend other highly compatible items (such as light blue jeans and white sneakers) based on database relationships and rules, and rank these combinations by compatibility score to help the user make decisions more efficiently.\nIn addition, as data accumulates, the system can analyze the overall structure of the wardrobe, such as:\n\nWhether certain styles or clothing categories are lacking\nWhether colors or item types are overly concentrated\nWhether newly purchased items overlap in function with existing ones\nWhich older items have not been used for a long time and could be considered for removal\n\nThis allows the wardrobe to function not just as an item list, but as a system that can be queried, analyzed, and optimized, and that can be extended to daily life applications such as outfit recommendations and purchase decision support.\nThis problem is particularly well suited for a relational database, because outfit selection inherently involves structured data and many-to-many relationships (such as items, styles, and compatibility rules), which can be efficiently combined and analyzed through relational queries.\n\n\n\nWhen users enter the system, the home page displays a table view of all items in the wardrobe, including basic information such as category, color, style, material, and seasonality. The interface supports multi-select functionality.\nUsers can select one or more items they plan to wear and submit their selection to generate outfit results.\nBased on the selected items and the compatibility rules stored in the database, the system generates multiple outfit candidates.\nThe outfit results page provides different sorting options, such as sorting by comfort score, aesthetic score, or climate fit score.\nEach outfit displays its corresponding numerical scores, allowing users to quickly compare options and select the most suitable combination without repeatedly trying on clothes or overthinking the decision.\nThe interface supports fast decision-making: select items â†’ generate outfits â†’ sort by scores â†’ pick the best match.\n\n\n\n\n\n\nThe purpose of this 3D printing farm database is to systematize the entire workflowâ€”from customer order intake to automated estimation, machine scheduling, and progress trackingâ€”so the farm can operate efficiently as order volume grows. The goals are to shorten turnaround time, reduce human scheduling errors, improve machine utilization, and maximize profitability.\nIn practice, 3D printing orders vary widely (model size, material, resolution, multi-color requirements, and post-processing such as painting). If pricing and scheduling rely on manual judgment, it is easy to underestimate time/cost, assign the wrong machine, or create bottlenecks in the order queue. Therefore, this system uses a relational database to store orders, machine capabilities, material usage, and scheduling states in a structured way, enabling fast and consistent decisions through rules and queries.\n\n\n\nOrder intake & requirement tagging (Order Intake & Requirement Tagging)\nWhen a customer submits an order, the system stores it as an order record with structured attributes, such as:\n\nModel size and volume (bounding box / volume)\nPrinting type (FDM / SLA)\nResolution settings (layer height / resolution)\nMulti-color requirement (multi-color)\nMaterial type (material type)\nPost-processing needs (post-processing, e.g., painting/sanding)\nOther customization requests (stored as tags)\n\nThese fields can be normalized into multiple tables, with many-to-many relationships used to represent that a single order can have multiple requirement tags.\nPer-machine estimation (Per-Machine Estimation)\nThe key is not only to calculate an overall price for the order, but to estimate how the same order would perform on different machines, since time, cost, and completion time may vary by machine. This supports better machine assignment and scheduling decisions.\nFor each candidate machine, the system applies pricing rules or an estimation model to perform per-machine estimation, including:\n\nEstimated print time (estimated print time)\nEstimated material usage (estimated material usage)\nMachine-specific estimated cost & quote (machine-specific estimated cost & quote)\nEstimated completion time (estimated completion time, considering current workload)\n\nThe system stores these â€œorder Ã— machineâ€ estimates for querying and ranking using different objective functions, such as lowest cost, earliest completion, or the most stable option within a deadline.\nOrder queue & status tracking (Order Queue & Status Tracking)\nAll orders are automatically added to an order queue (order list), and each order maintains a clear status, such as:\n\npending\nqueued\nprinting\npost-processing\ncompleted\nfailed\n\nManagers can query:\n\nWhat is currently in the queue and its priority\nWhich orders are printing vs.Â waiting for machines\nWhich failed orders require reprinting or manual intervention\n\nMachine capability modeling & assignment recommendations (Machine Capability & Assignment)\nThe database stores each machineâ€™s capabilities and constraints, such as: - Machine type: multi-color / single-color / SLA / FDM - Maximum build volume (max build volume) - Supported materials (supported materials) - Speed/quality profile (speed/quality profile) - Current workload and availability (workload & availability)\nWhen a new order arrives, the system first performs constraint filtering (e.g., size, material, multi-color requirements) to identify feasible machines, then uses per-machine estimation to generate recommended assignments, for example:\n\nEarliest completion time (earliest completion time)\nLowest estimated cost (lowest estimated cost)\nBalanced option (deadline + stability)\n\nThis turns scheduling into a decision-support process rather than manual guesswork.\n\n\n\nOn the customer side, the system provides a customer order page where users can upload a 3D model or specify printing requirements such as size, material, resolution, multi-color options, and post-processing needs. Based on this information, the system automatically returns an estimated price and an estimated delivery time.\nOn the admin side, the system offers an order dashboard that displays the current order queue and order statuses. Administrators can sort or filter orders by deadline, priority, or processing status to manage workflow more efficiently.\nThe system also includes a machine dashboard that lists all available machines along with their machine type, maximum build volume, supported materials, current workload, and estimated availability. This allows operators to quickly understand machine capacity and constraints.\nWhen an order is selected, the scheduling view presents a list of candidate machines that can fulfill the order. For each candidate machine, the system displays the estimated print time, estimated material usage, machine-specific cost and quote, and estimated completion time. The interface supports one-click sorting options, such as fastest, cheapest, or most stable, to assist administrators in making assignment decisions.\nThe interface supports efficient operations: submit order â†’ per-machine estimation â†’ queue order â†’ recommend machines â†’ schedule & track progress.\n\n\n\n\n\n\nThe purpose of this system is to manage the core information of a farmâ€”such as fields, crop types, growth stages, and irrigation equipmentâ€”using a relational database.\nAt the same time, the system retrieves and stores weather data through APIs provided by weather forecast services, and combines this information with a set of irrigation rules to automatically generate a daily irrigation schedule.\nThe goal is to reduce manual decision-making costs while improving water-use efficiency and consistency in crop management.\n\n\n\nIn this system, the database is not used only for data storage. Its core function is to integrate internal farm information with external weather data and automatically generate irrigation decisions based on predefined rules.\nCore Data Management\nThe system uses a relational schema to manage the main entities of the farm, including:\n\nField: field ID, location, area, and the crop currently planted\nCrop: crop type and its basic water requirements\nGrowth Stage: stages such as germination, growth, flowering, and fruiting, each with different water needs\nIrrigation Equipment: equipment type (e.g., drip irrigation, sprinkler), flow rate or efficiency factor, and availability status\n\nThese entities are connected through relationships. For example, each field is associated with a specific crop and a current growth stage, and can be assigned available irrigation equipment.\nWeather Data Integration\nThe system retrieves weather information through external weather forecast APIs, such as:\n\nPredicted rainfall amount\nProbability of precipitation\nTemperature range\n\nThis weather data is stored in the database and used as an important input for daily irrigation decisions, without requiring manual input from users.\nIrrigation Rules and Schedule Generation\nThe system maintains a set of irrigation rules that describe irrigation requirements under different conditions, such as:\n\nCrop type Ã— growth stage â†’ recommended baseline irrigation amount\nIf predicted rainfall exceeds a certain threshold â†’ automatically reduce or cancel irrigation for the day\nDifferences in irrigation equipment efficiency â†’ adjust actual irrigation duration\n\nWhen the daily scheduling process runs, the system combines:\n\nThe crop type and growth stage of each field\nThe weather forecast for the day\nThe availability and efficiency of irrigation equipment\n\nBased on this information, the system automatically generates a daily irrigation schedule, indicating whether each field requires irrigation and the recommended water amount or irrigation time.\n\n\n\nWhen users enter the system, the home page displays a table view of all fields on the farm, including the current crop type, growth stage, and the systemâ€™s irrigation recommendation for the day.\nUsers can generate the daily irrigation schedule with a single action. Based on field information, weather forecasts, and irrigation rules, the system lists which fields require irrigation and provides recommended water amounts or irrigation durations.\nThe schedule is presented in a simple list format, allowing users to quickly review and execute irrigation tasks. After completion, users can mark irrigation status for record-keeping and future reference.\n\n\n\n\n\nWhat are the things current database system cannot do?\nCurrent database systems are not capable of understanding the semantics behind data. As a result, in more complex applications, they often rely on manually defined rules or continuously adjusted weights to produce reasonable outputs. In addition, databases are limited in handling cross-context decision-making, where multiple competing objectives must be balanced simultaneously.\nFor example, in a wardrobe management database, the system can evaluate outfits based on structured criteria such as color combinations, style tags, material properties, and weather conditions. It can assign scores for factors like aesthetic quality, comfort, and climate suitability, and generate multiple candidate outfits that satisfy predefined rules. However, the database cannot determine which outfit represents the optimal balance among being visually appealing, comfortable, and suitable for the weather.\nThis limitation arises because preferences such as â€œlooking goodâ€ or â€œfeeling comfortableâ€ are inherently subjective and context-dependent, and there is no single optimal solution that applies to all users or situations. Therefore, the role of the database is not to make the final decision, but to support decision-making by filtering infeasible options, structuring relevant information, and presenting comparable alternatives with transparent evaluation metrics.\nUltimately, the final choice must be made by the user, who can decide whether to prioritize comfort, aesthetics, or climate suitability in a given context. This highlights a fundamental limitation of current database systems: they are effective at decision support, but they cannot replace human judgment in complex, value-driven decisions.\n\n\n\nDescribe at least three tables that might be used to store information in a social-network/social media system such as Twitter or Reddit.\nA social-network or social media system such as Twitter or Reddit may be supported by at least the following three core tables:\n1. User Table\nThe user table stores basic information about users, such as: - user_id - username - account creation time - profile metadata (e.g., bio or status)\nThis table represents the identities of users and serves as a reference for other tables in the system.\n2. Post Table\nThe post table stores content created by users, such as:\n\npost_id\nauthor_id (foreign key referencing the User table)\ncontent\ntimestamp\n\nEach post is associated with a specific user, forming a one-to-many relationship between users and posts.\n3. Comment Table\nThe comment table stores replies to posts (or other comments), such as:\n\ncomment_id\npost_id (foreign key referencing the Post table)\nauthor_id\ncontent\ntimestamp\n\nThis table supports threaded discussions and allows multiple users to participate in conversations under the same post.\nThese tables are separated to support relational queries, maintain data consistency, and enable efficient retrieval of users, posts, and discussion threads.\n\n\n\n\n\n\nWhat are the differences between relation schema, relation, and instance? Give an example using the university database to illustrate.\n\nRelation Schema = The logical structure of a relation: a list of attribute names and their domains. It does not change over time.\nExample: instructor(ID, name, dept_name, salary)\nRelation = Informally used to refer to both the schema and instance together.\nExample: â€œThe department relationâ€ can refer to either the schema department(dept_name, building, budget) or the actual data it currently holds.\nInstance = A snapshot of the actual data in a relation at a given point in time. It changes as tuples are inserted, updated, or deleted.\nExample: The department relation instance in Figure 2.5 contains 7 tuples. If the university adds a â€œData Scienceâ€ department, the instance grows to 8 tuples, but the schema remains department(dept_name, building, budget).\n\n\n\n\nDraw a schema diagram for the following bank database. Identify primary keys (underlined) and foreign keys.\nThe bank database consists of the following relations:\n\nbranch(branch_name, branch_city, assets)\ncustomer(ID, customer_name, customer_street, customer_city)\nloan(loan_number, branch_name, amount)\nborrower(ID, loan_number)\naccount(account_number, branch_name, balance)\ndepositor(ID, account_number)\n\n\n\n\nBank Database Schema Diagram\n\n\n\n\n\nDescribe two ways artificial intelligence or LLM can assist in managing or querying a database. In your answer, briefly explain how each method improves efficiency or accuracy compared to traditional (non-AI) approaches. (3â€“5 sentences)\n\nNatural Language to SQL (Querying): LLMs can translate plain language questions directly into executable SQL queries, lowering the barrier for non-technical users and reducing syntax errors compared to writing SQL manually.\nAI-Driven Database Tuning (Managing): LLMs can automatically analyze slow queries and recommend index optimizations, replacing the traditionally time-consuming process of a DBA manually examining query logs and execution plans.\n\nOverall, both approaches reduce the need for specialized expertise and allow faster, more accurate database operations compared to traditional manual methods.\n\n\n\n\n\n\nOpen the Online SQL interpreter and load the university database.\n\n\n\nWrite SQL codes to get a list of: i. Student IDs, ii. Instructors, iii. Departments\n\n\n\nQ2 â€” Student IDs (from takes), Instructors, and Departments\n\n\n\n\n\nWrite SQL codes to do the following queries:\ni. Find the ID and name of each student who has taken at least one Comp. Sci. course; make sure there are no duplicate names in the result.\n\n\n\nQ3i â€” Students who took at least one Comp. Sci. course\n\n\nii. Add grades to the list\n\n\n\nQ3ii â€” Add grades to the result\n\n\niii. Find the ID and name of each student who has not taken any course offered before 2017.\n\n\n\nQ3iii â€” Students who have not taken any course before 2017\n\n\niv. For each department, find the maximum salary of instructors in that department.\n\n\n\nQ3iv â€” Maximum instructor salary per department\n\n\nv. Find the lowest, across all departments, of the per-department maximum salary computed by the preceding query.\n\n\n\nQ3v â€” Lowest of the per-department maximum salaries\n\n\nvi. Add names to the list\n\n\n\nQ3vi â€” Add instructor names to the result\n\n\n\n\n\nFind instructor (with name and ID) who has never given an A grade in any course she or he has taught. (Instructors who have never taught a course trivially satisfy this condition.)\n\n\n\nQ4 â€” Instructors who have never given an A grade"
  },
  {
    "objectID": "EPPS6354.html#final-project",
    "href": "EPPS6354.html#final-project",
    "title": "Information Management",
    "section": "Final Project",
    "text": "Final Project\n\nWeather- and Occasion-Aware Wardrobe Database with Rule-Based Outfit Recommendation\n\nProject Overview\nA single-user wardrobe management system. Each morning, the system reads the userâ€™s calendar (to determine the occasion) and the current weather, filters out unwearable items (dirty or archived), and ranks candidate outfits using a rule-based scoring engine built on color theory, fabric compatibility, and style coherence. The output is the best recommended outfit plus 3â€“5 ranked alternatives, each with a score and explanation.\nCore workflow: Wake up â†’ Read calendar (occasion) + weather â†’ Filter wearable items â†’ Score outfit combinations â†’ Output recommendation\nThree scoring dimensions:\n\nColor â€” 60/30/10 color theory; each item carries a primary / secondary / accent color role\nFabric â€” same-fabric bonus, mixed-fabric reasonableness, warmth adequacy relative to weather\nStyle â€” style-tag consistency across items, and styleâ€“occasion fit scores\n\n\n\nDatabase Tables\n\nReference Tables\n\n\n\n\n\n\n\n\nTable\nKey Columns\nDescription\n\n\n\n\nUser\nuser_id PK, name\nStores user identity\n\n\nCategory\ncategory_id PK, name\nClothing categories: top, bottom, shoes, outerwear, accessory\n\n\nColor\ncolor_id PK, name\nColor names (black, white, navy, grey, beigeâ€¦)\n\n\nFabric\nfabric_id PK, name, warmth_weight, breathability\nFabric type with warmth (0â€“100) and breathability (0â€“100) ratings\n\n\nStyleTag\nstyle_id PK, name\nStyle labels: street, formal, clean fit, simple, blokecoreâ€¦\n\n\nOccasion\noccasion_id PK, name, target_formality_min, target_formality_max\nOccasion with required formality range\n\n\n\n\n\nStyle & Calendar\n\n\n\n\n\n\n\n\nTable\nKey Columns\nDescription\n\n\n\n\nStyleOccasionFit\nstyle_id FK, occasion_id FK, fit_score\nFit score (0â€“100) between a style and an occasion (PK: style_id + occasion_id)\n\n\nCalendarEvent\nevent_id PK, user_id FK, occasion_id FK, event_date, start_time\nCalendar entry mapped to an occasion; drives automatic occasion detection\n\n\n\n\n\nClothing Item & Tags\n\n\n\n\n\n\n\n\nTable\nKey Columns\nDescription\n\n\n\n\nClothingItem\nitem_id PK, user_id FK, category_id FK, fabric_id FK, formality_score, warmth_score, clean_score, status\nMain clothing table; recommendable only if clean_score &gt; 0 and status = 'active'\n\n\nItemStyle\nitem_id FK, style_id FK\nMany-to-many: clothing item to style tags\n\n\nItemColor\nitem_id FK, color_id FK, role (primary/secondary/accent)\nColor role assignment per item; max 3 colors per item\n\n\n\n\n\nWeather\n\n\n\n\n\n\n\n\nTable\nKey Columns\nDescription\n\n\n\n\nWeatherSnapshot\nweather_id PK, location, temp_c, feels_like_c, humidity, wind_speed, precip_mm\nReal-time weather data snapshot\n\n\nWeatherCondition\ncondition_id PK, name, temp/precip/wind/humidity ranges\nNamed weather condition (e.g.Â cold_rainy, hot_humid) used for rule matching\n\n\n\n\n\nOutfit\n\n\n\n\n\n\n\n\nTable\nKey Columns\nDescription\n\n\n\n\nOutfit\noutfit_id PK, user_id FK, occasion_id FK, weather_id FK, total_score, explanation\nGenerated outfit with total score and explanation\n\n\nOutfitItem\nPK(outfit_id, slot, layer_order), item_id FK\nOutfit detail; supports multi-layer dressing within the same slot\n\n\n\n\n\nScoring Rules\n\n\n\n\n\n\n\n\nTable\nKey Columns\nDescription\n\n\n\n\nOutfitScoringRule\nscore_rule_id PK, rule_type, score_delta, condition_json, valid_from, valid_until\nScoring rule with optional expiry date to support trend-based rules\n\n\nWearDegradeRule\ndegrade_rule_id PK, category_id FK, occasion_id FK (nullable), condition_id FK (nullable), delta_clean_score\nClean score deduction rule; multiple matching rules are stacked per wear event\n\n\n\n\n\nWear Tracking & Laundry\n\n\n\n\n\n\n\n\nTable\nKey Columns\nDescription\n\n\n\n\nWearEvent\nevent_id PK, user_id FK, outfit_id FK, occasion_id FK, weather_id FK, worn_at\nRecords each time an outfit is worn\n\n\nWearEventItem\nPK(event_id, item_id), delta_applied, clean_score_after\nPer-item deduction detail and resulting clean score after wear\n\n\nLaundryBatch\nbatch_id PK, user_id FK, laundry_type (dark/light)\nLaundry batch grouped by color type\n\n\nLaundryBatchItem\nPK(batch_id, item_id), reset_to_score = 100\nResets clean score to 100 upon laundering"
  },
  {
    "objectID": "DataAnalysis.html#hw1-svd-image-compression-svd-å½±åƒå£“ç¸®",
    "href": "DataAnalysis.html#hw1-svd-image-compression-svd-å½±åƒå£“ç¸®",
    "title": "Data Analysis Mathematics",
    "section": "HW1 â€” SVD Image Compression / SVD å½±åƒå£“ç¸®",
    "text": "HW1 â€” SVD Image Compression / SVD å½±åƒå£“ç¸®\nTopic / ä¸»é¡Œï¼š ä»¥ SVD åˆ†è§£é©—è­‰ Eckart-Young å®šç†ï¼š\\(\\|A - A_k\\|_2 = \\sigma_{k+1}\\)\nDatasetï¼š å½©è‰²ç…§ç‰‡ 1546Ã—1029 pixelsï¼ˆRGB ä¸‰é€šé“ï¼‰\n\nPipeline Overview / æ•´é«”æµç¨‹\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35}}}%%\nflowchart TD\n    A[\"Load Image 1546Ã—1029Ã—3     \"] --&gt; B[\"Split R, G, B     \"]\n    B --&gt; C[\"np.linalg.svd per channel     \"]\n    C --&gt; D[\"Reconstruct A_k for k = 1..1029     \"]\n    D --&gt; E[\"Compute MSE, PSNR, 2-norm     \"]\n    E --&gt; F[\"Compare 2-norm vs Ïƒ_k+1     \"]\n\n    style A fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style B fill:#F5F5F5,color:#424242,stroke:#BDBDBD,stroke-width:2px\n    style C fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style D fill:#F3E5F5,color:#6A1B9A,stroke:#CE93D8,stroke-width:2px\n    style E fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style F fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n\n\n\n\n\n\n\næ–¹æ³•èªªæ˜ï¼š ä»¥ç…§ç‰‡ä½œç‚ºçŸ©é™£ Aï¼Œå° RGB ä¸‰é€šé“åˆ†åˆ¥åš SVDã€‚åˆ»æ„ä¸ä½¿ç”¨ np.linalg.norm(ord=2)ï¼ˆå› ç‚ºå…¶å…§éƒ¨å¯¦ä½œå°±æ˜¯ SVDï¼Œæœƒé€ æˆå¾ªç’°è«–è­‰ï¼‰ï¼Œæ”¹ç”¨ Monte Carlo éš¨æ©Ÿå‘é‡æ³• è¿‘ä¼¼ operator 2-normï¼Œå†èˆ‡ Ïƒ_{k+1} æ¯”è¼ƒè¶¨å‹¢ã€‚\n\n\n\nKey Formulas / é—œéµå…¬å¼\nSVD åˆ†è§£ï¼š\n\\[A = U \\Sigma V^T, \\quad A_k = \\sum_{i=1}^{k} \\sigma_i u_i v_i^T\\]\nEckart-Young å®šç†ï¼š\n\\[\\|A - A_k\\|_2 = \\sigma_{k+1}\\]\n\n\nResults / çµæœ\n\n\n\nk\n2-norm (approx)\nÏƒ_{k+1}\nMSE\nPSNR (dB)\n\n\n\n\n1\n3,310\n34,072\n3,363\n12.95\n\n\n10\n1,556\n10,116\n1,403\n16.77\n\n\n50\n837\n3,645\n487\n21.35\n\n\n100\n577\n2,108\n241\n24.37\n\n\n300\n234\n699\n42\n31.88\n\n\n500\n116\n340\n10\n38.04\n\n\n700\n54\n172\n2.2\n44.70\n\n\n\n\n\nReconstruction Quality / é‡å»ºå“è³ª\n\n\n\nSVD å½±åƒé‡å»º â€” ä¸åŒ k å€¼çš„å£“ç¸®æ•ˆæœ\n\n\n\nè¦–è¦ºåŒ–ï¼š k=1 æ™‚å¹¾ä¹çœ‹ä¸å‡ºåŸåœ–ï¼›k=50 é–‹å§‹å¯è¾¨è­˜ä¸»è¦è¼ªå»“ï¼›k=300 ä»¥ä¸Šæ¥è¿‘åŸåœ–ã€‚å³ä¸‹æ–¹ k=507 æ™‚ PSNR å·²é” 38 dBï¼Œè‚‰çœ¼å¹¾ä¹ç„¡æ³•åˆ†è¾¨èˆ‡åŸåœ–å·®ç•°ã€‚\n\n\n\nMSE / PSNR / 2-norm Analysis\n\n\n\nMSEã€PSNRã€2-norm vs Ïƒ_{k+1} éš¨ k å€¼è®ŠåŒ–\n\n\n\nçµè«–ï¼š MSE éš¨ k éæ¸›ã€PSNR éå¢ï¼Œä¸” 2-norm èˆ‡ Ïƒ_{k+1} å‘ˆç¾ç›¸åŒéæ¸›è¶¨å‹¢ï¼Œé–“æ¥é©—è­‰ Eckart-Young å®šç†ã€‚Monte Carlo æ³•å› éš¨æ©ŸæŠ½æ¨£æœƒä½ä¼°çœŸå¯¦ 2-normï¼Œä½†è¶¨å‹¢ä¸€è‡´ã€‚"
  },
  {
    "objectID": "DataAnalysis.html#hw2-handwritten-digit-recognition-æ‰‹å¯«æ•¸å­—è¾¨è­˜",
    "href": "DataAnalysis.html#hw2-handwritten-digit-recognition-æ‰‹å¯«æ•¸å­—è¾¨è­˜",
    "title": "Data Analysis Mathematics",
    "section": "HW2 â€” Handwritten Digit Recognition / æ‰‹å¯«æ•¸å­—è¾¨è­˜",
    "text": "HW2 â€” Handwritten Digit Recognition / æ‰‹å¯«æ•¸å­—è¾¨è­˜\nTopic / ä¸»é¡Œï¼š æ¯”è¼ƒ 8 ç¨®åˆ†é¡æ–¹æ³•åœ¨ USPS æ‰‹å¯«æ•¸å­—è³‡æ–™é›†ä¸Šçš„è¡¨ç¾\nDatasetï¼š USPS æ‰‹å¯«æ•¸å­—ï¼ˆ16Ã—16 ç°éšï¼Œ0-9 å…± 10 é¡ï¼‰\n\nDataset Samples / è³‡æ–™é›†æ¨£æœ¬\n\n\n\nUSPS è¨“ç·´é›†æ¨£æœ¬ â€” 16Ã—16 ç°éšæ‰‹å¯«æ•¸å­—\n\n\n\nè³‡æ–™ç‰¹å¾µï¼š æ¯å¼µåœ–ç‰‡åƒ… 16Ã—16 = 256 pixelsï¼Œè§£æåº¦æ¥µä½ä½†ä¿ç•™äº†æ•¸å­—çš„åŸºæœ¬çµæ§‹ã€‚é€™ä½¿å¾—çŸ©é™£åˆ†è§£æ–¹æ³•ï¼ˆSVD / HOSVDï¼‰èƒ½å¤ æœ‰æ•ˆæ•æ‰ä½ç¶­ç‰¹å¾µã€‚\n\n\n\nMethods / æ–¹æ³•ç¸½è¦½\n\n\n\nCategory\nMethod\nDescription\n\n\n\n\nBaseline\nMean Method\nè¨ˆç®—æ¯é¡å¹³å‡å½±åƒï¼Œä»¥æ­æ°è·é›¢åˆ†é¡\n\n\n\nEnhanced Mean\nè¿­ä»£ä¿®æ­£ï¼Œè€ƒæ…®é¡é–“æ··æ·†æ¨¡å¼\n\n\nçŸ©é™£åˆ†è§£\nSVD (k=20)\nå»ºç«‹æ¯é¡ SVD åŸºåº•ï¼ŒæŠ•å½±æ®˜å·®åˆ†é¡\n\n\n\nHOSVD\nTucker åˆ†è§£ (tensorly)ï¼Œrank (10,10,20)\n\n\nå‚³çµ± ML\nSVM\nsklearn SVC\n\n\n\nKNN\nsklearn KNeighborsClassifier\n\n\n\nRandom Forest\nsklearn RandomForestClassifier\n\n\næ·±åº¦å­¸ç¿’\nCNN (PyTorch)\nConv2D â†’ MaxPool â†’ Denseï¼Œ50 epochs\n\n\n\n\n\nAccuracy Comparison / æº–ç¢ºç‡æ¯”è¼ƒ\n\n\n\n\n\n\n\n\n\n\nçµæœåˆ†æï¼š Mean æ–¹æ³•åƒ… 81.42% ä½œç‚º baselineã€‚SVD (k=20) è·³å‡è‡³ 94.12%ï¼Œèªªæ˜ä½ç§©è¿‘ä¼¼èƒ½æœ‰æ•ˆæ•æ‰æ•¸å­—çµæ§‹ã€‚å‚³çµ± MLï¼ˆSVM 94.92%ï¼‰èˆ‡ SVD ç³»åˆ—è¡¨ç¾æ¥è¿‘ã€‚CNN ä»¥ 95.76% å‹å‡ºï¼Œä½†è¨“ç·´æ™‚é–“ç‚º 98 ç§’ï¼Œé é«˜æ–¼ KNN çš„ 0.45 ç§’ã€‚\n\n\n\nTraining Time Comparison / è¨“ç·´æ™‚é–“æ¯”è¼ƒ\n\n\n\n\n\n\n\n\n\n\né€Ÿåº¦ vs ç²¾åº¦ï¼š KNN ä»¥ 0.45 ç§’é”åˆ° 94.47%ï¼Œæ˜¯æ€§åƒ¹æ¯”æœ€é«˜çš„æ¨¡å‹ã€‚CNN é›–ç„¶æœ€æº–ï¼ˆ95.76%ï¼‰ä½†è€—æ™‚ 98 ç§’ã€‚HOSVDï¼ˆTucker åˆ†è§£ï¼‰ç²¾åº¦èˆ‡ SVD æ¥è¿‘ä½†æ…¢äº† 30 å€ã€‚\n\n\n\nSVD Confusion Matrix\n\n\n\nSVD (k=20) Confusion Matrix â€” USPS æ‰‹å¯«æ•¸å­—åˆ†é¡\n\n\n\næ··æ·†çŸ©é™£åˆ†æï¼š SVD æ–¹æ³•å° 0ã€1 çš„è¾¨è­˜æœ€ä½³ï¼ˆ353/264 æ­£ç¢ºï¼‰ï¼Œå° 3ã€7 çš„æ··æ·†è¼ƒé«˜ï¼ˆ3â†’5 æœ‰ 13 ç­†èª¤åˆ¤ï¼‰ã€‚æ•´é«” 94.12% çš„æº–ç¢ºç‡åœ¨ç„¡éœ€è¨“ç·´çš„çŸ©é™£åˆ†è§£æ–¹æ³•ä¸­è¡¨ç¾å„ªç•°ã€‚\n\n\n\nMisclassified Samples / èª¤åˆ¤æ¨£æœ¬\n\n\n\nMean Method èª¤åˆ¤æ¡ˆä¾‹ â€” True label vs Predicted label\n\n\n\nèª¤åˆ¤åˆ†æï¼š é€™äº›æ¨£æœ¬å³ä½¿æ˜¯äººçœ¼ä¹Ÿä¸å®¹æ˜“è¾¨è­˜ã€‚ä¾‹å¦‚ True: 6 è¢«åˆ¤æˆ 2ï¼ˆç­†ç•«åœ“å¼§ç›¸ä¼¼ï¼‰ã€True: 3 è¢«åˆ¤æˆ 8ï¼ˆå½¢ç‹€æ¥è¿‘ï¼‰ã€‚ä½è§£æåº¦ 16Ã—16 ä¸‹ï¼Œéƒ¨åˆ†æ•¸å­—çš„çµæ§‹å·®ç•°æ¥µå°ã€‚\n\n\n\nReal Handwriting Test / æ‰‹å¯«å¯¦æ¸¬\n\n\n\nå¤–éƒ¨æ‰‹å¯«æ•¸å­—ç…§ç‰‡æ¨£æœ¬\n\n\n\n\n\n8 ç¨®æ¨¡å‹åœ¨å¤–éƒ¨æ‰‹å¯«ç…§ç‰‡ä¸Šçš„æº–ç¢ºç‡\n\n\n\nå¤–éƒ¨é©—è­‰ï¼š ç”¨æ‰‹æ©Ÿæ‹æ”æ‰‹å¯«æ•¸å­—ç…§ç‰‡æ¸¬è©¦æ‰€æœ‰æ¨¡å‹ã€‚CNN é”åˆ° 100% å®Œç¾è¾¨è­˜ï¼ŒRandom Forest 95%ï¼Œè€Œ Enhanced Mean åƒ… 56%ã€‚é€™é©—è­‰äº†æ·±åº¦å­¸ç¿’å°çœŸå¯¦ä¸–ç•Œæ‰‹å¯«è®Šç•°çš„æ³›åŒ–èƒ½åŠ›æœ€å¼·ã€‚"
  },
  {
    "objectID": "AM.html",
    "href": "AM.html",
    "title": "BS of Applied Mathematics",
    "section": "",
    "text": "å­¸æ­·ï¼š åœ‹ç«‹ä¸­èˆˆå¤§å­¸ æ‡‰ç”¨æ•¸å­¸ç³» å­¸å£« (B.S. Applied Mathematics, NCHU, 2020 â€“ 2024)"
  },
  {
    "objectID": "AM.html#about-the-program",
    "href": "AM.html#about-the-program",
    "title": "BS of Applied Mathematics",
    "section": "About the Program",
    "text": "About the Program\nThe Department of Applied Mathematics at National Chung Hsing University provides a solid foundation in mathematical theory and its applications. The curriculum covers calculus, linear algebra, differential equations, probability and statistics, numerical analysis, and optimization â€” preparing students for advanced studies and careers in data science, finance, engineering, and scientific computing."
  },
  {
    "objectID": "AM.html#research",
    "href": "AM.html#research",
    "title": "BS of Applied Mathematics",
    "section": "Research",
    "text": "Research\n\n\nQuantum Bayesian Inference\nç§‘æŠ€éƒ¨å¤§å°ˆç”Ÿç ”ç©¶è¨ˆç•« (NSTC 112-2813-C-005-038-E)\nInvestigated whether quantum computer outputs conform to Bayesian probability laws. Developed Qiskit programs on IBMQ hardware and simulators to test the law of large numbers, revealing divergence between actual quantum runs and simulator predictions.\nTools: Python, Qiskit, IBMQ (ibm_osaka), Bayesian Updating, matplotlib"
  },
  {
    "objectID": "AM_thesis.html",
    "href": "AM_thesis.html",
    "title": "Quantum Bayesian Inference",
    "section": "",
    "text": "ç§‘æŠ€éƒ¨å¤§å°ˆç”Ÿç ”ç©¶è¨ˆç•« (NSTC Undergraduate Research Project) Project No.Â 112-2813-C-005-038-E Â· Advisor: Prof.Â æˆ´æ·¯ç® Â· Period: 2023/07 â€“ 2024/02"
  },
  {
    "objectID": "AM_thesis.html#overview",
    "href": "AM_thesis.html#overview",
    "title": "Quantum Bayesian Inference",
    "section": "Overview",
    "text": "Overview\nThis thesis investigated whether quantum computer outputs conform to Bayesian probability laws. Using IBMâ€™s Qiskit framework on real IBMQ hardware and simulators, we tested the law of large numbers for quantum bit measurements and applied Bayesian updating to analyze qubit measurement outcomes.\nThe key finding: real quantum computers and simulators behave differently â€” the quantum computerâ€™s cumulative probability showed a systematic drift away from the theoretical 0.5, raising fundamental questions about objective randomness in quantum states."
  },
  {
    "objectID": "AM_thesis.html#methodology",
    "href": "AM_thesis.html#methodology",
    "title": "Quantum Bayesian Inference",
    "section": "Methodology",
    "text": "Methodology\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"primaryColor\": \"#E3F2FD\", \"primaryTextColor\": \"#111\", \"primaryBorderColor\": \"#90CAF9\", \"lineColor\": \"#18A3A3\", \"secondaryColor\": \"#FFF3E0\", \"tertiaryColor\": \"#F3E5F5\"}, \"flowchart\": {\"curve\": \"basis\", \"padding\": 35}, \"fontSize\": \"18px\"}}%%\nflowchart TD\n    A[\"Phase 1: Law of Large Numbers     \"] --&gt; B[\"Build H-gate Quantum Circuit     \"]\n    B --&gt; C[\"Run on Simulator (20,000 shots)     \"]\n    B --&gt; D[\"Run on IBMQ Hardware (20,000 shots)     \"]\n    C --&gt; E[\"Cumulative P(0) Visualization     \"]\n    D --&gt; E\n    E --&gt; F[\"Compare Convergence Behavior     \"]\n    F --&gt; G[\"Phase 2: Bayesian Inference     \"]\n    G --&gt; H[\"Collect Measurement Results as CSV     \"]\n    H --&gt; I[\"Compute Prior from First 50 Observations     \"]\n    I --&gt; J[\"Iterative Bayesian Updating (20,000 rounds)     \"]\n    J --&gt; K[\"Find MAP Estimate of P(0)     \"]\n\n    style A fill:#E3F2FD,stroke:#90CAF9\n    style B fill:#E3F2FD,stroke:#90CAF9\n    style C fill:#E8F5E9,stroke:#A5D6A7\n    style D fill:#FFF3E0,stroke:#FFCC80\n    style E fill:#F3E5F5,stroke:#CE93D8\n    style F fill:#F3E5F5,stroke:#CE93D8\n    style G fill:#FFF3E0,stroke:#FFCC80\n    style H fill:#FFF3E0,stroke:#FFCC80\n    style I fill:#FFF3E0,stroke:#FFCC80\n    style J fill:#FFF3E0,stroke:#FFCC80\n    style K fill:#FCE4EC,stroke:#F48FB1"
  },
  {
    "objectID": "AM_thesis.html#phase-1-law-of-large-numbers-verification",
    "href": "AM_thesis.html#phase-1-law-of-large-numbers-verification",
    "title": "Quantum Bayesian Inference",
    "section": "Phase 1: Law of Large Numbers Verification",
    "text": "Phase 1: Law of Large Numbers Verification\nWe applied a Hadamard gate (H-gate) to a single qubit, putting it into superposition state \\(|+\\rangle = \\frac{1}{\\sqrt{2}}(|0\\rangle + |1\\rangle)\\), then measured it 20,000 times. If quantum measurements follow classical probability, the cumulative \\(P(|0\\rangle)\\) should converge to 0.5.\n\n\n\n\n\n\n\n\n\nSetup\nBackend\nShots\nExpected \\(P(0)\\)\n\n\n\n\nH-gate \\(\\rightarrow\\) Measure\nibmq_qasm_simulator\n20,000\n0.5\n\n\nH-gate \\(\\rightarrow\\) Measure\nibm_osaka (real hardware)\n20,000\n0.5\n\n\n\n\nSimulator Result\nThe simulatorâ€™s cumulative probability converges smoothly to 0.5, perfectly following the law of large numbers:\n\n\n\nSimulator: Cumulative P(0) over 20,000 measurements â€” converges to 0.5\n\n\n\n\nReal Quantum Computer Result\nThe real quantum computer (IBM Osaka) shows a systematic downward drift â€” the cumulative probability does NOT converge to 0.5:\n\n\n\nIBM Osaka: Cumulative P(0) over 20,000 measurements â€” drifts below 0.5\n\n\n\nKey Finding: The simulator perfectly follows the law of large numbers, but the real quantum computer exhibits a persistent bias. This divergence suggests that either: (1) hardware noise introduces systematic error, (2) the sample size is insufficient, or (3) quantum superposition may not produce truly objective randomness.\n\n\n\nZoomed-in Comparison (1,000 shots)\nA closer look at individual runs with 1,000 measurements each:\n\n\n\n\n\nSimulator: 1,000 shots â€” tight convergence to 0.5\n\n\n\nSimulator: Rapid, clean convergence to the theoretical 0.5 line.\n\n\n\n\n\nQuantum Computer: 1,000 shots â€” wider oscillation around 0.5\n\n\n\nIBMQ: Noticeably wider variance and slower convergence, with persistent oscillation."
  },
  {
    "objectID": "AM_thesis.html#phase-2-bayesian-inference-on-quantum-outputs",
    "href": "AM_thesis.html#phase-2-bayesian-inference-on-quantum-outputs",
    "title": "Quantum Bayesian Inference",
    "section": "Phase 2: Bayesian Inference on Quantum Outputs",
    "text": "Phase 2: Bayesian Inference on Quantum Outputs\nIn the second phase, we treated the quantum measurement outcomes as observed data and applied Bayesian updating to infer the most likely probability parameter \\(\\theta = P(|0\\rangle)\\).\nSteps:\n\nCollected 20,000 measurement results from IBMQ as a binary sequence (0/1)\nUsed the first 50 observations to construct the prior distribution over \\(\\theta\\)\nIteratively applied Bayesâ€™ theorem for 19,950 rounds of updating\nIdentified the MAP (Maximum A Posteriori) estimate of \\(\\theta\\)\n\n\nBayesian Convergence\n\n\n\nBayesian updating: cumulative probability (blue) vs Bayesian posterior mode (red) â€” both converge, but the posterior MAP drifts toward ~0.483 instead of 0.5\n\n\n\nBayesian Result: With prior density \\(n = 50\\), the MAP estimate converged to \\(\\theta \\approx 0.483\\) with posterior probability 0.955. This confirms the quantum hardwareâ€™s systematic bias â€” Bayesian inference correctly identifies that the true \\(P(|0\\rangle)\\) on real hardware is NOT exactly 0.5."
  },
  {
    "objectID": "AM_thesis.html#conclusions",
    "href": "AM_thesis.html#conclusions",
    "title": "Quantum Bayesian Inference",
    "section": "Conclusions",
    "text": "Conclusions\n\n\n\n\n\n\n\n\nAspect\nSimulator\nReal Quantum Computer\n\n\n\n\nLaw of Large Numbers\nPerfectly satisfied\nViolated (systematic drift)\n\n\nConvergence to 0.5\nYes (smooth)\nNo (biased toward ~0.49)\n\n\nBayesian MAP estimate\n\\(\\theta \\approx 0.500\\)\n\\(\\theta \\approx 0.483\\)\n\n\nVariance\nLow\nHigh (noisy)\n\n\n\nThree possible explanations for the divergence:\n\nInsufficient sample size â€” 20,000 shots may not be enough for quantum hardware (limited by IBMQ queue constraints)\nHardware noise and error â€” current IBMQ devices have gate errors, decoherence, and readout errors that introduce systematic bias\nQuantum randomness is not classical randomness â€” the superposition state may follow measurement-theoretic probability models (e.g., Gleasonâ€™s theorem) that donâ€™t fully align with Kolmogorovâ€™s axioms\n\n\nTech Stack: Python, Qiskit, IBMQ (ibm_osaka), matplotlib, csv Â· Grant: NSTC 112-2813-C-005-038-E"
  },
  {
    "objectID": "AM_thesis.html#source-code",
    "href": "AM_thesis.html#source-code",
    "title": "Quantum Bayesian Inference",
    "section": "Source Code",
    "text": "Source Code\nThe Qiskit programs developed for this research are available on GitHub:\n\ni_use_quantum_do_something_cool â€” Quantum circuit code, simulator/hardware comparison, Bayesian analysis notebooks"
  },
  {
    "objectID": "BigData.html",
    "href": "BigData.html",
    "title": "Big Data Analysis",
    "section": "",
    "text": "Paper / è«–æ–‡ï¼š Using the NystrÃ¶m Method to Speed Up Kernel Machines Authors / ä½œè€…ï¼š Christopher K. I. Williams & Matthias Seeger (NIPS 2000)\n\nè«–æ–‡æ‘˜è¦ï¼š Kernel-based æ–¹æ³•ï¼ˆå¦‚ SVMã€Gaussian Processï¼‰çš„ä¸»è¦ç“¶é ¸åœ¨æ–¼ Gram matrix çš„è¨ˆç®—èˆ‡åçŸ©é™£æ“ä½œï¼Œæ™‚é–“è¤‡é›œåº¦ç‚º O(nÂ³)ã€‚æœ¬è«–æ–‡æå‡ºä½¿ç”¨ NystrÃ¶m method è¿‘ä¼¼ Gram matrix â€” å¾ n ç­†è¨“ç·´è³‡æ–™ä¸­åƒ…æŠ½å‡º m ç­†ï¼Œè¨ˆç®—å°çŸ©é™£å¾Œå†å±•é–‹ç‚ºå…¨åŸŸè¿‘ä¼¼ï¼Œå°‡è¨ˆç®—é‡å¾ O(nÂ³) é™ç‚º O(mÂ²n)ã€‚\n\n\n\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35}}}%%\nflowchart LR\n    A[\"Full Gram Matrix K (nÃ—n)     \"] --&gt; B[\"Sample m points     \"]\n    B --&gt; C[\"K_mm (mÃ—m)     \"]\n    B --&gt; D[\"K_nm (nÃ—m)     \"]\n    C --&gt; E[\"Eigen-decompose K_mm     \"]\n    D --&gt; F[\"NystrÃ¶m Approx:\\nK â‰ˆ K_nm K_mmâ»Â¹ K_nm^T     \"]\n\n    style A fill:#FCE4EC,color:#C62828,stroke:#F48FB1,stroke-width:2px\n    style B fill:#F5F5F5,color:#424242,stroke:#BDBDBD,stroke-width:2px\n    style C fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style D fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style E fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style F fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n\n\n\n\n\n\n\nNystrÃ¶m æµç¨‹ï¼š å¾å®Œæ•´çš„ nÃ—n Gram matrixï¼ˆç²‰ç´…è‰²ï¼Œè¨ˆç®—æˆæœ¬æ¥µé«˜ï¼‰ä¸­ï¼Œéš¨æ©ŸæŠ½å– m ç­†æ¨£æœ¬çµ„æˆ è—è‰² å­çŸ©é™£ K_mm èˆ‡ K_nmï¼Œå†é€é eigen-decompositionï¼ˆæ©˜è‰²ï¼‰é‡å»º ç¶ è‰² è¿‘ä¼¼çŸ©é™£ã€‚ç•¶ m &lt;&lt; n æ™‚å¤§å¹…åŠ é€Ÿã€‚\n\n\n\n\nMercer Expansion (Kernel å±•é–‹)ï¼š\n\\[K(x, y) = \\sum_{i=1}^{\\infty} \\lambda_i \\phi_i(x) \\phi_i(y)\\]\nNystrÃ¶m Approximation (è¿‘ä¼¼å…¬å¼)ï¼š\n\\[\\tilde{K} = K_{nm} K_{mm}^{-1} K_{nm}^T\\]\nComplexity Reduction / è¤‡é›œåº¦é™ä½ï¼š\n\\[O(n^3) \\rightarrow O(m^2 n), \\quad m \\ll n\\]\n\n\n\n\n\nTask / ä»»å‹™ï¼š Implement the NystrÃ¶m method to accelerate RBF Kernel Ridge Classification on the USPS handwritten digit dataset (binary: digit â€œ4â€ vs.Â rest). å¯¦ä½œ NystrÃ¶m æ–¹æ³•åŠ é€Ÿ RBF Kernel Ridge äºŒå…ƒåˆ†é¡ï¼Œè³‡æ–™é›†ç‚º USPS æ‰‹å¯«æ•¸å­—ï¼ˆè¾¨è­˜æ•¸å­— 4ï¼‰ã€‚\nDataset / è³‡æ–™é›†ï¼š USPS â€” 7,291 training + 2,007 test images, 256 features (16Ã—16 pixels)\nMethod / æ–¹æ³•ï¼š RBF Kernel (Ïƒ=4) + Ridge Regression (Î»=0.001) + NystrÃ¶m Approximation (m = 128, 256, 512)\n\n\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35}}}%%\nflowchart LR\n    A[\"USPS 7291Ã—256     \"] --&gt; B[\"StandardScaler     \"] --&gt; C[\"RBF Kernel Ïƒ=4     \"]\n    C --&gt; D[\"Full: K (7291Ã—7291)     \"]\n    C --&gt; E[\"NystrÃ¶m: K_mm + K_nm     \"]\n    D --&gt; F[\"Ridge Solve     \"]\n    E --&gt; F\n    F --&gt; G[\"Predict Â±1     \"]\n\n    style A fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style B fill:#F5F5F5,color:#424242,stroke:#BDBDBD,stroke-width:2px\n    style C fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style D fill:#FCE4EC,color:#C62828,stroke:#F48FB1,stroke-width:2px\n    style E fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n    style F fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style G fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n\n\n\n\n\n\n\nPipeline èªªæ˜ï¼š USPS è³‡æ–™å…ˆæ¨™æº–åŒ–ï¼Œå†åˆ†ç‚ºå…©æ¢è·¯å¾‘ï¼šç²‰ç´…è‰² Full Kernel è¨ˆç®—å®Œæ•´ 7291Ã—7291 Gram matrixï¼Œç¶ è‰² NystrÃ¶m åƒ…è¨ˆç®—å­é›†çŸ©é™£ K_mm + K_nmã€‚å…©è€…çš†é€é Ridge Regression æ±‚è§£ä¿‚æ•¸å‘é‡ Î±ï¼Œæœ€çµ‚é æ¸¬ Â±1ã€‚\n\n\n\n\n# RBF Kernel function\ndef rbf_kernel(X1, X2, sigma=4.0):\n    sq_dists = (np.sum(X1**2, axis=1)[:, None]\n                + np.sum(X2**2, axis=1)[None, :]\n                - 2 * X1 @ X2.T)\n    return np.exp(-sq_dists / (2 * sigma**2))\n\n# Kernel Ridge: solve (K + Î»I) Î± = y\ndef kernel_ridge_train(K, y, lam=1e-3):\n    n = K.shape[0]\n    return np.linalg.solve(K + lam * np.eye(n), y)\n\n# NystrÃ¶m approximation\ndef nystrom_approximation(X_train, m, sigma=4.0):\n    indices = np.random.choice(n, m, replace=False)\n    X_sub = X_train[indices]\n    K_mm = rbf_kernel(X_sub, X_sub, sigma)\n    K_nm = rbf_kernel(X_train, X_sub, sigma)\n    K_mm_inv = np.linalg.inv(K_mm + 1e-8 * np.eye(m))\n    K_approx = K_nm @ K_mm_inv @ K_nm.T\n    return K_approx, indices, K_nm, K_mm_inv\n\n\n\n\n\n\nMethod\nErrors\nAccuracy\nTrain Time (s)\n\n\n\n\nFull Kernel\n9\n99.55%\n7.52\n\n\nNystrÃ¶m m=128\n10\n99.50%\n0.38\n\n\nNystrÃ¶m m=256\n10\n99.50%\n0.63\n\n\nNystrÃ¶m m=512\n9\n99.55%\n1.83\n\n\n\n\n\n\n\n\n\n\n\n\n\næ•ˆèƒ½åˆ†æï¼š ç²‰è‰² Full Kernel éœ€ 7.52 ç§’è¨ˆç®—å®Œæ•´ 7291Ã—7291 Gram matrixã€‚é’è‰² NystrÃ¶m m=128 åƒ…éœ€ 0.38 ç§’ï¼ˆåŠ é€Ÿç´„ 20 å€ï¼‰ï¼Œä¸” accuracy å¹¾ä¹ä¸è®Šï¼ˆ99.50% vs 99.55%ï¼‰ã€‚m=512 æ™‚ç²¾åº¦å®Œå…¨åŒ¹é… Full Kernelï¼Œè¨“ç·´æ™‚é–“ä»ç¯€çœ 75%ã€‚\n\n\n\n\n\n\n\nFull Kernel vs NystrÃ¶m Approximation (first 100 samples)\n\n\n\nçŸ©é™£æ¯”è¼ƒï¼š å·¦åœ–ç‚ºå®Œæ•´ Gram matrixï¼ˆå‰ 100 ç­†æ¨£æœ¬ï¼‰ï¼Œå³åœ–ç‚º NystrÃ¶m è¿‘ä¼¼çµæœã€‚å…©è€…çµæ§‹é«˜åº¦ç›¸ä¼¼ï¼Œé©—è­‰äº† NystrÃ¶m æ–¹æ³•åœ¨ä¿ç•™ kernel çµæ§‹çš„åŒæ™‚å¤§å¹…æ¸›å°‘è¨ˆç®—é‡ã€‚"
  },
  {
    "objectID": "BigData.html#assignments",
    "href": "BigData.html#assignments",
    "title": "Big Data Analysis",
    "section": "",
    "text": "Paper / è«–æ–‡ï¼š Using the NystrÃ¶m Method to Speed Up Kernel Machines Authors / ä½œè€…ï¼š Christopher K. I. Williams & Matthias Seeger (NIPS 2000)\n\nè«–æ–‡æ‘˜è¦ï¼š Kernel-based æ–¹æ³•ï¼ˆå¦‚ SVMã€Gaussian Processï¼‰çš„ä¸»è¦ç“¶é ¸åœ¨æ–¼ Gram matrix çš„è¨ˆç®—èˆ‡åçŸ©é™£æ“ä½œï¼Œæ™‚é–“è¤‡é›œåº¦ç‚º O(nÂ³)ã€‚æœ¬è«–æ–‡æå‡ºä½¿ç”¨ NystrÃ¶m method è¿‘ä¼¼ Gram matrix â€” å¾ n ç­†è¨“ç·´è³‡æ–™ä¸­åƒ…æŠ½å‡º m ç­†ï¼Œè¨ˆç®—å°çŸ©é™£å¾Œå†å±•é–‹ç‚ºå…¨åŸŸè¿‘ä¼¼ï¼Œå°‡è¨ˆç®—é‡å¾ O(nÂ³) é™ç‚º O(mÂ²n)ã€‚\n\n\n\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35}}}%%\nflowchart LR\n    A[\"Full Gram Matrix K (nÃ—n)     \"] --&gt; B[\"Sample m points     \"]\n    B --&gt; C[\"K_mm (mÃ—m)     \"]\n    B --&gt; D[\"K_nm (nÃ—m)     \"]\n    C --&gt; E[\"Eigen-decompose K_mm     \"]\n    D --&gt; F[\"NystrÃ¶m Approx:\\nK â‰ˆ K_nm K_mmâ»Â¹ K_nm^T     \"]\n\n    style A fill:#FCE4EC,color:#C62828,stroke:#F48FB1,stroke-width:2px\n    style B fill:#F5F5F5,color:#424242,stroke:#BDBDBD,stroke-width:2px\n    style C fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style D fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style E fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style F fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n\n\n\n\n\n\n\nNystrÃ¶m æµç¨‹ï¼š å¾å®Œæ•´çš„ nÃ—n Gram matrixï¼ˆç²‰ç´…è‰²ï¼Œè¨ˆç®—æˆæœ¬æ¥µé«˜ï¼‰ä¸­ï¼Œéš¨æ©ŸæŠ½å– m ç­†æ¨£æœ¬çµ„æˆ è—è‰² å­çŸ©é™£ K_mm èˆ‡ K_nmï¼Œå†é€é eigen-decompositionï¼ˆæ©˜è‰²ï¼‰é‡å»º ç¶ è‰² è¿‘ä¼¼çŸ©é™£ã€‚ç•¶ m &lt;&lt; n æ™‚å¤§å¹…åŠ é€Ÿã€‚\n\n\n\n\nMercer Expansion (Kernel å±•é–‹)ï¼š\n\\[K(x, y) = \\sum_{i=1}^{\\infty} \\lambda_i \\phi_i(x) \\phi_i(y)\\]\nNystrÃ¶m Approximation (è¿‘ä¼¼å…¬å¼)ï¼š\n\\[\\tilde{K} = K_{nm} K_{mm}^{-1} K_{nm}^T\\]\nComplexity Reduction / è¤‡é›œåº¦é™ä½ï¼š\n\\[O(n^3) \\rightarrow O(m^2 n), \\quad m \\ll n\\]\n\n\n\n\n\nTask / ä»»å‹™ï¼š Implement the NystrÃ¶m method to accelerate RBF Kernel Ridge Classification on the USPS handwritten digit dataset (binary: digit â€œ4â€ vs.Â rest). å¯¦ä½œ NystrÃ¶m æ–¹æ³•åŠ é€Ÿ RBF Kernel Ridge äºŒå…ƒåˆ†é¡ï¼Œè³‡æ–™é›†ç‚º USPS æ‰‹å¯«æ•¸å­—ï¼ˆè¾¨è­˜æ•¸å­— 4ï¼‰ã€‚\nDataset / è³‡æ–™é›†ï¼š USPS â€” 7,291 training + 2,007 test images, 256 features (16Ã—16 pixels)\nMethod / æ–¹æ³•ï¼š RBF Kernel (Ïƒ=4) + Ridge Regression (Î»=0.001) + NystrÃ¶m Approximation (m = 128, 256, 512)\n\n\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35}}}%%\nflowchart LR\n    A[\"USPS 7291Ã—256     \"] --&gt; B[\"StandardScaler     \"] --&gt; C[\"RBF Kernel Ïƒ=4     \"]\n    C --&gt; D[\"Full: K (7291Ã—7291)     \"]\n    C --&gt; E[\"NystrÃ¶m: K_mm + K_nm     \"]\n    D --&gt; F[\"Ridge Solve     \"]\n    E --&gt; F\n    F --&gt; G[\"Predict Â±1     \"]\n\n    style A fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style B fill:#F5F5F5,color:#424242,stroke:#BDBDBD,stroke-width:2px\n    style C fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style D fill:#FCE4EC,color:#C62828,stroke:#F48FB1,stroke-width:2px\n    style E fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n    style F fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style G fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n\n\n\n\n\n\n\nPipeline èªªæ˜ï¼š USPS è³‡æ–™å…ˆæ¨™æº–åŒ–ï¼Œå†åˆ†ç‚ºå…©æ¢è·¯å¾‘ï¼šç²‰ç´…è‰² Full Kernel è¨ˆç®—å®Œæ•´ 7291Ã—7291 Gram matrixï¼Œç¶ è‰² NystrÃ¶m åƒ…è¨ˆç®—å­é›†çŸ©é™£ K_mm + K_nmã€‚å…©è€…çš†é€é Ridge Regression æ±‚è§£ä¿‚æ•¸å‘é‡ Î±ï¼Œæœ€çµ‚é æ¸¬ Â±1ã€‚\n\n\n\n\n# RBF Kernel function\ndef rbf_kernel(X1, X2, sigma=4.0):\n    sq_dists = (np.sum(X1**2, axis=1)[:, None]\n                + np.sum(X2**2, axis=1)[None, :]\n                - 2 * X1 @ X2.T)\n    return np.exp(-sq_dists / (2 * sigma**2))\n\n# Kernel Ridge: solve (K + Î»I) Î± = y\ndef kernel_ridge_train(K, y, lam=1e-3):\n    n = K.shape[0]\n    return np.linalg.solve(K + lam * np.eye(n), y)\n\n# NystrÃ¶m approximation\ndef nystrom_approximation(X_train, m, sigma=4.0):\n    indices = np.random.choice(n, m, replace=False)\n    X_sub = X_train[indices]\n    K_mm = rbf_kernel(X_sub, X_sub, sigma)\n    K_nm = rbf_kernel(X_train, X_sub, sigma)\n    K_mm_inv = np.linalg.inv(K_mm + 1e-8 * np.eye(m))\n    K_approx = K_nm @ K_mm_inv @ K_nm.T\n    return K_approx, indices, K_nm, K_mm_inv\n\n\n\n\n\n\nMethod\nErrors\nAccuracy\nTrain Time (s)\n\n\n\n\nFull Kernel\n9\n99.55%\n7.52\n\n\nNystrÃ¶m m=128\n10\n99.50%\n0.38\n\n\nNystrÃ¶m m=256\n10\n99.50%\n0.63\n\n\nNystrÃ¶m m=512\n9\n99.55%\n1.83\n\n\n\n\n\n\n\n\n\n\n\n\n\næ•ˆèƒ½åˆ†æï¼š ç²‰è‰² Full Kernel éœ€ 7.52 ç§’è¨ˆç®—å®Œæ•´ 7291Ã—7291 Gram matrixã€‚é’è‰² NystrÃ¶m m=128 åƒ…éœ€ 0.38 ç§’ï¼ˆåŠ é€Ÿç´„ 20 å€ï¼‰ï¼Œä¸” accuracy å¹¾ä¹ä¸è®Šï¼ˆ99.50% vs 99.55%ï¼‰ã€‚m=512 æ™‚ç²¾åº¦å®Œå…¨åŒ¹é… Full Kernelï¼Œè¨“ç·´æ™‚é–“ä»ç¯€çœ 75%ã€‚\n\n\n\n\n\n\n\nFull Kernel vs NystrÃ¶m Approximation (first 100 samples)\n\n\n\nçŸ©é™£æ¯”è¼ƒï¼š å·¦åœ–ç‚ºå®Œæ•´ Gram matrixï¼ˆå‰ 100 ç­†æ¨£æœ¬ï¼‰ï¼Œå³åœ–ç‚º NystrÃ¶m è¿‘ä¼¼çµæœã€‚å…©è€…çµæ§‹é«˜åº¦ç›¸ä¼¼ï¼Œé©—è­‰äº† NystrÃ¶m æ–¹æ³•åœ¨ä¿ç•™ kernel çµæ§‹çš„åŒæ™‚å¤§å¹…æ¸›å°‘è¨ˆç®—é‡ã€‚"
  },
  {
    "objectID": "BigData.html#final-project",
    "href": "BigData.html#final-project",
    "title": "Big Data Analysis",
    "section": "Final Project / æœŸæœ«å ±å‘Š",
    "text": "Final Project / æœŸæœ«å ±å‘Š\n\nSmoothed & Distributed SVM / å¹³æ»‘åŒ–èˆ‡åˆ†æ•£å¼ SVM\nTask / ä»»å‹™ï¼š Implement and compare three SVM variants: (1) standard LinearSVC, (2) Smoothed SVM using log-sum-exp approximation of hinge loss, and (3) Distributed Smoothed SVM with communication-efficient gradient aggregation. å¯¦ä½œä¸¦æ¯”è¼ƒä¸‰ç¨® SVM è®Šé«”ï¼šæ¨™æº– LinearSVCã€å¹³æ»‘åŒ– SVMï¼ˆlog-sum-exp è¿‘ä¼¼ hinge lossï¼‰ã€ä»¥åŠåˆ†æ•£å¼å¹³æ»‘ SVMï¼ˆé€šè¨Šé«˜æ•ˆæ¢¯åº¦èšåˆï¼‰ã€‚\nDataset / è³‡æ–™é›†ï¼š a9a (Adult Income) â€” 32,561 training + 16,281 test samples, 123 features\nMethod / æ–¹æ³•ï¼š LinearSVC baseline â†’ Smoothed Hinge Loss (Î²=5) + L-BFGS-B â†’ Distributed SGD (K=5 workers)\n\nThree SVM Variants / ä¸‰ç¨® SVM è®Šé«”\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35}}}%%\nflowchart TD\n    DATA[\"a9a Dataset 32KÃ—123     \"] --&gt; S1[\"LinearSVC (sklearn)     \"]\n    DATA --&gt; S2[\"Smoothed SVM (L-BFGS-B)     \"]\n    DATA --&gt; S3[\"Distributed Smooth SVM     \"]\n\n    S1 --&gt; R1[\"Baseline Accuracy     \"]\n    S2 --&gt; R2[\"Single-node Accuracy     \"]\n    S3 --&gt; |\"K=5 workers\"| R3[\"Distributed Accuracy     \"]\n\n    S3 --&gt; W1[\"Worker 1     \"]\n    S3 --&gt; W2[\"Worker 2     \"]\n    S3 --&gt; W3[\"...     \"]\n    S3 --&gt; W4[\"Worker 5     \"]\n\n    style DATA fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style S1   fill:#F5F5F5,color:#424242,stroke:#BDBDBD,stroke-width:2px\n    style S2   fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style S3   fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n    style R1   fill:#F5F5F5,color:#424242,stroke:#BDBDBD\n    style R2   fill:#FFF3E0,color:#E65100,stroke:#FFCC80\n    style R3   fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7\n    style W1   fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7\n    style W2   fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7\n    style W3   fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7\n    style W4   fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7\n\n\n\n\n\n\n\næ¶æ§‹èªªæ˜ï¼š ç°è‰² = sklearn LinearSVC baselineï¼Œæ©˜è‰² = å–®ç¯€é» Smoothed SVMï¼ˆL-BFGS-B æœ€ä½³åŒ–ï¼‰ï¼Œç¶ è‰² = åˆ†æ•£å¼ç‰ˆæœ¬ï¼ˆK=5 workersï¼Œæ¯è¼ªå„ worker è¨ˆç®—å±€éƒ¨æ¢¯åº¦å†èšåˆï¼‰ã€‚\n\n\n\nSmoothed Hinge Loss / å¹³æ»‘åŒ– Hinge Loss\nStandard hinge loss is non-differentiable at u=1. The smoothed version uses log-sum-exp to create a differentiable approximation:\n\\[\\ell_\\beta(u) = \\frac{1}{\\beta} \\log\\left(1 + e^{\\beta(1-u)}\\right)\\]\n\n\n\n\n\n\n\n\n\n\nLoss æ¯”è¼ƒï¼š ç°è‰²è™›ç·š æ¨™æº– Hinge Loss åœ¨ u=1 è™•ä¸å¯å¾®ã€‚æ©˜è‰² Smoothed (Î²=5) è¿‘ä¼¼åº¦æœ€é«˜ï¼Œæ¥è¿‘åŸå§‹ hinge ä½†è™•è™•å¯å¾®ã€‚é’è‰² (Î²=1) è¼ƒå¹³æ»‘ä½†é›¢åŸå§‹ hinge è¼ƒé ã€‚Î² è¶Šå¤§è¶Šæ¥è¿‘åŸå§‹å½¢ç‹€ã€‚\n\n\n\nDistributed Gradient Aggregation / åˆ†æ•£å¼æ¢¯åº¦èšåˆ\n# Distributed Smoothed SVM â€” Communication-efficient gradient\nK = 5  # number of workers\nB = 5  # communication rounds\nbeta = np.zeros(X_train.shape[1])\n\nfor b in range(B):\n    # Pilot gradient (computed on worker 0)\n    grad_pilot = compute_gradient(beta, X_pilot, y_pilot, lambda_, beta_smooth)\n\n    # Each worker computes local gradient correction\n    grad_total = np.zeros_like(beta)\n    for X_k, y_k in zip(split_X, split_y):\n        grad_k = compute_gradient(beta, X_k, y_k, lambda_, beta_smooth)\n        grad_total += (grad_k - grad_pilot) * (len(X_k) / len(X_train))\n\n    # Aggregate: surrogate gradient = pilot + corrections\n    beta -= 0.1 * (grad_pilot + grad_total)\n\nåˆ†æ•£å¼ç­–ç•¥ï¼š è³‡æ–™åˆ†æˆ K=5 ä»½ã€‚æ¯è¼ªåƒ…éœ€ä¸€å€‹ pilot worker è¨ˆç®—å®Œæ•´æ¢¯åº¦ï¼Œå…¶é¤˜ workers è¨ˆç®—ã€Œèˆ‡ pilot çš„æ¢¯åº¦å·®ã€ï¼ˆcorrection termï¼‰ï¼Œæœ€å¾Œèšåˆã€‚ç›¸æ¯”æ¯å€‹ worker éƒ½å‚³å®Œæ•´æ¢¯åº¦ï¼Œæ­¤æ–¹æ³•å¤§å¹…é™ä½é€šè¨Šé‡ã€‚\n\n\n\nResults / çµæœ\n\n\n\nModel\nTest Accuracy\nTraining Time\n\n\n\n\nLinearSVC (sklearn baseline)\n84.93%\n1.52s\n\n\nSmoothed SVM (single-node, L-BFGS-B)\n84.88%\n0.78s\n\n\nDistributed Smoothed SVM (K=5, B=5)\n83.79%\n0.01s\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrade-off åˆ†æï¼š ç°è‰² LinearSVC ç‚º baselineï¼ˆ84.93%ï¼‰ã€‚æ©˜è‰² Smoothed SVM ç²¾åº¦å¹¾ä¹ç›¸åŒï¼ˆ84.88%ï¼‰ï¼Œä½†å›  L-BFGS-B æœ€ä½³åŒ–æ›´é«˜æ•ˆè€Œæ›´å¿«ï¼ˆ0.78s vs 1.52sï¼‰ã€‚é’è‰² Distributed ç‰ˆæœ¬çŠ§ç‰² ~1.1% ç²¾åº¦æ›å– 150 å€åŠ é€Ÿï¼ˆ0.01sï¼‰ï¼Œå±•ç¾äº†åˆ†æ•£å¼è¨ˆç®—åœ¨å¤§è¦æ¨¡è³‡æ–™ä¸‹çš„æ½›åŠ›ã€‚\n\n\n\nAblation Study â€” Î² Sensitivity / æ¶ˆèå¯¦é©—ï¼šÎ² æ•æ„Ÿåº¦\n\n\n\nÎ² (smoothness)\nTest Accuracy\n\n\n\n\n1\n83.69%\n\n\n2\n83.79%\n\n\n5\n83.79%\n\n\n10\n83.73%\n\n\n20\n81.21%\n\n\n\n\n\n\n\n\n\n\n\n\n\nÎ² é¸æ“‡å»ºè­°ï¼š Î²=2 å’Œ Î²=5 è¡¨ç¾æœ€ä½³ä¸”ç›¸åŒï¼ˆ83.79%ï¼‰ã€‚Î² å¤ªå¤§ï¼ˆ=20ï¼‰åè€Œå°è‡´ç²¾åº¦ä¸‹é™è‡³ 81.21%ï¼Œå› ç‚ºéåº¦é€¼è¿‘ä¸å¯å¾®çš„åŸå§‹ hinge lossï¼Œé€ æˆ gradient ä¼°è¨ˆä¸ç©©å®šã€‚Î²=5 ç‚ºæœ€ä½³å¹³è¡¡é»ã€‚"
  },
  {
    "objectID": "GISPY.html",
    "href": "GISPY.html",
    "title": "GIS & Python Programming",
    "section": "",
    "text": "Topic / ä¸»é¡Œï¼š Financial planning with compound interest calculations ä»¥è¤‡åˆ©è¨ˆç®—ç‚ºåŸºç¤çš„é€€ä¼‘åŸºé‡‘è¦åŠƒå·¥å…·ã€‚\nKey Concepts / æ ¸å¿ƒæ¦‚å¿µï¼š Functions, loops, formatted output, compound interest\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35}}}%%\nflowchart LR\n    A[\"User Input     \"] --&gt; B[\"Monthly Contribution     \"] --&gt; C[\"Compound Interest     \"] --&gt; D[\"Year-by-Year Projection     \"] --&gt; E[\"Millionaire Calculator     \"]\n\n    style A fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style B fill:#F5F5F5,color:#424242,stroke:#BDBDBD,stroke-width:2px\n    style C fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style D fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n    style E fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n\n\n\n\n\n\n\nåŠŸèƒ½èªªæ˜ï¼š ä¸‰ç¨®æ¨¡å¼ â€” (1) é€å¹´åŸºé‡‘é æ¸¬ï¼ˆæœˆè¤‡åˆ©ï¼‰ï¼Œ(2) ç™¾è¬å¯Œç¿è¨ˆç®—å™¨ï¼ˆå¤šå°‘å¹´é”åˆ° $1Mï¼‰ï¼Œ(3) åˆ©ç‡æ¯”è¼ƒè¡¨ï¼ˆ1-30% å¹´åˆ©ç‡ï¼‰ã€‚ä¾‹ï¼š$500/æœˆã€10% å¹´åˆ©ç‡ã€30 å¹´ â†’ $1.14Mã€‚\n\n# Year-by-year retirement fund projection\ndef calculate_fund(monthly, rate, years):\n    balance = 0\n    monthly_rate = rate / 12\n    for year in range(1, years + 1):\n        for month in range(12):\n            balance += monthly\n            balance *= (1 + monthly_rate)\n        print(f\"Year {year:3d}: ${balance:&gt;12,.2f}\")\n    return balance\n\n# Millionaire Calculator\ndef years_to_million(monthly, rate):\n    balance, months = 0, 0\n    monthly_rate = rate / 12\n    while balance &lt; 1_000_000:\n        balance += monthly\n        balance *= (1 + monthly_rate)\n        months += 1\n    return months // 12, months % 12\n\n\n\n\n\n\n\n\n\n\nè¤‡åˆ©æ•ˆæœï¼š å‰ 15 å¹´æˆé•·ç·©æ…¢ï¼Œå¾Œ 15 å¹´æŒ‡æ•¸åŠ é€Ÿã€‚$500/æœˆåœ¨ 10% å¹´åˆ©ç‡ä¸‹ç´„ 21 å¹´é”åˆ°ç™¾è¬ç¾å…ƒï¼ˆæ©˜è‰²è±å½¢ï¼‰ã€‚è¤‡åˆ©æ˜¯é•·æœŸæŠ•è³‡çš„æ ¸å¿ƒé©…å‹•åŠ›ã€‚\n\n\n\n\n\nTopic / ä¸»é¡Œï¼š Interactive student grade management with CRUD operations äº’å‹•å¼å­¸ç”Ÿæˆç¸¾ç®¡ç†ç³»çµ±ï¼Œæ”¯æ´æ–°å¢ã€åˆªé™¤ã€ä¿®æ”¹ã€æŸ¥è©¢ã€‚\nKey Concepts / æ ¸å¿ƒæ¦‚å¿µï¼š Lists, nested data structures, menu-driven programming, input validation\n# Menu-driven gradebook\ndef main_menu():\n    while True:\n        print(\"\\n1. Add student\")\n        print(\"2. Remove student\")\n        print(\"3. Modify grade\")\n        print(\"4. Display gradebook\")\n        print(\"5. Find highest/lowest\")\n        print(\"6. Exit\")\n        choice = input(\"Select: \")\n        # ... handle each option\n\nè¨­è¨ˆé‡é»ï¼š ä½¿ç”¨å·¢ç‹€ list å„²å­˜å­¸ç”Ÿè³‡æ–™ [name, grade]ï¼Œæ­é… while loop å¯¦ç¾æŒçºŒäº’å‹•é¸å–®ã€‚åŒ…å«è¼¸å…¥é©—è­‰ï¼ˆæˆç¸¾ç¯„åœ 0-100ï¼‰èˆ‡éŒ¯èª¤è™•ç†ã€‚\n\n\n\n\n\nTopic / ä¸»é¡Œï¼š Modular programming, reusable modules, and object-oriented design æ¨¡çµ„åŒ–ç¨‹å¼è¨­è¨ˆã€å¯é‡ç”¨æ¨¡çµ„èˆ‡ç‰©ä»¶å°å‘è¨­è¨ˆã€‚\nKey Concepts / æ ¸å¿ƒæ¦‚å¿µï¼š Module imports, OOP (classes), CSV I/O, distance calculations\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35}}}%%\nflowchart TD\n    A[\"Lab 4 Modules     \"] --&gt; B[\"(a) CSV Field Counter     \"]\n    A --&gt; C[\"(b) Parcel Tax Calculator     \"]\n    A --&gt; D[\"(c) Distance Calculator     \"]\n\n    B --&gt; B1[\"mycount.py + callingscript.py     \"]\n    C --&gt; C1[\"parcelclass.py â†’ OOP     \"]\n    D --&gt; D1[\"Euclidean + Great Circle     \"]\n\n    style A fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style B fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style C fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n    style D fill:#F3E5F5,color:#6A1B9A,stroke:#CE93D8,stroke-width:2px\n    style B1 fill:#F5F5F5,color:#424242,stroke:#BDBDBD,stroke-width:2px\n    style C1 fill:#F5F5F5,color:#424242,stroke:#BDBDBD,stroke-width:2px\n    style D1 fill:#F5F5F5,color:#424242,stroke:#BDBDBD,stroke-width:2px\n\n\n\n\n\n\n# (b) Parcel class with tax assessment\nclass Parcel:\n    def __init__(self, parcel_id, land_use, market_value):\n        self.parcel_id = parcel_id\n        self.land_use = land_use\n        self.market_value = market_value\n\n    def assess_tax(self):\n        rates = {\"SFR\": 0.05, \"MFR\": 0.04}\n        rate = rates.get(self.land_use, 0.02)\n        return self.market_value * rate\n\n# (c) Great Circle Distance (Haversine formula)\nimport math\ndef great_circle(lat1, lon1, lat2, lon2):\n    R = 6371  # Earth radius in km\n    dlat = math.radians(lat2 - lat1)\n    dlon = math.radians(lon2 - lon1)\n    a = (math.sin(dlat/2)**2 +\n         math.cos(math.radians(lat1)) *\n         math.cos(math.radians(lat2)) *\n         math.sin(dlon/2)**2)\n    return R * 2 * math.asin(math.sqrt(a))\n\nä¸‰å€‹å­ä»»å‹™ï¼š (a) è¨ˆç®— CSV å„æ¬„ä½ç©ºå€¼æ•¸é‡ï¼Œ(b) ä»¥ OOP å»ºç«‹ Parcel é¡åˆ¥è¨ˆç®—ä¸å‹•ç”¢ç¨…ï¼ˆSFR 5%ã€MFR 4%ã€å…¶ä»– 2%ï¼‰ï¼Œ(c) ä»¥ Haversine å…¬å¼è¨ˆç®—åœ°çƒè¡¨é¢å…©é»è·é›¢ã€‚\n\n\n\n\n\nTopic / ä¸»é¡Œï¼š DataFrame manipulation, grouping, pivoting, and multi-dataset merging Pandas DataFrame æ“ä½œã€åˆ†çµ„ã€æ¨ç´åˆ†æèˆ‡å¤šè³‡æ–™é›†åˆä½µã€‚\nDatasets / è³‡æ–™é›†ï¼š MovieLens (100K ratings), COVID-19 global time series\nKey Concepts / æ ¸å¿ƒæ¦‚å¿µï¼š pandas Series/DataFrame, boolean indexing, GroupBy, pivot tables, merge/join\n# COVID-19 fatality rate analysis\nfatality = (deaths_total / confirmed_total * 100).sort_values(ascending=False)\n# Peru: 9.17%, Mexico: 7.58%, South Africa: 2.68%\n\n# MovieLens: most rated movies\ntop_movies = ratings.groupby('movieId').size().sort_values(ascending=False).head(10)\n\n# Multi-DataFrame merge\nmerged = pd.merge(users, ratings, on='userId')\nmerged = pd.merge(merged, movies, on='movieId')\n\nåˆ†æé‡é»ï¼š (1) MovieLens â€” æ‰¾å‡ºè©•åˆ†æœ€å¤šçš„é›»å½±ã€ç”·å¥³è©•åˆ†å·®ç•°æœ€å¤§çš„é›»å½±ï¼Œ(2) COVID-19 â€” ç¢ºè¨ºæ•¸å‰ 25 åœ‹ã€å„åœ‹è‡´æ­»ç‡æ’åï¼ˆç§˜é­¯ 9.17% æœ€é«˜ï¼‰ã€æœˆåº¦å¢é‡åˆ†æï¼ˆç¾åœ‹ 2021 å¹´ 9 æœˆå¢ +430 è¬ä¾‹ï¼‰ã€‚\n\n\n\n\n\nTopic / ä¸»é¡Œï¼š Publication-quality charts with matplotlib and Altair ä½¿ç”¨ matplotlib èˆ‡ Altair è£½ä½œå‡ºç‰ˆå“è³ªçš„åœ–è¡¨èˆ‡äº’å‹•å¼è¦–è¦ºåŒ–ã€‚\nDatasets / è³‡æ–™é›†ï¼š MovieLens, COVID-19, Seattle weather (Vega)\n\n\n\n\n\n\nTop 10 Most-Reviewed Movies\n\n\n\n\n\n\n\nCOVID-19 Top 25 Countries\n\n\n\n\n\n\n\n\n\n\n\nGender Rating Differences\n\n\n\n\n\n\n\nSeattle Weather Distribution\n\n\n\n\n\n\nè¦–è¦ºåŒ–æŠ€å·§ï¼š æ°´å¹³é•·æ¢åœ–æ¯”è¼ƒé›»å½±è©•è«–æ•¸é‡ã€æ•£é»åœ–å‘ˆç¾ç”·å¥³è©•åˆ†å·®ç•°ã€åœ“é¤…åœ–å±•ç¤ºå¤©æ°£é¡å‹åˆ†å¸ƒã€æŠ˜ç·šåœ–è¿½è¹¤ COVID-19 è¶¨å‹¢ã€‚Altair éƒ¨åˆ†è£½ä½œäº†äº’å‹•å¼ linked chartï¼ˆé»é¸åœ‹å®¶ â†’ é¡¯ç¤ºè©²åœ‹æ­»äº¡è¶¨å‹¢ï¼‰ã€‚\n\n\n\n\n\nTopic / ä¸»é¡Œï¼š Desktop application: Number guessing game with graphical interface ä½¿ç”¨ PyQt5 é–‹ç™¼æ•¸å­—çŒœè¬éŠæˆ²æ¡Œé¢æ‡‰ç”¨ç¨‹å¼ã€‚\nKey Concepts / æ ¸å¿ƒæ¦‚å¿µï¼š PyQt5 widgets, event-driven programming, signal/slot, Qt Designer\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35}}}%%\nflowchart LR\n    A[\"Qt Designer     \"] --&gt; B[\"frmGuess.py     \"] --&gt; C[\"lab7.py     \"] --&gt; D[\"Number Game GUI     \"]\n\n    style A fill:#F5F5F5,color:#424242,stroke:#BDBDBD,stroke-width:2px\n    style B fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style C fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style D fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n\n\n\n\n\n\n# Number guessing game with hint system\nclass GuessGame:\n    def __init__(self):\n        self.target = random.randint(1, 100)\n        self.guesses = 0\n\n    def make_guess(self, n):\n        self.guesses += 1\n        if n == self.target:\n            return \"Correct!\"\n        return \"Higher!\" if n &lt; self.target else \"Lower!\"\n\n    def use_hint(self):\n        \"\"\"Costs 5 guesses, reveals number within Â±5\"\"\"\n        self.guesses += 5\n        return (self.target - 5, self.target + 5)\n\nGUI åŠŸèƒ½ï¼š éš¨æ©Ÿæ•¸å­— (1-100) çŒœè¬éŠæˆ²ã€‚åŒ…å« (1) çŒœæ¸¬è¿½è¹¤ï¼Œ(2) æç¤ºç³»çµ±ï¼ˆæ¶ˆè€— 5 æ¬¡æ©Ÿæœƒï¼Œç¸®å°ç¯„åœåˆ° Â±5ï¼‰ï¼Œ(3) å‹åˆ©åµæ¸¬èˆ‡é‡è¨­ã€‚ä½¿ç”¨ Qt Designer è¨­è¨ˆä»‹é¢ï¼ŒfrmGuess.py ç‚ºè‡ªå‹•ç”Ÿæˆçš„ UI ç¨‹å¼ç¢¼ã€‚\n\n\n\n\n\nTopic / ä¸»é¡Œï¼š Programmatic map creation and spatial data querying with ArcGIS Online ä½¿ç”¨ ArcGIS API ç¨‹å¼åŒ–å»ºç«‹åœ°åœ–èˆ‡æŸ¥è©¢ç©ºé–“è³‡æ–™ã€‚\nKey Concepts / æ ¸å¿ƒæ¦‚å¿µï¼š ArcGIS authentication, WebMap, FeatureLayer queries, basemap cycling\nfrom arcgis.gis import GIS\nfrom arcgis.mapping import WebMap\n\ngis = GIS(\"https://www.arcgis.com\", username, password)\nm = gis.map(\"University of Texas at Dallas\", zoomlevel=15)\n\n# Search and add feature layers\nitems = gis.content.search(\"UTD Buildings\", item_type=\"Feature Layer\")\nm.add_layer(items[0])\n\n# Query building attributes\nfl = items[0].layers[0]\nfl.properties.fields  # Inspect field schema\n\nGIS æ“ä½œï¼š é€é Python API é€£æ¥ ArcGIS Onlineï¼Œä»¥ UTD æ ¡åœ’ç‚ºä¸­å¿ƒå»ºç«‹äº’å‹•å¼åœ°åœ–ï¼Œæœå°‹ä¸¦ç–ŠåŠ å»ºç¯‰ç‰©åœ–å±¤ï¼ŒæŸ¥è©¢å±¬æ€§æ¬„ä½çµæ§‹ï¼Œåˆ‡æ›ä¸åŒåº•åœ–æ¨£å¼ã€‚\n\n\n\n\n\nTopic / ä¸»é¡Œï¼š Data extraction from web sources using APIs and scraping tools ä½¿ç”¨ API èˆ‡çˆ¬èŸ²å·¥å…·å¾ç¶²é ä¾†æºæ“·å–è³‡æ–™ã€‚\nKey Concepts / æ ¸å¿ƒæ¦‚å¿µï¼š requests, JSON parsing, BeautifulSoup (HTML), Selenium (dynamic content)\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35}}}%%\nflowchart LR\n    A[\"requests GET     \"] --&gt; B[\"JSON / HTML     \"]\n    B --&gt; C[\"BeautifulSoup     \"]\n    B --&gt; D[\"json.loads()     \"]\n    C --&gt; E[\"Structured Data     \"]\n    D --&gt; E\n\n    style A fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style B fill:#F5F5F5,color:#424242,stroke:#BDBDBD,stroke-width:2px\n    style C fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style D fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n    style E fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n\n\n\n\n\n\nimport requests\nfrom bs4 import BeautifulSoup\n\n# RESTful API request\nresponse = requests.get(\"https://api.example.com/data\")\ndata = response.json()\n\n# HTML scraping\npage = requests.get(\"https://example.com\")\nsoup = BeautifulSoup(page.content, \"html.parser\")\nelements = soup.find_all(\"div\", class_=\"target\")\n\nå…©ç¨®æ–¹æ³•ï¼š (1) RESTful API â€” ç™¼é€ GET/POST è«‹æ±‚ï¼Œè§£æ JSON å›æ‡‰ï¼Œ(2) HTML çˆ¬èŸ² â€” ä½¿ç”¨ BeautifulSoup è§£æ DOM çµæ§‹ï¼ŒSelenium è™•ç†å‹•æ…‹è¼‰å…¥å…§å®¹ã€‚\n\n\n\n\n\nTopic / ä¸»é¡Œï¼š Parse structured text files with regex, create spatial visualizations with GeoPandas ä½¿ç”¨æ­£è¦è¡¨é”å¼è§£æçµæ§‹åŒ–æ–‡å­—ï¼Œæ­é… GeoPandas è£½ä½œåœ°ç†ç©ºé–“è¦–è¦ºåŒ–ã€‚\nDataset / è³‡æ–™é›†ï¼š worldcities.txt â€” city coordinates in degrees-minutes format\nimport re\nimport geopandas as gpd\nfrom shapely.geometry import Point\n\n# Parse DMS coordinates with regex\npattern = r\"^(.*)\\t(\\d+)\\t(\\d+) ([NS])\\t(\\d+)\\t(\\d+) ([EW])\\t(.*)$\"\n\nfor line in open(\"worldcities.txt\"):\n    match = re.match(pattern, line.strip())\n    if match:\n        city, lat_d, lat_m, ns, lon_d, lon_m, ew, country = match.groups()\n        lat = (int(lat_d) + int(lat_m)/60) * (-1 if ns == 'S' else 1)\n        lon = (int(lon_d) + int(lon_m)/60) * (-1 if ew == 'W' else 1)\n\n\n\nWorld Cities Map â€” GeoPandas with Natural Earth basemap\n\n\n\nè™•ç†æµç¨‹ï¼š Regex å¾ worldcities.txt æ“·å–åŸå¸‚åã€ç¶“ç·¯åº¦ï¼ˆåº¦åˆ†æ ¼å¼ï¼‰èˆ‡åœ‹å®¶ â†’ è½‰æ› DMS ç‚ºåé€²ä½åº¦æ•¸ â†’ å»ºç«‹ Shapely Point å¹¾ä½• â†’ GeoPandas GeoDataFrame â†’ ç–ŠåŠ  Natural Earth åº•åœ–ç¹ªè£½å…¨çƒåŸå¸‚åˆ†å¸ƒåœ–ã€‚"
  },
  {
    "objectID": "GISPY.html#assignments",
    "href": "GISPY.html#assignments",
    "title": "GIS & Python Programming",
    "section": "",
    "text": "Topic / ä¸»é¡Œï¼š Financial planning with compound interest calculations ä»¥è¤‡åˆ©è¨ˆç®—ç‚ºåŸºç¤çš„é€€ä¼‘åŸºé‡‘è¦åŠƒå·¥å…·ã€‚\nKey Concepts / æ ¸å¿ƒæ¦‚å¿µï¼š Functions, loops, formatted output, compound interest\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35}}}%%\nflowchart LR\n    A[\"User Input     \"] --&gt; B[\"Monthly Contribution     \"] --&gt; C[\"Compound Interest     \"] --&gt; D[\"Year-by-Year Projection     \"] --&gt; E[\"Millionaire Calculator     \"]\n\n    style A fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style B fill:#F5F5F5,color:#424242,stroke:#BDBDBD,stroke-width:2px\n    style C fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style D fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n    style E fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n\n\n\n\n\n\n\nåŠŸèƒ½èªªæ˜ï¼š ä¸‰ç¨®æ¨¡å¼ â€” (1) é€å¹´åŸºé‡‘é æ¸¬ï¼ˆæœˆè¤‡åˆ©ï¼‰ï¼Œ(2) ç™¾è¬å¯Œç¿è¨ˆç®—å™¨ï¼ˆå¤šå°‘å¹´é”åˆ° $1Mï¼‰ï¼Œ(3) åˆ©ç‡æ¯”è¼ƒè¡¨ï¼ˆ1-30% å¹´åˆ©ç‡ï¼‰ã€‚ä¾‹ï¼š$500/æœˆã€10% å¹´åˆ©ç‡ã€30 å¹´ â†’ $1.14Mã€‚\n\n# Year-by-year retirement fund projection\ndef calculate_fund(monthly, rate, years):\n    balance = 0\n    monthly_rate = rate / 12\n    for year in range(1, years + 1):\n        for month in range(12):\n            balance += monthly\n            balance *= (1 + monthly_rate)\n        print(f\"Year {year:3d}: ${balance:&gt;12,.2f}\")\n    return balance\n\n# Millionaire Calculator\ndef years_to_million(monthly, rate):\n    balance, months = 0, 0\n    monthly_rate = rate / 12\n    while balance &lt; 1_000_000:\n        balance += monthly\n        balance *= (1 + monthly_rate)\n        months += 1\n    return months // 12, months % 12\n\n\n\n\n\n\n\n\n\n\nè¤‡åˆ©æ•ˆæœï¼š å‰ 15 å¹´æˆé•·ç·©æ…¢ï¼Œå¾Œ 15 å¹´æŒ‡æ•¸åŠ é€Ÿã€‚$500/æœˆåœ¨ 10% å¹´åˆ©ç‡ä¸‹ç´„ 21 å¹´é”åˆ°ç™¾è¬ç¾å…ƒï¼ˆæ©˜è‰²è±å½¢ï¼‰ã€‚è¤‡åˆ©æ˜¯é•·æœŸæŠ•è³‡çš„æ ¸å¿ƒé©…å‹•åŠ›ã€‚\n\n\n\n\n\nTopic / ä¸»é¡Œï¼š Interactive student grade management with CRUD operations äº’å‹•å¼å­¸ç”Ÿæˆç¸¾ç®¡ç†ç³»çµ±ï¼Œæ”¯æ´æ–°å¢ã€åˆªé™¤ã€ä¿®æ”¹ã€æŸ¥è©¢ã€‚\nKey Concepts / æ ¸å¿ƒæ¦‚å¿µï¼š Lists, nested data structures, menu-driven programming, input validation\n# Menu-driven gradebook\ndef main_menu():\n    while True:\n        print(\"\\n1. Add student\")\n        print(\"2. Remove student\")\n        print(\"3. Modify grade\")\n        print(\"4. Display gradebook\")\n        print(\"5. Find highest/lowest\")\n        print(\"6. Exit\")\n        choice = input(\"Select: \")\n        # ... handle each option\n\nè¨­è¨ˆé‡é»ï¼š ä½¿ç”¨å·¢ç‹€ list å„²å­˜å­¸ç”Ÿè³‡æ–™ [name, grade]ï¼Œæ­é… while loop å¯¦ç¾æŒçºŒäº’å‹•é¸å–®ã€‚åŒ…å«è¼¸å…¥é©—è­‰ï¼ˆæˆç¸¾ç¯„åœ 0-100ï¼‰èˆ‡éŒ¯èª¤è™•ç†ã€‚\n\n\n\n\n\nTopic / ä¸»é¡Œï¼š Modular programming, reusable modules, and object-oriented design æ¨¡çµ„åŒ–ç¨‹å¼è¨­è¨ˆã€å¯é‡ç”¨æ¨¡çµ„èˆ‡ç‰©ä»¶å°å‘è¨­è¨ˆã€‚\nKey Concepts / æ ¸å¿ƒæ¦‚å¿µï¼š Module imports, OOP (classes), CSV I/O, distance calculations\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35}}}%%\nflowchart TD\n    A[\"Lab 4 Modules     \"] --&gt; B[\"(a) CSV Field Counter     \"]\n    A --&gt; C[\"(b) Parcel Tax Calculator     \"]\n    A --&gt; D[\"(c) Distance Calculator     \"]\n\n    B --&gt; B1[\"mycount.py + callingscript.py     \"]\n    C --&gt; C1[\"parcelclass.py â†’ OOP     \"]\n    D --&gt; D1[\"Euclidean + Great Circle     \"]\n\n    style A fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style B fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style C fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n    style D fill:#F3E5F5,color:#6A1B9A,stroke:#CE93D8,stroke-width:2px\n    style B1 fill:#F5F5F5,color:#424242,stroke:#BDBDBD,stroke-width:2px\n    style C1 fill:#F5F5F5,color:#424242,stroke:#BDBDBD,stroke-width:2px\n    style D1 fill:#F5F5F5,color:#424242,stroke:#BDBDBD,stroke-width:2px\n\n\n\n\n\n\n# (b) Parcel class with tax assessment\nclass Parcel:\n    def __init__(self, parcel_id, land_use, market_value):\n        self.parcel_id = parcel_id\n        self.land_use = land_use\n        self.market_value = market_value\n\n    def assess_tax(self):\n        rates = {\"SFR\": 0.05, \"MFR\": 0.04}\n        rate = rates.get(self.land_use, 0.02)\n        return self.market_value * rate\n\n# (c) Great Circle Distance (Haversine formula)\nimport math\ndef great_circle(lat1, lon1, lat2, lon2):\n    R = 6371  # Earth radius in km\n    dlat = math.radians(lat2 - lat1)\n    dlon = math.radians(lon2 - lon1)\n    a = (math.sin(dlat/2)**2 +\n         math.cos(math.radians(lat1)) *\n         math.cos(math.radians(lat2)) *\n         math.sin(dlon/2)**2)\n    return R * 2 * math.asin(math.sqrt(a))\n\nä¸‰å€‹å­ä»»å‹™ï¼š (a) è¨ˆç®— CSV å„æ¬„ä½ç©ºå€¼æ•¸é‡ï¼Œ(b) ä»¥ OOP å»ºç«‹ Parcel é¡åˆ¥è¨ˆç®—ä¸å‹•ç”¢ç¨…ï¼ˆSFR 5%ã€MFR 4%ã€å…¶ä»– 2%ï¼‰ï¼Œ(c) ä»¥ Haversine å…¬å¼è¨ˆç®—åœ°çƒè¡¨é¢å…©é»è·é›¢ã€‚\n\n\n\n\n\nTopic / ä¸»é¡Œï¼š DataFrame manipulation, grouping, pivoting, and multi-dataset merging Pandas DataFrame æ“ä½œã€åˆ†çµ„ã€æ¨ç´åˆ†æèˆ‡å¤šè³‡æ–™é›†åˆä½µã€‚\nDatasets / è³‡æ–™é›†ï¼š MovieLens (100K ratings), COVID-19 global time series\nKey Concepts / æ ¸å¿ƒæ¦‚å¿µï¼š pandas Series/DataFrame, boolean indexing, GroupBy, pivot tables, merge/join\n# COVID-19 fatality rate analysis\nfatality = (deaths_total / confirmed_total * 100).sort_values(ascending=False)\n# Peru: 9.17%, Mexico: 7.58%, South Africa: 2.68%\n\n# MovieLens: most rated movies\ntop_movies = ratings.groupby('movieId').size().sort_values(ascending=False).head(10)\n\n# Multi-DataFrame merge\nmerged = pd.merge(users, ratings, on='userId')\nmerged = pd.merge(merged, movies, on='movieId')\n\nåˆ†æé‡é»ï¼š (1) MovieLens â€” æ‰¾å‡ºè©•åˆ†æœ€å¤šçš„é›»å½±ã€ç”·å¥³è©•åˆ†å·®ç•°æœ€å¤§çš„é›»å½±ï¼Œ(2) COVID-19 â€” ç¢ºè¨ºæ•¸å‰ 25 åœ‹ã€å„åœ‹è‡´æ­»ç‡æ’åï¼ˆç§˜é­¯ 9.17% æœ€é«˜ï¼‰ã€æœˆåº¦å¢é‡åˆ†æï¼ˆç¾åœ‹ 2021 å¹´ 9 æœˆå¢ +430 è¬ä¾‹ï¼‰ã€‚\n\n\n\n\n\nTopic / ä¸»é¡Œï¼š Publication-quality charts with matplotlib and Altair ä½¿ç”¨ matplotlib èˆ‡ Altair è£½ä½œå‡ºç‰ˆå“è³ªçš„åœ–è¡¨èˆ‡äº’å‹•å¼è¦–è¦ºåŒ–ã€‚\nDatasets / è³‡æ–™é›†ï¼š MovieLens, COVID-19, Seattle weather (Vega)\n\n\n\n\n\n\nTop 10 Most-Reviewed Movies\n\n\n\n\n\n\n\nCOVID-19 Top 25 Countries\n\n\n\n\n\n\n\n\n\n\n\nGender Rating Differences\n\n\n\n\n\n\n\nSeattle Weather Distribution\n\n\n\n\n\n\nè¦–è¦ºåŒ–æŠ€å·§ï¼š æ°´å¹³é•·æ¢åœ–æ¯”è¼ƒé›»å½±è©•è«–æ•¸é‡ã€æ•£é»åœ–å‘ˆç¾ç”·å¥³è©•åˆ†å·®ç•°ã€åœ“é¤…åœ–å±•ç¤ºå¤©æ°£é¡å‹åˆ†å¸ƒã€æŠ˜ç·šåœ–è¿½è¹¤ COVID-19 è¶¨å‹¢ã€‚Altair éƒ¨åˆ†è£½ä½œäº†äº’å‹•å¼ linked chartï¼ˆé»é¸åœ‹å®¶ â†’ é¡¯ç¤ºè©²åœ‹æ­»äº¡è¶¨å‹¢ï¼‰ã€‚\n\n\n\n\n\nTopic / ä¸»é¡Œï¼š Desktop application: Number guessing game with graphical interface ä½¿ç”¨ PyQt5 é–‹ç™¼æ•¸å­—çŒœè¬éŠæˆ²æ¡Œé¢æ‡‰ç”¨ç¨‹å¼ã€‚\nKey Concepts / æ ¸å¿ƒæ¦‚å¿µï¼š PyQt5 widgets, event-driven programming, signal/slot, Qt Designer\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35}}}%%\nflowchart LR\n    A[\"Qt Designer     \"] --&gt; B[\"frmGuess.py     \"] --&gt; C[\"lab7.py     \"] --&gt; D[\"Number Game GUI     \"]\n\n    style A fill:#F5F5F5,color:#424242,stroke:#BDBDBD,stroke-width:2px\n    style B fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style C fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style D fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n\n\n\n\n\n\n# Number guessing game with hint system\nclass GuessGame:\n    def __init__(self):\n        self.target = random.randint(1, 100)\n        self.guesses = 0\n\n    def make_guess(self, n):\n        self.guesses += 1\n        if n == self.target:\n            return \"Correct!\"\n        return \"Higher!\" if n &lt; self.target else \"Lower!\"\n\n    def use_hint(self):\n        \"\"\"Costs 5 guesses, reveals number within Â±5\"\"\"\n        self.guesses += 5\n        return (self.target - 5, self.target + 5)\n\nGUI åŠŸèƒ½ï¼š éš¨æ©Ÿæ•¸å­— (1-100) çŒœè¬éŠæˆ²ã€‚åŒ…å« (1) çŒœæ¸¬è¿½è¹¤ï¼Œ(2) æç¤ºç³»çµ±ï¼ˆæ¶ˆè€— 5 æ¬¡æ©Ÿæœƒï¼Œç¸®å°ç¯„åœåˆ° Â±5ï¼‰ï¼Œ(3) å‹åˆ©åµæ¸¬èˆ‡é‡è¨­ã€‚ä½¿ç”¨ Qt Designer è¨­è¨ˆä»‹é¢ï¼ŒfrmGuess.py ç‚ºè‡ªå‹•ç”Ÿæˆçš„ UI ç¨‹å¼ç¢¼ã€‚\n\n\n\n\n\nTopic / ä¸»é¡Œï¼š Programmatic map creation and spatial data querying with ArcGIS Online ä½¿ç”¨ ArcGIS API ç¨‹å¼åŒ–å»ºç«‹åœ°åœ–èˆ‡æŸ¥è©¢ç©ºé–“è³‡æ–™ã€‚\nKey Concepts / æ ¸å¿ƒæ¦‚å¿µï¼š ArcGIS authentication, WebMap, FeatureLayer queries, basemap cycling\nfrom arcgis.gis import GIS\nfrom arcgis.mapping import WebMap\n\ngis = GIS(\"https://www.arcgis.com\", username, password)\nm = gis.map(\"University of Texas at Dallas\", zoomlevel=15)\n\n# Search and add feature layers\nitems = gis.content.search(\"UTD Buildings\", item_type=\"Feature Layer\")\nm.add_layer(items[0])\n\n# Query building attributes\nfl = items[0].layers[0]\nfl.properties.fields  # Inspect field schema\n\nGIS æ“ä½œï¼š é€é Python API é€£æ¥ ArcGIS Onlineï¼Œä»¥ UTD æ ¡åœ’ç‚ºä¸­å¿ƒå»ºç«‹äº’å‹•å¼åœ°åœ–ï¼Œæœå°‹ä¸¦ç–ŠåŠ å»ºç¯‰ç‰©åœ–å±¤ï¼ŒæŸ¥è©¢å±¬æ€§æ¬„ä½çµæ§‹ï¼Œåˆ‡æ›ä¸åŒåº•åœ–æ¨£å¼ã€‚\n\n\n\n\n\nTopic / ä¸»é¡Œï¼š Data extraction from web sources using APIs and scraping tools ä½¿ç”¨ API èˆ‡çˆ¬èŸ²å·¥å…·å¾ç¶²é ä¾†æºæ“·å–è³‡æ–™ã€‚\nKey Concepts / æ ¸å¿ƒæ¦‚å¿µï¼š requests, JSON parsing, BeautifulSoup (HTML), Selenium (dynamic content)\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35}}}%%\nflowchart LR\n    A[\"requests GET     \"] --&gt; B[\"JSON / HTML     \"]\n    B --&gt; C[\"BeautifulSoup     \"]\n    B --&gt; D[\"json.loads()     \"]\n    C --&gt; E[\"Structured Data     \"]\n    D --&gt; E\n\n    style A fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style B fill:#F5F5F5,color:#424242,stroke:#BDBDBD,stroke-width:2px\n    style C fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style D fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n    style E fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n\n\n\n\n\n\nimport requests\nfrom bs4 import BeautifulSoup\n\n# RESTful API request\nresponse = requests.get(\"https://api.example.com/data\")\ndata = response.json()\n\n# HTML scraping\npage = requests.get(\"https://example.com\")\nsoup = BeautifulSoup(page.content, \"html.parser\")\nelements = soup.find_all(\"div\", class_=\"target\")\n\nå…©ç¨®æ–¹æ³•ï¼š (1) RESTful API â€” ç™¼é€ GET/POST è«‹æ±‚ï¼Œè§£æ JSON å›æ‡‰ï¼Œ(2) HTML çˆ¬èŸ² â€” ä½¿ç”¨ BeautifulSoup è§£æ DOM çµæ§‹ï¼ŒSelenium è™•ç†å‹•æ…‹è¼‰å…¥å…§å®¹ã€‚\n\n\n\n\n\nTopic / ä¸»é¡Œï¼š Parse structured text files with regex, create spatial visualizations with GeoPandas ä½¿ç”¨æ­£è¦è¡¨é”å¼è§£æçµæ§‹åŒ–æ–‡å­—ï¼Œæ­é… GeoPandas è£½ä½œåœ°ç†ç©ºé–“è¦–è¦ºåŒ–ã€‚\nDataset / è³‡æ–™é›†ï¼š worldcities.txt â€” city coordinates in degrees-minutes format\nimport re\nimport geopandas as gpd\nfrom shapely.geometry import Point\n\n# Parse DMS coordinates with regex\npattern = r\"^(.*)\\t(\\d+)\\t(\\d+) ([NS])\\t(\\d+)\\t(\\d+) ([EW])\\t(.*)$\"\n\nfor line in open(\"worldcities.txt\"):\n    match = re.match(pattern, line.strip())\n    if match:\n        city, lat_d, lat_m, ns, lon_d, lon_m, ew, country = match.groups()\n        lat = (int(lat_d) + int(lat_m)/60) * (-1 if ns == 'S' else 1)\n        lon = (int(lon_d) + int(lon_m)/60) * (-1 if ew == 'W' else 1)\n\n\n\nWorld Cities Map â€” GeoPandas with Natural Earth basemap\n\n\n\nè™•ç†æµç¨‹ï¼š Regex å¾ worldcities.txt æ“·å–åŸå¸‚åã€ç¶“ç·¯åº¦ï¼ˆåº¦åˆ†æ ¼å¼ï¼‰èˆ‡åœ‹å®¶ â†’ è½‰æ› DMS ç‚ºåé€²ä½åº¦æ•¸ â†’ å»ºç«‹ Shapely Point å¹¾ä½• â†’ GeoPandas GeoDataFrame â†’ ç–ŠåŠ  Natural Earth åº•åœ–ç¹ªè£½å…¨çƒåŸå¸‚åˆ†å¸ƒåœ–ã€‚"
  },
  {
    "objectID": "GISPY.html#midterm",
    "href": "GISPY.html#midterm",
    "title": "GIS & Python Programming",
    "section": "Midterm / æœŸä¸­è€ƒ",
    "text": "Midterm / æœŸä¸­è€ƒ\n\nMidterm Project â€” Data Analysis Suite / è³‡æ–™åˆ†æçµ„åˆ\nä¸‰å€‹ç¨ç«‹çš„ Python ç¨‹å¼ï¼Œå±•ç¤ºæª”æ¡ˆæ“ä½œã€è³‡æ–™åˆ†æèˆ‡ä¸å‹•ç”¢æŸ¥è©¢èƒ½åŠ›ã€‚\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35}}}%%\nflowchart TD\n    A[\"Midterm Project     \"] --&gt; B[\"Alumni Research     \"]\n    A --&gt; C[\"File Manipulation     \"]\n    A --&gt; D[\"Real Estate Search     \"]\n\n    B --&gt; B1[\"Income & Debt by Major     \"]\n    C --&gt; C1[\"Recursive CSV Processing     \"]\n    D --&gt; D1[\"Multi-criteria Property Filter     \"]\n\n    style A fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style B fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style C fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n    style D fill:#F3E5F5,color:#6A1B9A,stroke:#CE93D8,stroke-width:2px\n    style B1 fill:#F5F5F5,color:#424242,stroke:#BDBDBD,stroke-width:2px\n    style C1 fill:#F5F5F5,color:#424242,stroke:#BDBDBD,stroke-width:2px\n    style D1 fill:#F5F5F5,color:#424242,stroke:#BDBDBD,stroke-width:2px\n\n\n\n\n\n\n\n1. Alumni Research / æ ¡å‹ç ”ç©¶\nåˆ†ææ ¡å‹æ”¶å…¥èˆ‡å­¸è²¸ï¼šæŒ‰ç§‘ç³»çµ±è¨ˆå¹³å‡å¹´é½¡ã€æ€§åˆ¥æ¯”ä¾‹ã€æ”¶å…¥æ’åã€å­¸è²¸é‚„æ¬¾è¨ˆç®—å™¨ï¼ˆä»¥å¹´æ”¶å…¥ 5% ç‚ºæœˆé‚„æ¬¾ï¼‰ã€‚\n# Loan payoff calculator: 5% annual income as monthly payment\ndef loan_payoff(debt, annual_income, interest_rate=0.05):\n    monthly_payment = annual_income * 0.05 / 12\n    balance, months = debt, 0\n    while balance &gt; 0:\n        balance *= (1 + interest_rate / 12)\n        balance -= monthly_payment\n        months += 1\n    return months // 12, months % 12\n\n\n2. File Manipulation / æª”æ¡ˆæ“ä½œ\néè¿´èµ°è¨ªç›®éŒ„ï¼Œåµæ¸¬ CSV æª”æ¡ˆä¸¦ä»¥ tab åˆ†éš”æ ¼å¼ç¾åŒ–è¼¸å‡ºã€‚\nimport os\ndef process_directory(path):\n    if os.path.isfile(path) and path.endswith('.csv'):\n        pretty_print_csv(path)\n    elif os.path.isdir(path):\n        for root, dirs, files in os.walk(path):\n            for f in files:\n                if f.endswith('.csv'):\n                    pretty_print_csv(os.path.join(root, f))\n\n\n3. Real Estate Search / ä¸å‹•ç”¢æœå°‹\nå¤šæ¢ä»¶éæ¿¾åœ°ç”¢ï¼šå·åˆ¥ä»£ç¢¼ã€æœ€å°å±…ä½é¢ç©ã€å¸‚åƒ¹ç¯„åœã€æŒ‡å®šå­¸å€ã€‚\n\næœŸä¸­ç¸½çµï¼š ä¸‰å€‹ç¨‹å¼åˆ†åˆ¥å±•ç¤º (1) pandas è³‡æ–™åˆ†æèˆ‡åˆä½µï¼ˆmerge on IDï¼‰ï¼Œ(2) os.walk() éè¿´æª”æ¡ˆè™•ç†ï¼Œ(3) å¤šæ¢ä»¶é‚è¼¯ç¯©é¸ã€‚æ¶µè“‹è³‡æ–™ç§‘å­¸ã€ç³»çµ±æ“ä½œèˆ‡å¯¦å‹™æ‡‰ç”¨ä¸‰å¤§é¢å‘ã€‚"
  },
  {
    "objectID": "GISPY.html#final-project",
    "href": "GISPY.html#final-project",
    "title": "GIS & Python Programming",
    "section": "Final Project / æœŸæœ«å°ˆæ¡ˆ",
    "text": "Final Project / æœŸæœ«å°ˆæ¡ˆ\n\nSVD Image Compression Application / SVD å½±åƒå£“ç¸®æ‡‰ç”¨ç¨‹å¼\nTask / ä»»å‹™ï¼š Build an interactive desktop application for image compression using Singular Value Decomposition (SVD), with real-time preview and quality metrics. å»ºç«‹äº’å‹•å¼æ¡Œé¢æ‡‰ç”¨ç¨‹å¼ï¼Œä½¿ç”¨å¥‡ç•°å€¼åˆ†è§£ (SVD) é€²è¡Œå½±åƒå£“ç¸®ï¼Œå…·å‚™å³æ™‚é è¦½èˆ‡å“è³ªæŒ‡æ¨™ã€‚\nMethod / æ–¹æ³•ï¼š PyQt6 GUI + NumPy SVD + PIL image processing\nMathematical Foundation / æ•¸å­¸åŸºç¤ï¼š Eckart-Young Theorem â€” A_k = sum(sigma_i * u_i * v_i^T) for i=1 to k\n\nApplication Architecture / æ‡‰ç”¨ç¨‹å¼æ¶æ§‹\n\n\n\n\n\n%%{init: {\"theme\": \"base\", \"themeVariables\": {\"fontSize\": \"18px\"}, \"flowchart\": {\"padding\": 35}}}%%\nflowchart TD\n    A[\"Image Input (drag & drop)     \"] --&gt; B[\"RGB Channel Split     \"]\n    B --&gt; C[\"np.linalg.svd per channel     \"]\n    C --&gt; D[\"Low-rank Approximation A_k     \"]\n    D --&gt; E[\"Reconstruct RGB     \"]\n    E --&gt; F[\"PSNR + File Size     \"]\n    F --&gt; G[\"Preview & Export     \"]\n\n    style A fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style B fill:#E3F2FD,color:#1565C0,stroke:#90CAF9,stroke-width:2px\n    style C fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style D fill:#FFF3E0,color:#E65100,stroke:#FFCC80,stroke-width:2px\n    style E fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n    style F fill:#F3E5F5,color:#6A1B9A,stroke:#CE93D8,stroke-width:2px\n    style G fill:#E8F5E9,color:#2E7D32,stroke:#A5D6A7,stroke-width:2px\n\n\n\n\n\n\n\nSVD åŸç†ï¼š ä»»ä½•çŸ©é™£ A å¯åˆ†è§£ç‚º A = U Sigma V^Tã€‚å–å‰ k å€‹å¥‡ç•°å€¼é‡å»º A_kï¼Œå³ç‚ºæœ€ä½³ rank-k è¿‘ä¼¼ï¼ˆEckart-Young å®šç†ï¼‰ã€‚k è¶Šå°å£“ç¸®ç‡è¶Šé«˜ï¼Œä½†å“è³ªè¶Šä½ã€‚\n\n\n\nCore Algorithm / æ ¸å¿ƒæ¼”ç®—æ³•\nimport numpy as np\nfrom PIL import Image\n\ndef perform_svd(image_array):\n    \"\"\"SVD on each RGB channel separately\"\"\"\n    channels = {}\n    for i, name in enumerate(['R', 'G', 'B']):\n        U, S, Vt = np.linalg.svd(image_array[:, :, i], full_matrices=False)\n        channels[name] = (U, S, Vt)\n    return channels\n\ndef reconstruct(channels, k):\n    \"\"\"Low-rank approximation with k singular values\"\"\"\n    reconstructed = np.zeros_like(original)\n    for i, name in enumerate(['R', 'G', 'B']):\n        U, S, Vt = channels[name]\n        reconstructed[:, :, i] = np.clip(\n            U[:, :k] @ np.diag(S[:k]) @ Vt[:k, :], 0, 255\n        )\n    return reconstructed.astype(np.uint8)\n\ndef calculate_psnr(original, compressed):\n    mse = np.mean((original.astype(float) - compressed.astype(float)) ** 2)\n    return 10 * np.log10(255**2 / mse) if mse &gt; 0 else float('inf')\n\n\nGUI Features / ä»‹é¢åŠŸèƒ½\n# PyQt6 GUI with dual slider control\nclass SVDCompressor(QMainWindow):\n    def __init__(self):\n        # Drag-and-drop image upload\n        # Dual slider: compression ratio â†” target file size (linked)\n        # Smart presets:\n        #   Social media: 2 MB, ~35 dB PSNR\n        #   Email:        5 MB, ~40 dB PSNR\n        #   High quality: 80% compression, ~45 dB PSNR\n        pass\n\n\nApplication Demo / æ‡‰ç”¨ç¨‹å¼å±•ç¤º\n\n\n\nSVD Compression App â€” PyQt6 Desktop Application\n\n\n\nApp åŠŸèƒ½ä¸€è¦½ï¼š\n\næ‹–æ›³ä¸Šå‚³ â€” å°‡åœ–ç‰‡æ‹–ç§»è‡³ä»‹é¢å³å¯è¼‰å…¥\né›™æ»‘æ¡¿é€£å‹• â€” å£“ç¸®æ¯”ä¾‹ â†”ï¸ ç›®æ¨™æª”æ¡ˆå¤§å°ï¼ˆæ‹–å‹•ä¸€æ¢å¦ä¸€æ¢è‡ªå‹•èª¿æ•´ï¼‰\næ™ºæ…§å»ºè­° â€” ä¸‰ç¨®é è¨­æ¨¡æ¿ï¼šç¤¾ç¾¤åª’é«” (2 MB)ã€éƒµä»¶é™„ä»¶ (5 MB)ã€é«˜å“è³ªå­˜æª” (PSNR &gt; 40 dB)\nå³æ™‚é è¦½ â€” å·¦å³å°æ¯”åŸåœ– vs å£“ç¸®å¾Œï¼Œä¸‹æ–¹é¡¯ç¤º PSNR / æª”æ¡ˆå¤§å° / å£“ç¸®æ¯”\nå“è³ªè­¦å‘Š â€” PSNR ä½æ–¼ 40 dB æ™‚è‡ªå‹•å½ˆå‡ºè­¦å‘Š\nåŸºæ–¼ Eckart-Young å®šç† â€” ç†è«–ä¿è­‰æœ€ä½³ä½ç§©è¿‘ä¼¼\n\n\nğŸ“ Downloadï¼š SVD_app.py (åŸå§‹ç¢¼)\n\n\nSVD Quality Analysis / SVD å“è³ªåˆ†æ\n\n\n\n\n\n\nRGB Channel Decomposition\n\n\n\n\n\n\n\nSVD Metrics vs k (MSE, PSNR, norm, sigma)\n\n\n\n\n\n\n\nWarning in annotate(\"label\", x = 300, y = 34, label = \"k=300: 31.88 dB\", :\nIgnoring unknown parameters: `label.size`\n\n\n\n\n\n\n\n\n\n\nå“è³ªåˆ†æï¼š å·¦åœ– â€” PSNR éš¨ k å¢åŠ è€Œæå‡ï¼Œk=300 æ™‚é” 31.88 dBï¼ˆè¶…é 30 dB é–€æª»ï¼Œç°è‰²è™›ç·šï¼‰ï¼Œk=700 æ™‚æ¥è¿‘åŸåœ–å“è³ª (44.70 dB)ã€‚å³åœ– â€” MSE å‘ˆæŒ‡æ•¸ä¸‹é™ï¼Œk=300 å¾Œæ”¹å–„è¶¨ç·©ã€‚å¯¦å‹™çµè«–ï¼š k=200~400 æ˜¯å£“ç¸®ç‡èˆ‡å“è³ªçš„æœ€ä½³å¹³è¡¡å€é–“ã€‚\n\n\n\n\nk\nPSNR (dB)\nMSE\nCompression Ratio\n\n\n\n\n1\n12.50\n3650\n99.9%\n\n\n50\n27.10\n127\n95.1%\n\n\n100\n29.50\n73\n90.3%\n\n\n300\n31.88\n42.29\n70.9%\n\n\n700\n44.70\n2.2\n32.0%\n\n\n\n\nEckart-Young å®šç†é©—è­‰ï¼š å¯¦é©—ç¢ºèª ||A - A_k||â‚‚ â‰ˆ sigma_{k+1}ï¼Œå³ä½ç§©è¿‘ä¼¼çš„èª¤å·®ç­‰æ–¼ç¬¬ k+1 å€‹å¥‡ç•°å€¼ã€‚é€™ç‚ºé¸æ“‡æœ€ä½³ k å€¼æä¾›äº†ç†è«–ä¾æ“š â€” ç•¶ sigma_{k+1} å°æ–¼å“è³ªé–€æª»æ™‚å³å¯åœæ­¢ã€‚"
  },
  {
    "objectID": "GISPY.html#course-skills-summary-èª²ç¨‹æŠ€èƒ½ç¸½è¦½",
    "href": "GISPY.html#course-skills-summary-èª²ç¨‹æŠ€èƒ½ç¸½è¦½",
    "title": "GIS & Python Programming",
    "section": "Course Skills Summary / èª²ç¨‹æŠ€èƒ½ç¸½è¦½",
    "text": "Course Skills Summary / èª²ç¨‹æŠ€èƒ½ç¸½è¦½\n\n\n\n\n\n\n\n\n\n\nèª²ç¨‹ç¸½çµï¼š å¾åŸºç¤ Pythonï¼ˆå‡½å¼ã€OOPã€æª”æ¡ˆæ“ä½œï¼‰åˆ°è³‡æ–™ç§‘å­¸ï¼ˆpandasã€è¦–è¦ºåŒ–ï¼‰ã€GIS ç©ºé–“åˆ†æï¼ˆArcGISã€GeoPandasï¼‰ã€GUI é–‹ç™¼ï¼ˆPyQt5/6ï¼‰ã€ç¶²é çˆ¬èŸ²ï¼ˆrequestsã€BeautifulSoupï¼‰ï¼Œæœ€çµ‚ä»¥ SVD å½±åƒå£“ç¸®å°ˆæ¡ˆæ•´åˆæ•¸å­¸ç†è«–èˆ‡è»Ÿé«”å·¥ç¨‹èƒ½åŠ›ã€‚"
  }
]